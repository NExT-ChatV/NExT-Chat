{
    "summary": "The `SingleImageConvDatasetMixin` processes conversation datasets, initializes attributes, extends classes, retrieves items, applies transforms, checks image existence, and utilizes persistent workers. The WRAPPER_DATASET dictionary maps dataset names to their respective classes for processing single-image conversation tasks in the NExT-Chat project.",
    "details": [
        {
            "comment": "This code defines a class `SingleImageConvDatasetMixin` for a dataset that involves single images and conversations. It includes an initializer with arguments like preprocessor, process_func, conv_template, mode, tokenize_kwargs, training_args, and transforms. The assert statement ensures the mode is either 'train', 'validation', or 'test'. This class extends another class (not shown) and calls its initializer using `super().__init__()`.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/single_image_convsation.py\":0-32",
            "content": "import warnings\nfrom functools import partial\nfrom typing import Dict, Any, Callable, List, Optional, Tuple, Type\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom transformers import TrainingArguments\nfrom .root import IMAGE_PLACEHOLDER, BOXES_PLACEHOLDER\nfrom ..conversation import Conversation, get_conv_template\nfrom ..utils import post_process_generate_ids\nclass SingleImageConvDatasetMixin:\n    def __init__(\n            self,\n            *args,\n            preprocessor: Dict[str, Any],\n            process_func: Dict[str, Any],\n            conv_template: Callable[[], Conversation] = partial(get_conv_template, name='vicuna_v1.1'),\n            mode='train',\n            tokenize_kwargs: dict = None,\n            training_args: TrainingArguments = None,\n            transforms: Optional[Callable] = None,\n            **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n        assert mode in ['train', 'validation', 'test']\n        self.preprocessor = preprocessor\n        self.process_func = process_func"
        },
        {
            "comment": "This code represents a class that initializes attributes, such as conv_template, mode, tokenize_kwargs, and training_args. The __getitem__ method retrieves the raw item from an index and applies transforms to the image and target using the defined transforms attribute. It also checks if the item is a multimage and handles it accordingly.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/single_image_convsation.py\":33-55",
            "content": "        self.conv_template = conv_template\n        self.mode = mode\n        self.tokenize_kwargs = tokenize_kwargs if tokenize_kwargs is not None else {}\n        self.training_args = training_args\n        self.transforms = transforms\n    def __getitem__(self, index, debug_mode=False, return_conv=False) -> Dict[str, Any]:\n        # getitem\n        item = self.get_raw_item(index)\n        image: Image.Image = item.get('image', None)\n        target: Dict[str, Any] = item.get('target', None)\n        raw_conv: List[Dict[str, Any]] = item['conversations']\n        # transform\n        assert isinstance(image, list) == isinstance(target, list)\n        multimage_mode = isinstance(image, list)\n        if isinstance(image, list):\n            # TODO: validate raw item\n            transformed_image, transformed_target = [], []\n            for img, tgt in zip(image, target):\n                if self.transforms is not None and image is not None:\n                    img, tgt = self.transforms(img, tgt)\n                if tgt is not None:"
        },
        {
            "comment": "Code snippet initializes 'width' and 'height' attributes in the target dictionary, appends images to 'transformed_image', targets to 'transformed_target', checks for image and target existence in item, applies transforms if provided, sets width and height of target based on image dimensions, processes conv and image using multimage mode, and finally builds the conversation object.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/single_image_convsation.py\":56-73",
            "content": "                    tgt['width'], tgt['height'] = img.width, img.height\n                transformed_image.append(img)\n                transformed_target.append(tgt)\n            image, target = transformed_image, transformed_target\n        else:\n            self.validate_raw_item(item)  # only validate for single image.\n            if self.transforms is not None and image is not None:\n                image, target = self.transforms(image, target)\n            has_image = 'image' in item and bool(item['image'])\n            has_target = 'target' in item and bool(item['target']) and any(bool(elem) for elem in item['target'].values())\n            if has_target and has_image:\n                target['width'], target['height'] = image.width, image.height\n        # preprocess\n        raw_conv = self.process_conv(raw_conv)\n        raw_conv, image = self.process_conv_multimage(raw_conv, image)\n        raw_conv, tar_boxes = self.process_target(raw_conv, target, multimage_mode=multimage_mode)\n        conv = self.build_conv(raw_conv)"
        },
        {
            "comment": "This code is from the single_image_conversation.py file in the NExT-Chat project. The function processes a conversation and related image, updating dictionaries for text and image data and adding location input/targets. If debug mode is enabled, it returns multiple dictionaries containing the processed data; otherwise, it only returns the updated ret_dict. If no image or non-list/tuple image is given, it simply returns raw conversation and image as they are.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/single_image_convsation.py\":74-103",
            "content": "        if return_conv:\n            # noinspection PyTypeChecker\n            return conv\n        text_dict = self.process_text(conv)\n        image_dict = self.process_image(image)\n        # return\n        ret_dict = {}\n        ret_dict.update(text_dict)\n        ret_dict.update(image_dict)\n        ret_dict[\"loc_inputs\"] = tar_boxes[\"all_boxes\"]\n        ret_dict[\"loc_targets\"] = tar_boxes[\"gpt_boxes\"]\n        self._print_sample(ret_dict, raw_conv, conv)\n        if debug_mode:\n            return {'ret': ret_dict, 'raw_conv': raw_conv, 'conv': conv, 'image': image}\n        return ret_dict\n    def __len__(self):\n        raise NotImplementedError\n    # noinspection PyMethodMayBeStatic\n    def process_conv_multimage(self, raw_conv, image):\n        # re-sort multi image\n        if image is None:\n            return raw_conv, image\n        if not isinstance(image, (list, tuple)):\n            return raw_conv, image\n        image_seqs = []\n        for conv in raw_conv:\n            image_seqs.extend(conv['image_seq'] if 'image_seq' in conv else [])"
        },
        {
            "comment": "The code is defining a function to return 'raw_conv' and images from the provided 'image_seqs'. It also has another function called 'get_raw_item' which returns an item in a specific format. The item includes PIL Image, target boxes coordinates, and conversations with their respective types (human or gpt) and associated values. The boxes are represented by their coordinates and the items are indexed.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/single_image_convsation.py\":104-132",
            "content": "        images = []\n        for idx in image_seqs:\n            images.append(image[idx])\n        return raw_conv, images\n    def get_raw_item(self, index) -> Dict[str, Any]:\n        \"\"\"\n        return item format like this.\n        item = {\n            'image': # PIL.Image.Image,\n            'target': {\n                # xmin, ymin, xmax, ymax\n                'boxes': [\n                    [10, 10, 256, 265],  # dog1\n                    [24, 18, 378, 768],  # dog2\n                    [100, 310, 670, 653],  # man\n                    [278, 320, 809, 673],  # rope\n                ],\n            }\n            \"conversations\": [\n                {\n                    'from': 'human',\n                    'value': 'What is the relation between the two dogs <boxes> and the man <boxes> in the image <image> ?',\n                    'boxes_seq': [[0, 1], [2], ],\n                },\n                {\n                    'from': 'gpt',\n                    'value': 'a rope <boxes> is connecting the left dog <boxes> with the man <boxes>. '"
        },
        {
            "comment": "This code defines a class method for validating raw items in the dataset. It checks if each item contains an 'image' and 'target' field, and ensures that the 'target' field has non-empty values. If the target field exists, it also checks if it contains a 'boxes' key with non-empty values. Additionally, the code verifies if the human input includes an image placeholder in any sentence within the conversation.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/single_image_convsation.py\":133-155",
            "content": "                             'So the man <boxes> is walking the dog <boxes>.'\n                            'And the man <boxes> has no relationship with the right dog <boxes>',\n                    'boxes_seq': [[3], [0], [2], [2], [0], [2], [1]],\n                }\n            ]\n        }\n        # placeholder: <image> <boxes>\n        \"\"\"\n        raise NotImplementedError\n    # noinspection PyMethodMayBeStatic\n    def validate_raw_item(self, item):\n        has_image = 'image' in item and bool(item['image'])\n        has_target = 'target' in item and bool(item['target']) and any(bool(elem) for elem in item['target'].values())\n        has_target_boxes = 'boxes' in item['target'] if has_target else False\n        raw_conv: List[Dict[str, Any]] = item['conversations']\n        # check image\n        human_input_has_image_placeholder = any(\n            sentence['from'] == 'human' and IMAGE_PLACEHOLDER in sentence['value'] for sentence in raw_conv\n        )\n        if human_input_has_image_placeholder:\n            assert has_image"
        },
        {
            "comment": "The code checks if the image placeholder is present in the human input but not in the GPT response, and if there are any boxes placeholder in the conversation. It asserts that GPT should not have an image placeholder and if boxes placeholder exists, it asserts that there must be target boxes. The function then appends messages to the conversation based on their roles.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/single_image_convsation.py\":156-178",
            "content": "        if has_image and (not human_input_has_image_placeholder):\n            warnings.warn(f'item has image but the question has no image placeholder.\\n{item}')\n        gpt_input_has_image_placeholder = any(\n            sentence['from'] == 'gpt' and IMAGE_PLACEHOLDER in sentence['value'] for sentence in raw_conv\n        )\n        assert not gpt_input_has_image_placeholder\n        # check target\n        has_boxes_placeholder = any(\n            BOXES_PLACEHOLDER in sentence['value'] for sentence in raw_conv\n        )\n        if has_boxes_placeholder:\n            assert has_target_boxes\n        # not check box placeholder num this will be checked in format process\n    def build_conv(self, source: List[Dict[str, Any]]) -> Conversation:\n        conv = self.conv_template()\n        role_map = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n        assert len(source) > 0\n        assert source[0]['from'] == 'human'\n        for sentence in source:\n            role = role_map[sentence['from']]\n            conv.append_message(role, sentence['value'])"
        },
        {
            "comment": "The code defines three functions: `process_conv`, `process_target`, and `process_text`. These functions preprocess conversation data, replace image placeholders with sequences, and convert Conversation objects to Tensor format for machine learning models. The code is part of a larger project that utilizes Conversational AI techniques.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/single_image_convsation.py\":179-199",
            "content": "        return conv\n    def process_conv(self, raw_conv: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        \"\"\"\n        some utils preprocess for raw_conv.\n            e.g. replace <image> placeholder to sequence <im_start> <im_patch>*256 <im_end>\n        \"\"\"\n        return self.process_func['conv'](raw_conv, self.preprocessor, self.conv_template)\n    def process_target(self, raw_conv: List[Dict[str, Any]], target: Dict[str, Any], multimage_mode=False) -> Tuple[\n        List[Dict[str, Any]], Dict[str, Any]]:\n        \"\"\"\n        convert target placeholder to actual information in raw_conv.\n            e.g. normalize bounding boxes; convert bounding boxes format; replace <boxes> placeholder\n        \"\"\"\n        return self.process_func['target'](raw_conv, target, self.preprocessor, multimage_mode=multimage_mode)\n    def process_text(self, conv: Conversation) -> Dict[str, Any]:\n        \"\"\"\n        convert Conversation object to torch.Tensor, e.g. input_ids, labels, attention_mask, etc.\n            self.tokenize_kwargs control something like padding/truncation behavior."
        },
        {
            "comment": "This code defines three methods: \"process_text\", \"process_image\", and \"_print_sample\". The first method processes text input, the second converts an Image.Image object to a torch.Tensor, and the last one prints sample information if it's the first time running. It appears to be part of a larger class for processing images and text in a conversation dataset.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/single_image_convsation.py\":200-217",
            "content": "        \"\"\"\n        return self.process_func['text'](conv, self.preprocessor, self.mode, **self.tokenize_kwargs)\n    def process_image(self, image: Image.Image) -> Dict[str, Any]:\n        \"\"\"\n        convert Image.Image object to torch.Tensor\n        \"\"\"\n        return self.process_func['image'](image, self.preprocessor)\n    def _print_sample(self, ret_dict, raw_conv, conv):\n        if not hasattr(self, '_printed_sample'):\n            self._printed_sample = True\n            post_processed_labels = post_process_generate_ids(self.preprocessor['text'], ret_dict['labels'])\n            print(f\"=================== {self.mode} sample ===================\", flush=True)\n            print(f\"        input_ids: {self.preprocessor['text'].convert_ids_to_tokens(ret_dict['input_ids'])}\")\n            print(f\"           labels: {self.preprocessor['text'].convert_ids_to_tokens(post_processed_labels)}\")\n            print(f\"decoded input_ids: {self.preprocessor['text'].decode(ret_dict['input_ids'])}\")\n            print(f\"decoded    labels: {self.preprocessor['text'].decode(post_processed_labels)}\")"
        },
        {
            "comment": "The code checks if 'image' is in the ret_dict and if it is not None. If so, it prints the image shape if it is a tensor, image keys if it is a dictionary, number of images and their type if it is a list, and the image type otherwise. Then, it tries to save an object containing ret_dict, raw_conv, and conv prompt if training_args are not None and output_dir exists.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/single_image_convsation.py\":218-237",
            "content": "            if 'image' in ret_dict and ret_dict['image'] is not None:\n                image = ret_dict['image']\n                if isinstance(image, torch.Tensor):\n                    print(f\"            image: {image.shape}\")\n                elif isinstance(image, dict):\n                    print(f\"            image: {image.keys()}\")\n                elif isinstance(image, list) and len(image) > 0:\n                    print(f\"            image: {len(image)}, {type(image[0])}\")\n                else:\n                    print(f\"            image: {type(image)}\")\n            print(\"====================================================\", flush=True)\n            try:\n                if self.training_args is not None:\n                    _save_obj = {\n                        'ret_dict': ret_dict,\n                        'raw_conv': raw_conv,\n                        'conv': conv.get_prompt(),\n                    }\n                    from pathlib import Path\n                    output_dir = Path(self.training_args.output_dir)"
        },
        {
            "comment": "The code is creating a SingleImageConvDataset class, which inherits from SingleImageConvDatasetMixin and Dataset classes. It initializes an instance of the dataset_generator type specified during object creation, sets it as self.dataset, and provides a method for lazy initialization if num_worker > 0 to avoid 'copy-on-read' behavior.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/single_image_convsation.py\":238-259",
            "content": "                    output_dir.mkdir(exist_ok=True, parents=True)\n                    _local_rank = self.training_args.local_rank\n                    _word_size = self.training_args.world_size\n                    _file_path = str(output_dir / f'sample_check_{self.mode}_{_local_rank}_{_word_size}.pt')\n                    print(f'saving some sample to {_file_path} for check.')\n                    torch.save(_save_obj, _file_path)\n            except Exception as e:\n                warnings.warn(f'try to save samples but get exception: {e.args}. ignored.')\nclass SingleImageConvDataset(SingleImageConvDatasetMixin, Dataset):\n    _repr_indent = 4\n    def __init__(self, *args, dataset_generator: Type[Dataset], **kwargs):\n        super().__init__(*args, **kwargs)\n        self.dataset_generator = dataset_generator\n        self.dataset = None\n    def initialize_if_needed(self):\n        \"\"\"\n        lazy initialize for big in-memory python object due to python 'copy-on-read' behavior\n        when num_worker > 0. refer: https://github.com/pytorch/pytorch/issues/13246"
        },
        {
            "comment": "This code defines a class, SingleImageConvSegDataset, which is a subclass of the Dataset class and extends another class SingleImageConvDatasetMixin. The class has an optional dataset_generator parameter for generating datasets, and it includes methods like __len__, get_raw_item, and __repr__ to return information about the dataset, such as number of datapoints and its representation. It also uses ResizeAndPad transforms for data processing.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/single_image_convsation.py\":260-289",
            "content": "        \"\"\"\n        if self.dataset is None:\n            # warnings.warn(\"it's highly recommended that set persistent_workers=True, \"\n            #               \"otherwise this initialize code will run in every epoch beginning.\"\n            #               \"(ignore me if set)\")\n            self.dataset = self.dataset_generator()\n    def __len__(self):\n        self.initialize_if_needed()\n        return len(self.dataset)\n    def get_raw_item(self, index) -> Dict[str, Any]:\n        self.initialize_if_needed()\n        return self.dataset[index]\n    def __repr__(self) -> str:\n        head = \"Dataset \" + self.__class__.__name__\n        body = [\n            f\"Number of datapoints: {self.__len__()}\",\n        ]\n        body += self.dataset.__repr__().splitlines()\n        lines = [head] + [\" \" * self._repr_indent + line for line in body]\n        return \"\\n\".join(lines)\nfrom mllm.models.sam.transforms import ResizeAndPad\nclass SingleImageConvSegDataset(SingleImageConvDatasetMixin, Dataset):\n    _repr_indent = 4\n    def __init__(self, *args, dataset_generator: Type[Dataset], **kwargs):"
        },
        {
            "comment": "This code defines a class for a dataset generator with lazy initialization to handle big in-memory Python objects. The `initialize_if_needed` method is called when needed, and the `__len__` and `get_raw_item` methods provide access to the dataset items. It is recommended to set `persistent_workers=True` to avoid running the initialize code in every epoch.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/single_image_convsation.py\":290-315",
            "content": "        super().__init__(*args, **kwargs)\n        self.dataset_generator = dataset_generator\n        self.dataset = None\n        self.sam_transform = ResizeAndPad(1024)\n    def initialize_if_needed(self):\n        \"\"\"\n        lazy initialize for big in-memory python object due to python 'copy-on-read' behavior\n        when num_worker > 0. refer: https://github.com/pytorch/pytorch/issues/13246\n        \"\"\"\n        if self.dataset is None:\n            # warnings.warn(\"it's highly recommended that set persistent_workers=True, \"\n            #               \"otherwise this initialize code will run in every epoch beginning.\"\n            #               \"(ignore me if set)\")\n            self.dataset = self.dataset_generator()\n    def __len__(self):\n        self.initialize_if_needed()\n        return len(self.dataset)\n    def get_raw_item(self, index) -> Dict[str, Any]:\n        self.initialize_if_needed()\n        return self.dataset[index]\n    def __repr__(self) -> str:\n        head = \"Dataset \" + self.__class__.__name__"
        },
        {
            "comment": "This function returns a string representation of the dataset, including the number of datapoints and the content of each item. It also retrieves an item from the dataset at the given index, performing any necessary transformations on the image and masks before returning them in a list format.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/single_image_convsation.py\":316-341",
            "content": "        body = [\n            f\"Number of datapoints: {self.__len__()}\",\n        ]\n        body += self.dataset.__repr__().splitlines()\n        lines = [head] + [\" \" * self._repr_indent + line for line in body]\n        return \"\\n\".join(lines)\n    def __getitem__(self, index, debug_mode=False, return_conv=False) -> Dict[str, Any]:\n        # getitem\n        item = self.get_raw_item(index)\n        image: Image.Image = item.get('image', None)\n        target: Dict[str, Any] = item.get('target', None)\n        raw_conv: List[Dict[str, Any]] = item['conversations']\n        ret_w, ret_h = image.width, image.height\n        ret_masks = target.get(\"masks\", None)\n        # sam transform\n        sam_image, sam_masks, sam_hw = self.sam_transform(image, target.get(\"masks\", None))\n        # transform\n        assert isinstance(image, list) == isinstance(target, list)\n        multimage_mode = isinstance(image, list)\n        if isinstance(image, list):\n            # TODO: validate raw item\n            transformed_image, transformed_target = [], []"
        },
        {
            "comment": "The code performs image preprocessing for single and multi-image conversations. It iterates over images and targets, applies transformations if specified, updates target dimensions based on image size, appends processed images and targets into lists, and validates raw items for single image cases. If a target and image exist, it also updates target and image width/height based on the image size.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/single_image_convsation.py\":342-361",
            "content": "            for img, tgt in zip(image, target):\n                if self.transforms is not None and image is not None:\n                    img, tgt = self.transforms(img, tgt)\n                if tgt is not None:\n                    tgt['width'], tgt['height'] = img.width, img.height\n                transformed_image.append(img)\n                transformed_target.append(tgt)\n            image, target = transformed_image, transformed_target\n        else:\n            self.validate_raw_item(item)  # only validate for single image.\n            if self.transforms is not None and image is not None:\n                image, target = self.transforms(image, target)\n            has_image = 'image' in item and bool(item['image'])\n            has_target = 'target' in item and bool(item['target']) and any(bool(elem) for elem in item['target'].values())\n            if has_target and has_image:\n                target['width'], target['height'] = image.width, image.height\n        # preprocess\n        raw_conv = self.process_conv(raw_conv)"
        },
        {
            "comment": "The code defines a class that processes multi-image conversations and returns a dictionary containing text, image, and target information. It first processes the raw conversation data and images, then builds the conversation and finally, it processes the text and image data separately. The resulting dictionaries contain information like location inputs, targets, sampled images, masks, image size, and unresized masks. The function also has a debug mode to return additional information.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/single_image_convsation.py\":362-387",
            "content": "        raw_conv, image = self.process_conv_multimage(raw_conv, image)\n        raw_conv, tar_boxes = self.process_target(raw_conv, target, multimage_mode=multimage_mode)\n        conv = self.build_conv(raw_conv)\n        if return_conv:\n            # noinspection PyTypeChecker\n            return conv\n        text_dict = self.process_text(conv)\n        image_dict = self.process_image(image)\n        # return\n        ret_dict = {}\n        ret_dict.update(text_dict)\n        ret_dict.update(image_dict)\n        ret_dict[\"loc_inputs\"] = tar_boxes[\"all_boxes\"]\n        ret_dict[\"loc_targets\"] = tar_boxes[\"gpt_boxes\"]\n        ret_dict[\"images_sam\"] = sam_image\n        ret_dict[\"masks_sam\"] = sam_masks\n        ret_dict[\"img_size\"] = torch.tensor([ret_h, ret_w])\n        ret_dict[\"unresized_masks\"] = torch.tensor(ret_masks[0])\n        self._print_sample(ret_dict, raw_conv, conv)\n        if debug_mode:\n            return {'ret': ret_dict, 'raw_conv': raw_conv, 'conv': conv, 'image': image}\n        return ret_dict\n__all__ = ['SingleImageConvDatasetMixin', 'SingleImageConvDataset']"
        },
        {
            "comment": "This code defines a dictionary called WRAPPER_DATASET that maps dataset names \"conv\" and \"conv_seg\" to their respective classes SingleImageConvDataset and SingleImageConvSegDataset. These datasets are likely used for processing single image conversation tasks in the NExT-Chat project.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/single_image_convsation.py\":388-391",
            "content": "WRAPPER_DATASET = {\n    \"conv\": SingleImageConvDataset,\n    \"conv_seg\": SingleImageConvSegDataset,\n}"
        }
    ]
}