{
    "summary": "The ResizeLongestSide class handles image resizing, while the ResizeAndPad class resizes and pads images to target size, normalizes, and adjusts bounding boxes if masks are provided. Both classes define transform functions in `transforms.py`.",
    "details": [
        {
            "comment": "This code defines a ResizeLongestSide class that resizes images to the longest side specified and provides methods for resizing coordinates and boxes. It accepts numpy arrays or batched torch tensors, with the expectation of a numpy array being in uint8 format.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/transforms.py\":0-32",
            "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\nimport PIL.Image\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nimport numpy as np\nimport torch\nfrom torch.nn import functional as F\nfrom torchvision.transforms.functional import resize, to_pil_image  # type: ignore\nfrom copy import deepcopy\nfrom typing import Tuple\nimport torchvision.transforms as transforms\nclass ResizeLongestSide:\n    \"\"\"\n    Resizes images to the longest side 'target_length', as well as provides\n    methods for resizing coordinates and boxes. Provides methods for\n    transforming both numpy array and batched torch tensors.\n    \"\"\"\n    def __init__(self, target_length: int) -> None:\n        self.target_length = target_length\n    def apply_image(self, image) -> np.ndarray:\n        \"\"\"\n        Expects a numpy array with shape HxWxC in uint8 format.\n        \"\"\"\n        if type(image) is np.ndarray:\n            h, w = image.shape[0], image.shape[1]\n            target_size = self.get_preprocess_shape(h, w, self.target_length)"
        },
        {
            "comment": "The code defines a class with methods to resize and apply transformations to images. It has a method for resizing images with target sizes, another for applying coordinate transformations based on image size changes, and yet another for applying box transformations to numpy arrays of shape Bx4. The class is likely used in an image processing pipeline for machine learning tasks.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/transforms.py\":33-55",
            "content": "            return np.array(resize(to_pil_image(image), target_size))\n        else:\n            h, w = image.height, image.width\n            target_size = self.get_preprocess_shape(h, w, self.target_length)\n            return np.array(resize(image, target_size))\n    def apply_coords(self, coords: np.ndarray, original_size: Tuple[int, ...]) -> np.ndarray:\n        \"\"\"\n        Expects a numpy array of length 2 in the final dimension. Requires the\n        original image size in (H, W) format.\n        \"\"\"\n        old_h, old_w = original_size\n        new_h, new_w = self.get_preprocess_shape(\n            original_size[0], original_size[1], self.target_length\n        )\n        coords = deepcopy(coords).astype(float)\n        coords[..., 0] = coords[..., 0] * (new_w / old_w)\n        coords[..., 1] = coords[..., 1] * (new_h / old_h)\n        return coords\n    def apply_boxes(self, boxes: np.ndarray, original_size: Tuple[int, ...]) -> np.ndarray:\n        \"\"\"\n        Expects a numpy array shape Bx4. Requires the original image size"
        },
        {
            "comment": "The code contains functions for applying transformations to images and coordinates in a deep learning model. The apply_image_torch function resizes the input image to match the target size, while the apply_coords_torch function applies coordinate transformations based on the original image size. These functions are designed to be used with torch tensors and may not exactly match the apply_image function.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/transforms.py\":56-80",
            "content": "        in (H, W) format.\n        \"\"\"\n        boxes = self.apply_coords(boxes.reshape(-1, 2, 2), original_size)\n        return boxes.reshape(-1, 4)\n    def apply_image_torch(self, image: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Expects batched images with shape BxCxHxW and float format. This\n        transformation may not exactly match apply_image. apply_image is\n        the transformation expected by the model.\n        \"\"\"\n        # Expects an image in BCHW format. May not exactly match apply_image.\n        target_size = self.get_preprocess_shape(image.shape[2], image.shape[3], self.target_length)\n        return F.interpolate(\n            image, target_size, mode=\"bilinear\", align_corners=False, antialias=True\n        )\n    def apply_coords_torch(\n        self, coords: torch.Tensor, original_size: Tuple[int, ...]\n    ) -> torch.Tensor:\n        \"\"\"\n        Expects a torch tensor with length 2 in the last dimension. Requires the\n        original image size in (H, W) format.\n        \"\"\"\n        old_h, old_w = original_size"
        },
        {
            "comment": "This code defines transform functions for image boxes. The `transforms.py` file contains methods like `get_preprocess_shape`, `apply_coords_torch`, and `apply_boxes_torch`. \nThe `get_preprocess_shape` function takes original image dimensions (oldh, oldw) and target length (long_side_length) to compute new height (newh) and width (neww). The `apply_coords_torch` applies coordinate transformation to input boxes using the given original size. Lastly, the `apply_boxes_torch` reshapes the boxes tensor and then applies the coordinate transformation.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/transforms.py\":81-107",
            "content": "        new_h, new_w = self.get_preprocess_shape(\n            original_size[0], original_size[1], self.target_length\n        )\n        coords = deepcopy(coords).to(torch.float)\n        coords[..., 0] = coords[..., 0] * (new_w / old_w)\n        coords[..., 1] = coords[..., 1] * (new_h / old_h)\n        return coords\n    def apply_boxes_torch(\n        self, boxes: torch.Tensor, original_size: Tuple[int, ...]\n    ) -> torch.Tensor:\n        \"\"\"\n        Expects a torch tensor with shape Bx4. Requires the original image\n        size in (H, W) format.\n        \"\"\"\n        boxes = self.apply_coords_torch(boxes.reshape(-1, 2, 2), original_size)\n        return boxes.reshape(-1, 4)\n    @staticmethod\n    def get_preprocess_shape(oldh: int, oldw: int, long_side_length: int) -> Tuple[int, int]:\n        \"\"\"\n        Resize to [long_side_length, long_side_length]\n        \"\"\"\n        scale = long_side_length * 1.0 / max(oldh, oldw)\n        newh, neww = oldh * scale, oldw * scale\n        neww = int(neww + 0.5)\n        newh = int(newh + 0.5)"
        },
        {
            "comment": "The code defines a ResizeAndPad class that takes an image and masks, resizes them to the target size using ResizeLongestSide, then pads them to form a square. The pixel mean and standard deviation are used for normalization. The class has an __init__ method and a __call__ method for applying the transformations.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/transforms.py\":108-137",
            "content": "        return (newh, neww)\nclass ResizeAndPad:\n    def __init__(self, target_size):\n        self.target_size = target_size\n        self.transform = ResizeLongestSide(target_size)\n        self.to_tensor = transforms.ToTensor()\n        self.pixel_mean = torch.Tensor([123.675, 116.28, 103.53]).view(-1, 1, 1)\n        self.pixel_std = torch.Tensor([58.395, 57.12, 57.375]).view(-1, 1, 1)\n    def __call__(self, image, masks):\n        # Resize image and masks\n        og_h, og_w = image.height, image.width\n        image = self.transform.apply_image(image)\n        if masks is not None:\n            masks = [torch.tensor(self.transform.apply_image(mask)).float() for mask in masks]\n        # image = self.to_tensor(image)\n        image = torch.as_tensor(image)\n        image = image.permute(2, 0, 1).contiguous()\n        image = (image - self.pixel_mean) / self.pixel_std\n        # Pad image and masks to form a square\n        _, h, w = image.shape\n        max_dim = max(w, h)\n        pad_w = (max_dim - w) // 2\n        pad_h = (max_dim - h) // 2"
        },
        {
            "comment": "Applies padding to the image and masks, adjusts bounding boxes if masks are not None, and returns the padded image, masks, and a tensor with the original image height and width.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/transforms.py\":139-148",
            "content": "        padding = (pad_w, pad_h, max_dim - w - pad_w, max_dim - h - pad_h)\n        image = transforms.Pad(padding)(image)\n        if masks is not None:\n            masks = [transforms.Pad(padding)(mask) for mask in masks]\n        # Adjust bounding boxes\n        # bboxes = self.transform.apply_boxes(bboxes, (og_h, og_w))\n        # bboxes = [[bbox[0] + pad_w, bbox[1] + pad_h, bbox[2] + pad_w, bbox[3] + pad_h] for bbox in bboxes]\n        return image, masks, torch.tensor([h, w], dtype=torch.float)"
        }
    ]
}