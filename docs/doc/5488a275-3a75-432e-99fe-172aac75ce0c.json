{
    "summary": "The code introduces two custom PyTorch dataset classes, \"ConcatDataset\" and \"InterleaveDataset\", with stop strategies and probabilities. It undersamples by interleaving datasets based on their lengths and creates infinite iterators for sampling and oversampling using NumPy's random.default_rng(). SubSet and ConcatDatasetWithShuffle classes are defined for subset selection and dataset concatenation with shuffling.",
    "details": [
        {
            "comment": "The code defines a class called \"ConcatDataset\" that concatenates multiple datasets together using the PyTorch's TorchConcatDataset. This class also provides methods to get the length of the dataset, access individual data points, and represents itself in a readable format.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/utils/concatenate_dataset.py\":0-33",
            "content": "from typing import List, Optional, Literal\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import ConcatDataset as TorchConcatDataset\nfrom torch.utils.data import Subset as TorchSubset\nfrom ..root import DATASETS\n@DATASETS.register_module()\nclass ConcatDataset(Dataset):\n    _repr_indent = 4\n    def __init__(self, cfgs):\n        self.cfgs = cfgs\n        datasets = [DATASETS.build(cfg) for cfg in cfgs]\n        self.concat_dataset = TorchConcatDataset(datasets)\n    def __len__(self):\n        return len(self.concat_dataset)\n    def __getitem__(self, index):\n        return self.concat_dataset[index]\n    def __repr__(self) -> str:\n        head = \"Dataset \" + self.__class__.__name__\n        body = [\n            f\"Number of datapoints: {self.__len__()}\",\n        ]\n        for i, ds in enumerate(self.concat_dataset.datasets):\n            body.append(f\"Subset {i + 1}/{len(self.concat_dataset.datasets)}\")\n            body += ds.__repr__().splitlines()\n        lines = [head] + [\" \" * self._repr_indent + line for line in body]"
        },
        {
            "comment": "This code defines a custom dataset class, InterleaveDataset, that interleaves multiple datasets. It initializes the class with a list of configs, optional probabilities for interleaving, and an optional seed value for randomization. The stopping strategy can be either \"first_exhausted\" or \"all_exhausted\". It builds the individual datasets using the DATASETS module, then concatenates them into a TorchConcatDataset object. The index mapping is determined by _interleave_dataset_index function based on dataset lengths, probabilities, seed, and stopping strategy. The class provides length, getitem, and repr methods for data access and representation.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/utils/concatenate_dataset.py\":34-69",
            "content": "        return \"\\n\".join(lines)\n@DATASETS.register_module()\nclass InterleaveDateset(Dataset):\n    _repr_indent = 4\n    def __init__(\n            self,\n            cfgs,\n            probabilities: Optional[List[float]] = None,\n            seed: Optional[int] = 42,\n            stopping_strategy: Literal[\"first_exhausted\", \"all_exhausted\"] = \"first_exhausted\",\n    ):\n        self.cfgs = cfgs\n        self.probabilities = probabilities\n        self.seed = seed\n        self.stopping_strategy = stopping_strategy\n        datasets = [DATASETS.build(cfg) for cfg in cfgs]\n        self.concat_dataset = TorchConcatDataset(datasets)\n        self.index_mapping = _interleave_dataset_index(\n            lengths=[len(ds) for ds in datasets],\n            probabilities=probabilities,\n            seed=seed,\n            stopping_strategy=stopping_strategy,\n        )\n    def __len__(self):\n        return len(self.index_mapping)\n    def __getitem__(self, index):\n        return self.concat_dataset[self.index_mapping[index]]\n    def __repr__(self) -> str:"
        },
        {
            "comment": "This function generates a string representation of a dataset. It includes the dataset's name, number of datapoints, probabilities, stopping strategy, and information about its subsets. The code is borrowed from the Hugging Face datasets library, specifically the arrow_dataset.py file.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/utils/concatenate_dataset.py\":70-93",
            "content": "        head = \"Dataset \" + self.__class__.__name__\n        body = [\n            f\"Number of datapoints: {self.__len__()}\",\n            f\"Probabilities: {self.probabilities}\",\n            f\"stopping_strategy: {self.stopping_strategy}\",\n            f\"seed: {self.seed}\",\n        ]\n        for i, ds in enumerate(self.concat_dataset.datasets):\n            body.append(f\"Subset {i + 1}/{len(self.concat_dataset.datasets)}\")\n            body += ds.__repr__().splitlines()\n        lines = [head] + [\" \" * self._repr_indent + line for line in body]\n        return \"\\n\".join(lines)\n# stolen from huggingface/datasets\n# https://github.com/huggingface/datasets/blob/074925b9b7c1dfd33b8675aa99c07cc26375665c/src/datasets/arrow_dataset.py#L5987\ndef _interleave_dataset_index(\n        *,\n        lengths: List[int],\n        probabilities: Optional[List[float]] = None,\n        seed: Optional[int] = None,\n        stopping_strategy: Literal[\"first_exhausted\", \"all_exhausted\"] = \"first_exhausted\",\n):\n    if probabilities is not None and 0 in probabilities:"
        },
        {
            "comment": "The code is performing undersampling by cycling through each source dataset based on their lengths, resulting in a new sequence of indices that interleave the datasets. This ensures all sources are included in the output, and the order of the data is determined by the minimum length dataset.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/utils/concatenate_dataset.py\":94-110",
            "content": "        assert stopping_strategy == 'first_exhausted', \"you will meet a Infinite loop\"\n    # Let's now build the indices to pass to .select()\n    offsets = np.cumsum([0] + lengths[:-1])\n    # if stopping_strategy is \"first_exhausted\", it is an undersampling situation whereas it is an oversampling situation if it is \"all_exhausted\"\n    oversampling = stopping_strategy == \"all_exhausted\"\n    if probabilities is None and not oversampling:\n        # Undersampling situation with cycling between each sources\n        # Example:: If lengths of the datasets are [3, 4, 5]\n        # Then the resulting indices should be [0, 3, 7, 1, 4, 8, 2, 6, 9]\n        # Note that we only have 3 examples per dataset since the first dataset ran out of examples\n        # Reasoning behind the following operation: keeping the min_length first indices of each dataset\n        # while offsetting in order to correspond to the right indices of the concatenated dataset\n        # and flattening to effectively interleave the datasets\n "
        },
        {
            "comment": "Code handles three different scenarios for creating indices to concatenate datasets. If probabilities are not None, it uses oversampling by cycling between each source dataset. If probabilities are None and lengths are equal, it simply creates a flat list of indices. In the last scenario, it checks if each dataset has been fully exhausted using a boolean array.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/utils/concatenate_dataset.py\":110-124",
            "content": "       indices = (offsets.reshape(1, -1) + np.arange(min(lengths)).reshape(-1, 1)).flatten().tolist()\n    elif probabilities is None:\n        # Oversampling situation with cycling between each sources\n        # Then the resulting indices should be [0, 3, 7, 1, 4, 8, 2, 5, 9, 0, 6, 10, 1, 3, 11]\n        # Note that we have 5 examples per dataset with a rolling window since the longest dataset has 5 samples\n        # Reasoning behind the following operation: for each dataset indices (i.e column) repeat the indices to have max_length indices per dataset\n        # For example, if the max_length is 5 and the i-th dataset has 3 samples, the i-th column will be [0,1,2,0,1]\n        indices = np.mod(np.arange(max(lengths)).reshape(-1, 1), np.array(lengths).reshape(1, -1))\n        # We have to keep the indices to their respective dataset offsets and to flatten to effectively interleave the datasets\n        indices = (indices + offsets).flatten().tolist()\n    else:\n        # boolean array indicating if at index i if the dataset_i has been fully exhausted"
        },
        {
            "comment": "This code generates an infinite iterator that randomly samples indices from datasets based on the specified strategy (first_exhausted or all_exhausted). It uses NumPy's random.default_rng() to generate random indices and tracks dataset exhaustion using is_exhausted boolean array. The function yields these indices, allowing for oversampling if specified by all_exhausted parameter, otherwise stopping when one dataset runs out of examples with first_exhausted.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/utils/concatenate_dataset.py\":125-142",
            "content": "        is_exhausted = np.full(len(lengths), False)\n        # if undersampling (\"first_exhausted\"), we stop as soon as one dataset is exhausted\n        # if oversampling (\"all_exhausted\"), we stop as soons as every dataset is exhausted, i.e as soon as every samples of every dataset has been visited at least once\n        bool_strategy_func = np.all if oversampling else np.any\n        def iter_random_indices():\n            \"\"\"Get an infinite iterator that randomly samples the index of the source to pick examples from.\"\"\"\n            rng = np.random.default_rng(seed)\n            while True:\n                yield from (int(i) for i in rng.choice(len(lengths), size=1000, p=probabilities))\n        current_index = [0] * len(lengths)\n        indices = []\n        for source_idx in iter_random_indices():\n            # If no oversampling, we stop as soon as a dataset has ran out of examples (np.any)\n            # Otherwise, we stop as soon as every dataset has ran out of examples (np.all)\n            if bool_strategy_func(is_exhausted):"
        },
        {
            "comment": "This code defines a class SubSet that inherits from TorchSubset. It takes a configuration, portion of the data to use, whether to shuffle and seed for randomization. It creates an instance of the specified dataset, calculates the target length based on portion, if shuffling is enabled it shuffles indices using np.random.default_rng and selects the first N indices to create a subset of the data with the specified portion.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/utils/concatenate_dataset.py\":143-167",
            "content": "                # the stopping condition was reached, let's stop\n                break\n            # let's add the example at the current index of the `source_idx`-th dataset\n            indices.append(current_index[source_idx] + offsets[source_idx])\n            current_index[source_idx] += 1\n            # we've ran out of examples for the current dataset, let's update our boolean array and bring the current_index back to 0\n            if current_index[source_idx] >= lengths[source_idx]:\n                is_exhausted[source_idx] = True\n                current_index[source_idx] = 0\n    return indices\n@DATASETS.register_module()\nclass SubSet(TorchSubset):\n    def __init__(self, cfg, portion, do_shuffle=True, seed=42):\n        assert 0 < portion <= 1\n        dataset = DATASETS.build(cfg=cfg)\n        target_len = int(len(dataset) * portion)\n        if do_shuffle:\n            rng = np.random.default_rng(seed)\n            indices = list(range(len(dataset)))\n            rng.shuffle(indices)\n            indices = indices[:target_len]"
        },
        {
            "comment": "This code defines a ConcatDatasetWithShuffle class, which concatenates multiple datasets based on provided configurations and then shuffles the indices to create a new dataset. The portion parameter determines the proportion of data to include from the concatenated dataset.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/utils/concatenate_dataset.py\":168-191",
            "content": "        else:\n            indices = list(range(target_len))\n        super().__init__(dataset, indices)\n@DATASETS.register_module()\nclass ConcatDatasetWithShuffle(TorchSubset):\n    _repr_indent = 4\n    def __init__(self, cfgs, seed=42, portion=1):\n        self.cfgs = cfgs\n        self.seed = seed\n        self.portion = portion\n        dataset = TorchConcatDataset([DATASETS.build(cfg) for cfg in cfgs])\n        target_len = int(len(dataset) * portion)\n        indices = list(range(len(dataset))) * int(np.ceil(portion))\n        rng = np.random.default_rng(seed)\n        rng.shuffle(indices)\n        indices = indices[:target_len]\n        super().__init__(dataset, indices)\n__all__ = ['ConcatDataset', 'InterleaveDateset', 'SubSet', 'ConcatDatasetWithShuffle']"
        }
    ]
}