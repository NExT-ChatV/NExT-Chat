{
    "summary": "The code creates a custom collator for data handling, defines prediction steps, calculates metrics, performs loss and evaluation, saves results, checks parallelism, deepcopies variables, and plots training loss. It also processes images, updates return dictionary with image outputs, adds tensors, and returns the updated dictionary as output.",
    "details": [
        {
            "comment": "The code imports necessary libraries, sets up logging, and defines a class for different collators in a trainer. The TrainerDifferentCollatorMixin class takes optional arguments for train_collator, eval_collator, and test_collator. It inherits from Seq2SeqTrainer to provide custom collation logic for training, evaluation, and testing data.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/engine/base_engine.py\":0-32",
            "content": "import os\nimport sys\nimport json\nimport logging\nimport warnings\nfrom copy import deepcopy\nfrom typing import Any, Dict, List, Optional, Tuple, Union, Sequence, Mapping\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nfrom transformers import Seq2SeqTrainer, DataCollator, DataCollatorForSeq2Seq\nfrom transformers.deepspeed import is_deepspeed_zero3_enabled\nfrom transformers.trainer import TRAINER_STATE_NAME\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\nlogging.basicConfig(\n    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n    datefmt=\"%m/%d/%Y %H:%M:%S\",\n    handlers=[logging.StreamHandler(sys.stdout), ],\n)\nclass TrainerDifferentCollatorMixin:\n    def __init__(self,\n                 *args,\n                 train_collator: Optional[DataCollator] = None,\n                 eval_collator: Optional[DataCollator] = None,\n                 test_collator: Optional[DataCollator] = None,\n                 **kwargs):"
        },
        {
            "comment": "This code raises a ValueError if no collator is provided for the trainer. It also issues a warning if different collators are provided for eval and test, as it's possible that only the test_collator is being used. If \"data_collator\" is passed in as an argument but has a non-null value, it will be ignored with a warning.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/engine/base_engine.py\":33-43",
            "content": "        if train_collator is None and eval_collator is None and test_collator is None:\n            raise ValueError(\"use different collator for trainer but get no collator function.\")\n        if eval_collator is not None and test_collator is not None and eval_collator != test_collator:\n            warnings.warn('[WARNING!!!] use different collator for eval and test. but maybe do_eval and '\n                          'do_predict both use trainer.predict (i.e. only test_collator is used.) u should'\n                          'check your code and know exactly what u are doing.')\n        self._train_collator = train_collator\n        self._eval_collator = eval_collator if eval_collator is not None else self._train_collator\n        self._test_collator = test_collator if test_collator is not None else self._eval_collator\n        if \"data_collator\" in kwargs and kwargs[\"data_collator\"] is not None:\n            warnings.warn(\"use different collator for trainer but get 'data_collator' argument. It will take no effect and be ignored.\")"
        },
        {
            "comment": "This code overrides the `get_train_dataloader`, `get_eval_dataloader`, and `get_test_dataloader` methods to temporarily replace the data collator with custom ones for training, evaluation, and testing respectively before calling the parent class's implementation and restoring the original data collator.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/engine/base_engine.py\":44-66",
            "content": "        super().__init__(*args, **kwargs)\n    # noinspection PyAttributeOutsideInit,PyUnresolvedReferences\n    def get_train_dataloader(self) -> DataLoader:\n        old_collator = self.data_collator\n        self.data_collator = self._train_collator\n        dataloader = super().get_train_dataloader()\n        self.data_collator = old_collator\n        return dataloader\n    # noinspection PyAttributeOutsideInit,PyUnresolvedReferences\n    def get_eval_dataloader(self, eval_dataset: Optional[Dataset] = None) -> DataLoader:\n        old_collator = self.data_collator\n        self.data_collator = self._eval_collator\n        dataloader = super().get_eval_dataloader(eval_dataset)\n        self.data_collator = old_collator\n        return dataloader\n    # noinspection PyAttributeOutsideInit,PyUnresolvedReferences\n    def get_test_dataloader(self, test_dataset: Dataset) -> DataLoader:\n        old_collator = self.data_collator\n        self.data_collator = self._test_collator\n        dataloader = super().get_test_dataloader(test_dataset)"
        },
        {
            "comment": "The code defines a TrainerForMMLLM class that extends Seq2SeqTrainer and implements TrainerDifferentCollatorMixin. It has a _prepare_inputs method to prepare inputs for the model by converting them into tensors if necessary, handling potential states, and adding past memory if needed. The code also includes an optional print statement for the learning rate lambdas and the last epoch of the LR scheduler when in local process zero.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/engine/base_engine.py\":67-89",
            "content": "        self.data_collator = old_collator\n        return dataloader\n# noinspection DuplicatedCode\nclass TrainerForMMLLM(TrainerDifferentCollatorMixin, Seq2SeqTrainer):\n    def _prepare_inputs(self, inputs: Dict[str, Union[torch.Tensor, Any]]) -> Dict[str, Union[torch.Tensor, Any]]:\n        \"\"\"\n        Prepare `inputs` before feeding them to the model, converting them to tensors if they are not already and\n        handling potential state.\n        \"\"\"\n        inputs = self._prepare_input(inputs)\n        if len(inputs) == 0:\n            raise ValueError(\n                \"The batch received was empty, your model won't be able to train on it. Double-check that your \"\n                f\"training dataset contains keys expected by the model: {','.join(self._signature_columns)}.\"\n            )\n        if self.args.past_index >= 0 and self._past is not None:\n            inputs[\"mems\"] = self._past\n        # if self.is_local_process_zero():\n        #     print(self.lr_scheduler.lr_lambdas[0])\n        #     print(self.lr_scheduler.last_epoch,"
        },
        {
            "comment": "This code defines a `prediction_step` function that performs prediction using the given model and inputs. It checks if predictions should be made with generation or not, and if necessary, it calls the superclass implementation. If generation is required, it prepares inputs and updates generation keyword arguments before returning the output.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/engine/base_engine.py\":90-113",
            "content": "        #           self.lr_scheduler.lr_lambdas[0](self.lr_scheduler.last_epoch),\n        #           not self.accelerator.optimizer_step_was_skipped)\n        return inputs\n    def prediction_step(\n            self,\n            model: nn.Module,\n            inputs: Dict[str, Union[torch.Tensor, Any]],\n            prediction_loss_only: bool,\n            ignore_keys: Optional[List[str]] = None,\n    ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n        # Override to inject custom behavior.\n        # noinspection PyUnresolvedReferences\n        if not self.args.predict_with_generate or prediction_loss_only:\n            return super().prediction_step(\n                model, inputs, prediction_loss_only=prediction_loss_only, ignore_keys=ignore_keys\n            )\n        has_labels = \"labels\" in inputs\n        inputs = self._prepare_inputs(inputs)\n        gen_kwargs = self._gen_kwargs.copy()\n        if gen_kwargs.get(\"max_length\") is None and gen_kwargs.get(\"max_new_tokens\") is None:"
        },
        {
            "comment": "This code sets default values for \"max_length\", \"num_beams\", and \"synced_gpus\" in the generate function. It also filters unnecessary keys, ensuring only necessary inputs are used. If specific conditions are met, it generates text using the model's `generate_rec` method.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/engine/base_engine.py\":114-133",
            "content": "            gen_kwargs[\"max_length\"] = self.model.config.max_length\n        gen_kwargs[\"num_beams\"] = (\n            gen_kwargs[\"num_beams\"] if gen_kwargs.get(\"num_beams\") is not None else self.model.config.num_beams\n        )\n        default_synced_gpus = True if is_deepspeed_zero3_enabled() else False\n        gen_kwargs[\"synced_gpus\"] = (\n            gen_kwargs[\"synced_gpus\"] if gen_kwargs.get(\"synced_gpus\") is not None else default_synced_gpus\n        )\n        # filter keys\n        filter_keys = [\"labels\"]\n        for k in inputs:\n            if not (k in filter_keys):\n                gen_kwargs[k] = inputs[k]\n        self._logging_generate_kwargs(gen_kwargs.keys())\n        with torch.inference_mode():\n            with self.compute_loss_context_manager():\n                if len(inputs[\"loc_targets\"])==len(inputs[\"loc_inputs\"]) and len(inputs['loc_targets'])>0 and \"masks_sam\" not in inputs: # eval rec\n                    generated_tokens = self.model.generate_rec(**gen_kwargs)\n                    # generated_tokens = self.tensor2token(generated_tokens)"
        },
        {
            "comment": "This code is part of a base engine for a language model. It generates tokens using the model and checks if the input contains certain values. The code also suppresses warnings related to generation config, extracts generated tokens, and retrieves GenerationConfig from model.generation_config.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/engine/base_engine.py\":134-153",
            "content": "                else:\n                    generated_tokens = self.model.generate(**gen_kwargs)\n                    if \"masks_sam\" not in inputs and type(generated_tokens) is tuple:\n                        generated_tokens = generated_tokens[0]\n                    # generated_tokens = self.tensor2token(generated_tokens)\n        # TODO: rewrite official seq2seq_trainer to suppress generation_config warning\n        if self.model.generation_config._from_model_config:\n            self.model.generation_config._from_model_config = False\n        # important for Decoder-Only LLM: only extract generated_tokens and discard origin inputs\n        generation_inputs = inputs['input_ids']\n        # generated_tokens = generated_tokens[:, generation_inputs.size()[-1]:]\n        if self.model.generation_config._from_model_config:\n            self.model.generation_config._from_model_config = False\n        # Retrieves GenerationConfig from model.generation_config\n        gen_config = self.model.generation_config\n        # in case the batch is shorter than max length, the output should be padded"
        },
        {
            "comment": "This code snippet checks if the generated tokens' length is less than the maximum allowed length or if it's less than the sum of max_new_tokens and 1. If either condition is true, it pads the generated tokens to match the max_length or max_new_tokens+1 length. If prediction_loss_only is True, it returns loss, None, None. If \"masks_sam\" exists in inputs (indicating res segmentation eval), it compares generated_tokens[1] with gt_masks and assigns the pred_masks accordingly. Finally, it transforms pred_masks using de_transform_mask function.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/engine/base_engine.py\":154-174",
            "content": "        # if generated_tokens.shape[-1] < gen_config.max_length:\n        #     generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_config.max_length)\n        # elif gen_config.max_new_tokens is not None and generated_tokens.shape[-1] < gen_config.max_new_tokens + 1:\n        #     generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_config.max_new_tokens + 1)\n        loss = None\n        if self.args.prediction_loss_only:\n            return loss, None, None\n        if has_labels:\n            if \"masks_sam\" in inputs: # res segmentation eval\n                gt_masks = inputs[\"masks_sam\"]\n                gt_masks = torch.stack(gt_masks, 0)\n                if generated_tokens[1] is None:\n                    pred_masks = gt_masks.clone()\n                    pred_masks[:] = 0\n                    print(\"fail\")\n                else:\n                    pred_masks = generated_tokens[1]\n                pred_masks = self.de_transform_mask(inputs[\"img_size\"][0, 1], inputs[\"img_size\"][0, 0], pred_masks)"
        },
        {
            "comment": "This code performs evaluation metrics calculation for generated tokens based on the input type. It checks if there is a match in size between predicted and ground truth masks, then calculates intersection and union of these masks to determine the generated tokens and labels. If the input is bounding box or image2text task, it assigns respective labels.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/engine/base_engine.py\":175-192",
            "content": "                gt_masks = inputs[\"unresized_masks\"][0].unsqueeze(0)\n                if (pred_masks.size()!=gt_masks.size()):\n                    pred_masks = gt_masks.clone()\n                    pred_masks[:] = 0\n                    print(\"unmatched\")\n                intersection = torch.sum(torch.mul(pred_masks, gt_masks), dim=(1, 2))\n                union = torch.sum(pred_masks, dim=(1, 2)) + torch.sum(gt_masks, dim=(1, 2)) - intersection\n                generated_tokens = intersection\n                labels = union\n            elif len(inputs[\"loc_targets\"]) > 0: # rec bounding box eval\n                # labels = self.tensor2token(inputs[\"loc_targets\"])\n                labels = inputs[\"loc_targets\"]\n            else: # image2text gen task eval\n                labels = inputs[\"labels\"]\n            # if labels.shape[-1] < gen_config.max_length:\n            #     labels = self._pad_tensors_to_max_len(labels, gen_config.max_length)\n            # elif gen_config.max_new_tokens is not None and labels.shape[-1] < gen_config.max_new_tokens + 1:"
        },
        {
            "comment": "The code defines a base engine class for a machine learning model, including functions for handling generated tokens and labels, transforming masks, and converting tensor lists to tokens. The `_pad_tensors_to_max_len` function pads tensors to the maximum length defined by the `gen_config`. The `de_transform_mask` function resizes a mask to fit the longest side of an original image, then applies binary mask operations and padding as necessary. Finally, the `tensor2token` function converts tensor lists into tokens using a tokenizer, handling padding for longest sequences.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/engine/base_engine.py\":193-217",
            "content": "            #     labels = self._pad_tensors_to_max_len(labels, gen_config.max_new_tokens + 1)\n        else:\n            labels = None\n        assert len(generated_tokens) == len(labels)\n        return loss, generated_tokens, labels\n    def de_transform_mask(self, orgw, orgh, mask):\n        long_side = max(orgw, orgh)\n        short_side = min(orgw, orgh)\n        pad = (long_side - short_side) // 2\n        mask = F.interpolate(mask, [long_side, long_side], mode=\"bilinear\", align_corners=False)\n        mask = mask > 0\n        mask[mask > 0] = 1\n        mask[mask<=0] = 0\n        if orgw < orgh:\n            mask = mask[..., :, pad: short_side + pad]\n        else:\n            mask = mask[..., pad: short_side + pad, :]\n        # mask = mask.transpose(2, 3)\n        # print(mask.shape)\n        return mask.squeeze(1)\n    def tensor2token(self, tensor_list):\n        lst = [str(tensor.cpu().tolist()) for tensor in tensor_list]\n        tokens = self.tokenizer(lst, return_tensors=\"pt\", add_special_tokens=False, padding=\"longest\")"
        },
        {
            "comment": "This code appears to be part of a machine learning model's base engine. It defines a method for returning input tensor device, sets logging generation kwargs if they change, and saves prediction results in numpy format to the output directory. The code also checks if it's the world process zero (possibly for parallelism) and deepcopies some variables.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/engine/base_engine.py\":218-239",
            "content": "        return tokens.input_ids.to(tensor_list[0].device)\n    def _logging_generate_kwargs(self, keys):\n        if not hasattr(self, '_generate_kwargs'):\n            self._generate_kwargs = None\n        if self._generate_kwargs != keys:\n            self._generate_kwargs = keys\n            logger.warning(f\"generate use kwargs: {keys}\")\n    def save_prediction(self, predict_results, file_key_prefix='predict'):\n        if not self.is_world_process_zero():\n            return\n        import numpy as np\n        os.makedirs(self.args.output_dir, exist_ok=True)\n        np.save(os.path.join(self.args.output_dir, f\"{file_key_prefix}_predictions.npy\"), predict_results.predictions)\n        np.save(os.path.join(self.args.output_dir, f\"{file_key_prefix}_label_ids.npy\"), predict_results.label_ids)\n        preds, targets = predict_results.predictions, predict_results.label_ids\n        origin_preds, origin_targets = preds, targets\n        preds, targets = deepcopy(preds), deepcopy(targets)\n        logger.warning(f\"preds shape: {preds.shape}. targets shape: {targets.shape}\")"
        },
        {
            "comment": "This code snippet saves prediction data from the model to a JSONL file. It creates an output directory if it doesn't exist, and then iterates through preds and targets, encoding them, decoding them, and saving them in a dictionary format with 'pred' and 'target' keys. The dictionary objects are then written to the JSONL file one by one.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/engine/base_engine.py\":241-258",
            "content": "        # decode text and save to json takes forever for big test set\n        os.makedirs(self.args.output_dir, exist_ok=True)\n        # with open(os.path.join(self.args.output_dir, f'{file_key_prefix}_extra_prediction.jsonl'), 'a', encoding=\"utf-8\") as g:\n        #     for p, t, pi, ti in tqdm(\n        #             zip(preds, targets, origin_preds, origin_targets),\n        #             total=len(preds), desc=f\"saving prediction for {file_key_prefix}\",\n        #     ):\n        #         p[p < 0] = self.tokenizer.pad_token_id\n        #         t[t < 0] = self.tokenizer.pad_token_id\n        #         p = self.tokenizer.decode(p, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n        #         t = self.tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n        #         obj = dict(\n        #             pred=p,\n        #             target=t,\n        #             # pred_id=pi.tolist(),\n        #             # target_id=ti.tolist(),\n        #         )\n        #         g.write(json.dumps(obj) + '\\n')"
        },
        {
            "comment": "This code saves the model in a specific way to avoid CUDA out of memory (OOM) errors when using transformers and FullyShardedDataParallel (FSDP). It offloads the state dictionary to CPU only for rank 0, then saves it if necessary. Additionally, it pushes the saved model to the Hub if not called internally.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/engine/base_engine.py\":259-278",
            "content": "        #         g.flush()\n    # transformers + FSDP + saving model -> cuda OOM for small memory gpu\n    # refer: https://github.com/tatsu-lab/stanford_alpaca/issues/65\n    def save_model(self, output_dir: Optional[str] = None, _internal_call: bool = False):\n        if self.fsdp is not None:\n            if output_dir is None:\n                output_dir = self.args.output_dir\n            from torch.distributed.fsdp import (\n                FullyShardedDataParallel as FSDP,\n                FullStateDictConfig,\n                StateDictType,\n            )\n            save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\n            with FSDP.state_dict_type(self.model, StateDictType.FULL_STATE_DICT, save_policy):\n                cpu_state_dict = self.model.state_dict()\n            if self.args.should_save:\n                self._save(output_dir, state_dict=cpu_state_dict)  # noqa\n            # Push to the Hub when `save_model` is called by the user.\n            if self.args.push_to_hub and not _internal_call:"
        },
        {
            "comment": "This code snippet saves a model and plots the training loss. If not the main process, it returns without doing anything. Else, it calls `save_model` and then plots the training loss for all steps in `log_history`. The plot is saved as \"trainer_state.png\" in the output directory with a title displaying the output directory.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/engine/base_engine.py\":279-301",
            "content": "                self.push_to_hub(commit_message=\"Model save\")\n        else:\n            super().save_model(output_dir, _internal_call)\n    def plot_loss(self) -> None:\n        if not self.is_world_process_zero():\n            return\n        training_args = self.args\n        FIGURE_NAME = \"trainer_state.png\"\n        import matplotlib.pyplot as plt\n        data = json.load(open(os.path.join(training_args.output_dir, TRAINER_STATE_NAME), \"r\"))\n        train_steps, train_losses = [], []\n        for i in range(len(data[\"log_history\"]) - 1):\n            train_steps.append(data[\"log_history\"][i][\"step\"])\n            train_losses.append(data[\"log_history\"][i][\"loss\"])\n        plt.figure()\n        plt.plot(train_steps, train_losses)\n        plt.title(\"training loss of {}\".format(training_args.output_dir))\n        plt.xlabel(\"step\")\n        plt.ylabel(\"training loss\")\n        plt.savefig(os.path.join(training_args.output_dir, FIGURE_NAME), format=\"png\", transparent=True, dpi=300)\n        print(\"Figure saved: {}\".format(os.path.join(training_args.output_dir, FIGURE_NAME)))"
        },
        {
            "comment": "The code defines a Seq2SeqDataCollator class, which is a subclass of DataCollatorForSeq2Seq. It has an inference_mode parameter that determines whether to use left or right padding for features. If inference mode is on, it uses left padding; otherwise, it uses right padding. The padding side is temporarily changed using the tokenizer's padding_side attribute, and then reverted back to its original value after collating the features.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/engine/base_engine.py\":304-326",
            "content": "class Seq2SeqDataCollator(DataCollatorForSeq2Seq):\n    def __init__(\n            self,\n            inference_mode: bool = False,\n            **kwargs,\n    ):\n        self.inference_mode = inference_mode\n        self.text_keys = ['input_ids', 'labels', 'attention_mask']\n        super().__init__(**kwargs)\n    def __call__(self, features: Sequence[Dict[str, Sequence]], return_tensors=None) -> Dict[str, torch.Tensor]:\n        # evaluation/inference adopts left-padding while training adopts right-padding\n        text_features = [{k: feature[k] for k in self.text_keys if k in feature} for feature in features]\n        if self.inference_mode:\n            old_padding_side = self.tokenizer.padding_side\n            self.tokenizer.padding_side = 'left'\n            text_features = super().__call__(text_features)\n            self.tokenizer.padding_side = old_padding_side\n        else:\n            old_padding_side = self.tokenizer.padding_side\n            self.tokenizer.padding_side = 'right'\n            text_features = super().__call__(text_features)"
        },
        {
            "comment": "This code initializes a data collator class that handles both text and image data. It uses a tokenizer to process text features, pads sequences based on the padding side, and includes a mock annotation for cases with untrained parameters. The `_image_process` method processes image features by stacking them into a tensor, and the `__call__` method checks for missing \"masks_sam\" in features and adds a mock annotation if necessary.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/engine/base_engine.py\":327-350",
            "content": "            self.tokenizer.padding_side = old_padding_side\n        return text_features\nclass Seq2Seq2DataCollatorWithImage(Seq2SeqDataCollator):\n    def __init__(self, preprocessor, **kwargs):\n        super().__init__(tokenizer=preprocessor['text'], **kwargs)\n        # sometimes there is either no location input or output in the current batch\n        # which will make some parameters untrained in the batch.\n        # use a mock annotation to prevent error\n        self.mock = torch.load(\"mock.pth\")\n    # noinspection PyMethodMayBeStatic\n    def _image_process(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n        images = [feature['image'] for feature in features]\n        images = torch.stack(images, dim=0)\n        ret = dict(images=images)\n        return ret\n    def __call__(self, features: List[Dict[str, Any]], return_tensors=None) -> Dict[str, Any]:\n        if not self.inference_mode and not (\"masks_sam\" in features[0]):\n            features.append(self.mock)\n        loc_inputs = [x['loc_inputs'] for x in features]"
        },
        {
            "comment": "This code performs image and mask processing for a model. It retrieves the features, processes the images using _image_process method, updates the return dictionary with image outputs, converts loc_targets to tensor, and optionally adds images_sam and masks_sam tensors if present in features. The img_size, unresized_masks are also added to the return dictionary if present. Finally, it returns the updated dictionary as output.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/engine/base_engine.py\":351-365",
            "content": "        loc_targets = [x['loc_targets'] for x in features]\n        ret = super().__call__(features, return_tensors)\n        image_outputs = self._image_process(features)\n        ret.update(image_outputs)\n        ret[\"loc_inputs\"] = torch.tensor([list(a) for b in loc_inputs for a in b])\n        ret[\"loc_targets\"] = torch.tensor([list(a) for b in loc_targets for a in b])\n        if \"images_sam\" in features[0]:\n            ret['images_sam'] = torch.stack([f[\"images_sam\"] for f in features], dim=0)\n        if \"masks_sam\" in features[0]:\n            ret['masks_sam'] = [torch.stack(f[\"masks_sam\"], 0) for f in features]\n            ret['img_size'] = torch.stack([f[\"img_size\"] for f in features], 0)\n            ret['unresized_masks'] = [f[\"unresized_masks\"] for f in features]\n            # ret['masks_sam'] = [y for x in ret[\"masks_sam\"] for y in x]\n        return ret"
        }
    ]
}