{
    "summary": "This code creates a language modeling class that utilizes 'vision_tower' for data processing, segmentation loss generation, and includes methods for model outputs, generation, and vision tower detection. It prepares chat model inputs with location info in the vision tower layer, handles input preprocessing using `inputs_embeds` and checks `past_key_values`.",
    "details": [
        {
            "comment": "This code defines a class `NextChatForSegLM` that inherits from `NextChatForCausalLM`. It initializes an instance of `SamForLMSeg`, `SamLoss` and sets the prompt MLP. The class overrides the forward function to integrate the `SamModel` and `SamLoss` for segmented language modeling.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_seg.py\":0-27",
            "content": "from typing import Optional, List, Union, Tuple\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers import LlamaConfig\nfrom mllm.models.nextchat.nextchat_base import NextChatForCausalLM, NextChatConfig\nfrom mllm.models.sam.modeling_sam import SamForLMSeg\nfrom mllm.models.sam.sam_loss import SamLoss\nclass NextChatForSegLM(NextChatForCausalLM):\n    def __init__(self, config: NextChatConfig):\n        super(NextChatForSegLM, self).__init__(config)\n        self.sam = SamForLMSeg(\"vit_h\", config.sam_path)\n        self.sam_loss = SamLoss()\n        self.sam_prompt_dim = self.sam.model.prompt_encoder.embed_dim\n        self.seg_prompt_mlp = nn.Sequential(\n            nn.Linear(self.model.config.hidden_size, self.model.config.hidden_size),\n            nn.ReLU(),\n            nn.Linear(self.model.config.hidden_size, self.sam_prompt_dim*4)\n            )\n    def forward(\n            self,\n            input_ids: torch.LongTensor = None,"
        },
        {
            "comment": "This code defines a function that takes various inputs and returns output from a model. It uses the 'vision_tower' to process data, handles labels by setting specific values to -100, and allows for optional control of attention mask, past key values, input embeddings, etc. The output_attentions parameter determines whether or not to return attention weights.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_seg.py\":28-47",
            "content": "            attention_mask: Optional[torch.Tensor] = None,\n            past_key_values: Optional[List[torch.FloatTensor]] = None,\n            inputs_embeds: Optional[torch.FloatTensor] = None,\n            labels: Optional[torch.LongTensor] = None,\n            use_cache: Optional[bool] = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            images: Optional[torch.FloatTensor] = None,\n            images_sam: Optional[torch.FloatTensor] = None,\n            masks_sam: Optional[torch.Tensor] = None,\n            return_dict: Optional[bool] = None,\n            loc_inputs=None,\n            loc_targets=None, # mask\n            **kwargs,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        vision_tower = self.model.get_vision_tower()\n        if labels is not None:\n            labels[labels==vision_tower.config.box_token] = -100\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions"
        },
        {
            "comment": "Code snippet from NExT-Chat/mllm/models/nextchat/nextchat_seg.py, lines 48-75, is responsible for processing input data in a transformer model, generating hidden states and logits for language modeling, and handling optional location embeddings. It returns the hidden states, computes loss based on the model's configuration, and allows for customization of return values and usage of cache.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_seg.py\":48-75",
            "content": "        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        # TODO change to loc_inputs\n        loc_embeds = None\n        if loc_inputs is not None and len(loc_inputs) > 0:\n            loc_embeds = self.loc_encoder(loc_inputs)\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=True,\n            return_dict=return_dict,\n            images=images,\n            loc_embeds=loc_embeds,\n        )\n        hidden_states = outputs[0]\n        logits = self.lm_head(hidden_states)\n        loss = 0"
        },
        {
            "comment": "This code checks if the `loc_targets` is not empty, then selects the last hidden states from the model's outputs, flattens them, and extracts the positions where the input IDs match a specified token and labels are greater than zero. These selected hidden states are used to predict locations with a decoder, prompt states are generated using an MLP layer, and the predicted masks and IOU values are computed by a SAM module. The segmentation loss is added to the overall loss, which is returned along with logits and past key values.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_seg.py\":76-93",
            "content": "        if loc_targets is not None and len(loc_targets) > 0:\n            last_hidden_states = outputs.hidden_states[-1]\n            last_hidden_states = last_hidden_states.view(-1, last_hidden_states.size(-1))\n            loc_positions = ( (input_ids.flatten() == vision_tower.config.at_token)\n                             & (labels.flatten()>0) ).nonzero().flatten()\n            selected_hidden_states = last_hidden_states[loc_positions]\n            pred_locs = self.loc_decoder(selected_hidden_states)*1024\n            prompt_states = self.seg_prompt_mlp(selected_hidden_states)\n            prompt_states = prompt_states.view(prompt_states.size(0), -1, self.sam_prompt_dim)\n            pred_masks, iou_predictions = self.sam(images_sam, prompt_states, pred_locs)\n            seg_loss = self.sam_loss(pred_masks, masks_sam, iou_predictions, last_hidden_states.device)\n            loss += seg_loss\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,"
        },
        {
            "comment": "This code defines a class with methods for model outputs, generation, and potentially image processing. The model takes input_ids, attention_mask, and images as arguments. It also has a loc_encoder that may embed location inputs if provided. If box tokens are found in the input_ids, it uses the loc_embeds from the loc_encoder to represent those locations. If no box tokens are found, loc_embeds is set to None. The dtype of the input is determined from the model's vision tower and patch embedding weight.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_seg.py\":94-122",
            "content": "            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n    def generate(\n        self,\n        inputs= None,\n        generation_config= None,\n        logits_processor= None,\n        stopping_criteria= None,\n        prefix_allowed_tokens_fn= None,\n        synced_gpus= None,\n        streamer= None,\n        images_sam= None,\n        **kwargs,\n    ):\n        dtype = self.model.vision_tower.vision_model.embeddings.patch_embedding.weight.dtype\n        input_ids = kwargs[\"input_ids\"]\n        attention_mask = kwargs[\"attention_mask\"]\n        images = kwargs[\"images\"]\n        loc_inputs = kwargs.pop(\"loc_inputs\", \"None\")\n        loc_embeds = None\n        if loc_inputs is not None and len(loc_inputs)>0:\n            loc_embeds = self.loc_encoder(loc_inputs.type(dtype))\n            vision_tower = self.model.get_vision_tower()\n            num = (input_ids==vision_tower.config.box_token).sum()\n            loc_embeds = loc_embeds[:num]\n            if num == 0:\n                loc_embeds = None"
        },
        {
            "comment": "This code snippet is from the NextChat model, which is a language model for open-ended text generation. It encodes input embeddings and generates new tokens using various parameters like beam size, maximum length, and temperature. The generated outputs include hidden states and scores.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_seg.py\":124-146",
            "content": "        orig_embeds_params = getattr(self.model, 'orig_embeds_params', None)\n        input_embeds = self.model.encode_input_embeds(input_ids, images.type(dtype), loc_embeds,\n                                                      orig_embeds_params, inputs_embeds=None)\n        outputs = super(NextChatForCausalLM, self).generate(\n            inputs_embeds=input_embeds,\n            attention_mask=attention_mask,\n            max_new_tokens=kwargs.pop(\"max_new_tokens\", 1024),\n            # stopping_criteria=stopping_criteria,\n            num_beams=kwargs.pop(\"num_beams\", 5),\n            min_length=1,\n            top_p=kwargs.get(\"top_p\", 0.8),\n            repetition_penalty=1.0,\n            length_penalty=1,\n            temperature=kwargs.get(\"temperature\", 0.75),\n            return_dict_in_generate=True,\n            output_hidden_states=True,\n            output_scores=True,\n            top_k=kwargs.get(\"top_k\", 5),\n        )\n        loc_hidden_states = []\n        if hasattr(outputs, \"beam_indices\"): # beam size > 1"
        },
        {
            "comment": "This code checks if the model output contains a specific token indicating the presence of a vision tower. If found, it assigns box tokens to sequences and extracts hidden states related to those locations. It handles both beam search scenarios (beam_size > 1 or beam_size == 1).",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_seg.py\":147-166",
            "content": "            vision_tower = self.model.get_vision_tower()\n            loc_ids = (outputs.sequences == vision_tower.config.at_token).nonzero()\n            hidden_states = outputs.hidden_states\n            beam_indices = outputs.beam_indices\n            for lid in loc_ids:\n                # assign to box\n                outputs.sequences[lid[0], lid[1]+1] = vision_tower.config.box_token\n                beam_idx = beam_indices[lid[0], lid[1]]\n                loc_h = hidden_states[lid[1]][-1][beam_idx]\n                loc_hidden_states.append(loc_h.squeeze())\n            if len(loc_hidden_states) > 0:\n                loc_hidden_states = torch.stack(loc_hidden_states)\n        else: # beam_size == 1\n            vision_tower = self.model.get_vision_tower()\n            loc_ids = (outputs.sequences == vision_tower.config.at_token).nonzero()\n            hidden_states = outputs.hidden_states\n            for lid in loc_ids:\n                outputs.sequences[lid[0], lid[1]+1] = vision_tower.config.box_token\n                loc_h = hidden_states[lid[1]][-1]"
        },
        {
            "comment": "This function prepares inputs for the generation process in a model. It takes input_ids, images_sam (optional), loc_inputs (optional), past_key_values (optional), attention_mask (optional) and additional keyword arguments. The function checks if images_sam and loc_inputs are not empty then it generates predictions by calling self.loc_decoder to predict the location decoding. It also passes these predicted locations, prompt states, and images through the model's self.sam to generate masks and IOU (Intersection over Union) predictions. The function returns sequences, pred_masks, iou_predictions, and pred_locs.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_seg.py\":167-185",
            "content": "                loc_hidden_states.append(loc_h.squeeze())\n            if len(loc_hidden_states) > 0:\n                loc_hidden_states = torch.stack(loc_hidden_states)\n        pred_masks, pred_locs, iou_predictions = None, None, None\n        if len(loc_hidden_states)>0:\n            loc_hidden_states = loc_hidden_states.type(dtype)\n            pred_locs = self.loc_decoder(loc_hidden_states)\n            prompt_states = self.seg_prompt_mlp(loc_hidden_states)\n            prompt_states = prompt_states.view(prompt_states.size(0), -1, self.sam_prompt_dim)\n            dtype = self.sam.model.image_encoder.patch_embed.proj.weight.dtype\n            if images_sam is not None:\n                pred_masks, iou_predictions = self.sam(images_sam.type(dtype), prompt_states, boxes=pred_locs.type(dtype)*1024)\n        return outputs.sequences, pred_masks, iou_predictions, pred_locs\n    def prepare_inputs_for_generation(\n            self, input_ids, images_sam=None, loc_inputs=None,\n            past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs"
        },
        {
            "comment": "This code is part of the NextChat model implementation. It handles input preprocessing for text generation and incorporates location information in the vision tower layer. If `inputs_embeds` are passed, it uses them only in the 1st generation step. It also checks if `past_key_values` is not None and sets `loc_ids` accordingly before generating embeddings with `self.model.embed_tokens()`.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_seg.py\":186-207",
            "content": "    ):\n        if past_key_values:\n            loc_ids = None\n            if input_ids.size(-1)>=2:\n                loc_ids = input_ids[:, -2]\n            input_ids = input_ids[:, -1:]\n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            inputs_embeds = self.model.embed_tokens(input_ids)\n            hidden_states = kwargs.pop(\"hidden_states\", None)\n            vision_tower = self.model.get_vision_tower()\n            # need to incorporate location information\n            if loc_ids is not None and (loc_ids==vision_tower.config.at_token).any():\n                mask = loc_ids==vision_tower.config.at_token\n                loc_embeds = hidden_states[-1][mask][:, -1:, :]\n                loc_embeds = loc_embeds.type(inputs_embeds.dtype)\n                pred_locs = self.loc_decoder(loc_embeds)\n                loc_embeds = self.loc_encoder(pred_locs)"
        },
        {
            "comment": "The code prepares the model inputs for a chat model, including embedding locations, setting past key values, using cache if specified, applying attention mask, handling images (optional), and including location inputs.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_seg.py\":208-222",
            "content": "                inputs_embeds[mask] = loc_embeds\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n            # model_inputs = {\"input_ids\": input_ids}\n        model_inputs.update(\n            {\n                \"past_key_values\": past_key_values,\n                \"use_cache\": kwargs.get(\"use_cache\"),\n                \"attention_mask\": attention_mask,\n                \"images\": kwargs.get(\"images\", None),\n                \"loc_inputs\": loc_inputs,\n                \"images_sam\": images_sam,\n            }\n        )\n        return model_inputs"
        }
    ]
}