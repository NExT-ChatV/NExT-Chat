{
    "summary": "The code imports libraries, defines a class for argument parsing, initializes training arguments, checks for missing values and config info, sets up logger, prepares training args, and handles existing output directories or checkpoint files.",
    "details": [
        {
            "comment": "The code imports necessary libraries and classes for configuration, argument parsing, logging, and handling of training arguments. It defines a new class Seq2SeqTrainingArguments that inherits from HFSeq2SeqTrainingArguments and adds an extra field 'do_multi_predict'. The prepare_args function initializes an ArgumentParser to handle command-line arguments for the program.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/config/config.py\":0-31",
            "content": "import os\nimport sys\nimport logging\nimport argparse\nfrom dataclasses import dataclass, field\nfrom typing import List, Tuple\nfrom argparse import SUPPRESS\nimport datasets\nimport transformers\nfrom mmengine.config import Config, DictAction\nfrom transformers import HfArgumentParser, set_seed, add_start_docstrings\nfrom transformers import Seq2SeqTrainingArguments as HFSeq2SeqTrainingArguments\nfrom transformers.trainer_utils import get_last_checkpoint, is_main_process\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\nlogging.basicConfig(\n    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n    datefmt=\"%m/%d/%Y %H:%M:%S\",\n    handlers=[logging.StreamHandler(sys.stdout), ],\n)\n@dataclass\n@add_start_docstrings(HFSeq2SeqTrainingArguments.__doc__)\nclass Seq2SeqTrainingArguments(HFSeq2SeqTrainingArguments):\n    do_multi_predict: bool = field(default=False, metadata={\"help\": \"Whether to run predictions on the multi-test set.\"})\ndef prepare_args(args=None):\n    parser = argparse.ArgumentParser()"
        },
        {
            "comment": "This code is parsing known arguments using ArgumentParser and HfArgumentParser, allowing for the overriding of settings in a config file with key-value pairs in the format \"xxx=yyy\". It also handles any unused specified arguments.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/config/config.py\":32-51",
            "content": "    parser.add_argument('config', help='train config file path')\n    parser.add_argument(\n        '--cfg-options',\n        nargs='+',\n        action=DictAction,\n        help='override some settings in the used config, the key-value pair '\n             'in xxx=yyy format will be merged into config file. If the value to '\n             'be overwritten is a list, it should be like key=\"[a,b]\" or key=a,b '\n             'It also allows nested list/tuple values, e.g. key=\"[(a,b),(c,d)]\" '\n             'Note that the quotation marks are necessary and that no white space '\n             'is allowed.')\n    hf_parser = HfArgumentParser((Seq2SeqTrainingArguments,))\n    hf_parser, required = block_required_error(hf_parser)\n    args, unknown_args = parser.parse_known_args(args)\n    known_hf_args, unknown_args = hf_parser.parse_known_args(unknown_args)\n    if unknown_args:\n        raise ValueError(f\"Some specified arguments are not used \"\n                         f\"by the ArgumentParser or HfArgumentParser\\n: {unknown_args}\")"
        },
        {
            "comment": "Code reads configuration file and command line arguments, merges them into training_args, checks for missing required values, updates cfg.training_args, initializes Seq2SeqTrainingArguments, checks output directory, logs config info if local_rank is 0, and sets up logger if should_log is True.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/config/config.py\":53-81",
            "content": "    # load 'cfg' and 'training_args' from file and cli\n    cfg = Config.fromfile(args.config)\n    if args.cfg_options is not None:\n        cfg.merge_from_dict(args.cfg_options)\n    training_args = cfg.training_args\n    training_args.update(vars(known_hf_args))\n    # check training_args require\n    req_but_not_assign = [item for item in required if item not in training_args]\n    if req_but_not_assign:\n        raise ValueError(f\"Requires {req_but_not_assign} but not assign.\")\n    # update cfg.training_args\n    cfg.training_args = training_args\n    # initialize and return\n    training_args = Seq2SeqTrainingArguments(**training_args)\n    training_args = check_output_dir(training_args)\n    # logging\n    if is_main_process(training_args.local_rank):\n        to_logging_cfg = Config()\n        to_logging_cfg.model_args = cfg.model_args\n        to_logging_cfg.data_args = cfg.data_args\n        to_logging_cfg.training_args = cfg.training_args\n        logger.info(to_logging_cfg.pretty_text)\n    # setup logger\n    if training_args.should_log:"
        },
        {
            "comment": "The code sets the log level to info, sets the logger's logging level, enables default handlers and explicit format, and logs training/evaluation parameters along with distributed training details before setting a seed.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/config/config.py\":82-105",
            "content": "        # The default of training_args.log_level is passive, so we set log level at info here to have that default.\n        transformers.logging.set_verbosity_info()\n    log_level = training_args.get_process_log_level()\n    logger.setLevel(log_level)\n    datasets.utils.logging.set_verbosity(log_level)\n    transformers.logging.set_verbosity(log_level)\n    transformers.logging.enable_default_handler()\n    transformers.logging.enable_explicit_format()\n    # setup_print_for_distributed(is_main_process(training_args))\n    # Log on each process the small summary:\n    logger.info(f\"Training/evaluation parameters {training_args}\")\n    logger.warning(\n        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\\n\"\n        + f\"  distributed training: {bool(training_args.local_rank != -1)}, fp16 training: {training_args.fp16}\"\n    )\n    # Set seed before initializing model.\n    set_seed(training_args.seed)\n    return cfg, training_args\ndef block_required_error(hf_parser: HfArgumentParser) -> Tuple[HfArgumentParser, List]:"
        },
        {
            "comment": "This code is setting up the required arguments and checking if an output directory already exists. If it does, it raises a ValueError unless the --overwrite_output_dir flag is used. It also detects if a checkpoint file exists in the output directory and resumes training from there if specified.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/config/config.py\":106-127",
            "content": "    required = []\n    # noinspection PyProtectedMember\n    for action in hf_parser._actions:\n        if action.required:\n            required.append(action.dest)\n        action.required = False\n        action.default = SUPPRESS\n    return hf_parser, required\ndef check_output_dir(training_args):\n    # Detecting last checkpoint.\n    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n            raise ValueError(\n                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n                \"Use --overwrite_output_dir to overcome.\"\n            )\n        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n            logger.info(\n                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \""
        },
        {
            "comment": "The code snippet checks if the `--output_dir` flag is set, and if not, it suggests either setting the `--output_dir` or using the `--overwrite_output_dir` to train from scratch. It then returns the training arguments.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/config/config.py\":128-134",
            "content": "                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n            )\n    return training_args\nif __name__ == \"__main__\":\n    _ = prepare_args()"
        }
    ]
}