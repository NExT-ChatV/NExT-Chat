{
    "summary": "This code contains functions for parameter counting, sentence generation, and token resizing. It also defines a class for image handling with annotations and includes methods to update state and reset masks. The code appends \"<at> <boxes>\" to 'to_sub_strs', formats 'text' using substrings, handles exceptions, and returns modified 'text' and 'ret_boxes'.",
    "details": [
        {
            "comment": "Function `print_trainable_params` counts the number of trainable and total parameters in a given model, displaying them as well as the percentage of trainable ones. `post_process_generate_ids` processes generated token IDs using the provided tokenizer.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/utils/common.py\":0-35",
            "content": "import copy\nfrom typing import List, Union, Dict\nimport os\nimport re, json\nimport gradio as gr\nimport numpy as np\nfrom PIL import ImageDraw\nfrom PIL import ImageFont\nfrom PIL import Image\nimport PIL.Image\nimport torch\nimport numpy as np\nimport torchvision.transforms.functional as F\nimport transformers\nfrom matplotlib import pyplot as plt\nfrom transformers import PreTrainedTokenizer\ndef print_trainable_params(model: torch.nn.Module) -> None:\n    trainable_params, all_param = 0, 0\n    for param in model.parameters():\n        num_params = param.numel()\n        # if using DS Zero 3 and the weights are initialized empty\n        if num_params == 0 and hasattr(param, \"ds_numel\"):\n            num_params = param.ds_numel\n        all_param += num_params\n        if param.requires_grad:\n            trainable_params += num_params\n    print(\"trainable params: {:d} || all params: {:d} || trainable%: {:.4f}\".format(\n        trainable_params, all_param, 100 * trainable_params / all_param))\ndef post_process_generate_ids(tokenizer: PreTrainedTokenizer, ids: torch.Tensor):"
        },
        {
            "comment": "This code contains several functions. The first function, \"copy_deepcopy\", makes a deep copy of the input list to avoid modifying the original. The second function, \"decode_generate_ids\", takes a tokenizer and tensor of ids as input, processes them for sentence generation, and returns either a single decoded sentence or a batch of them. Lastly, the \"show\" function plots a grid of images from a list, detaching and converting them to PIL images if needed.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/utils/common.py\":36-62",
            "content": "    ids = copy.deepcopy(ids)  # do not modify origin preds and targets\n    ids[ids < 0] = tokenizer.pad_token_id\n    return ids\ndef decode_generate_ids(tokenizer: PreTrainedTokenizer, ids: torch.Tensor) -> Union[List[str], str]:\n    assert ids.ndim in [1, 2]\n    only_one_sentence = ids.ndim == 1\n    if only_one_sentence:\n        ids = ids.unsqueeze(0)\n    ids = post_process_generate_ids(tokenizer, ids)\n    res = tokenizer.batch_decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n    if only_one_sentence:\n        return res[0]\n    return res\ndef show(imgs: Union[torch.Tensor, List[Union[torch.Tensor, PIL.Image.Image]]]):\n    if not isinstance(imgs, list):\n        imgs = [imgs]\n    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        if isinstance(img, torch.Tensor):\n            img = img.detach()\n            img = F.to_pil_image(img)\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
        },
        {
            "comment": "The code defines a function `draw_bounding_boxes` that takes an image and boxes coordinates, converts the image to a tensor if necessary, checks the data types of inputs, and returns the image with bounding boxes drawn using another function from torchvision. The second code is a function `smart_tokenizer_and_embedding_resize` that adds special tokens to the tokenizer and resizes the embedding. It also mentions that there may be unoptimized versions which might not make the embedding size divisible by 64.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/utils/common.py\":65-93",
            "content": "def draw_bounding_boxes(\n        image: Union[torch.Tensor, PIL.Image.Image],\n        boxes: Union[torch.Tensor, List, np.ndarray],\n        **kwargs,\n):\n    if isinstance(image, PIL.Image.Image):\n        from torchvision.transforms import PILToTensor\n        image = PILToTensor()(image)\n    assert isinstance(image, torch.Tensor), \"\"\n    if not isinstance(boxes, torch.Tensor):\n        boxes = torch.as_tensor(boxes)\n    assert isinstance(boxes, torch.Tensor)\n    from torchvision.utils import draw_bounding_boxes as _draw_bounding_boxes\n    return _draw_bounding_boxes(image, boxes, **kwargs)\n# https://github.com/huggingface/tokenizers/issues/247#issuecomment-675458087\ndef smart_tokenizer_and_embedding_resize(\n        special_tokens_dict: Dict,\n        tokenizer: transformers.PreTrainedTokenizer,\n        model: transformers.PreTrainedModel,\n):\n    \"\"\"Resize tokenizer and embedding.\n    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n    \"\"\"\n    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)"
        },
        {
            "comment": "The code snippet is a part of the NExT-Chat project and contains two separate functions. The first function resizes the token embeddings in a model based on the length of the provided tokenizer, while the second defines a class 'ImageBoxState' for image processing tasks, with methods to initialize its attributes and reset its state.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/utils/common.py\":94-123",
            "content": "    model.resize_token_embeddings(len(tokenizer))\n    if num_new_tokens > 0:\n        input_embeddings = model.get_input_embeddings().weight.data\n        output_embeddings = model.get_output_embeddings().weight.data\n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n        output_embeddings[-num_new_tokens:] = output_embeddings_avg\nclass ImageBoxState:\n    def __init__(self, draw_size=512):\n        if isinstance(draw_size, (float, int)):\n            draw_size = (draw_size, draw_size)\n        assert len(draw_size) == 2\n        self.size = draw_size\n        self.height, self.width = self.size[0], self.size[1]\n        self.reset_state()\n        self.cnt = 0\n    # noinspection PyAttributeOutsideInit\n    def reset_state(self):\n        self.image = None\n        self.boxes = []\n        self.masks = []\n    # noinspection PyAttributeOutsideInit"
        },
        {
            "comment": "The code defines three methods in a class. The `reset_masks` method resets the `boxes` and `masks` lists. The `update_image` method compares the current image with the input image and resets the state if they are different. The `update_mask` method calculates the difference between the new mask and the last mask, handles cases of no changes or full clearing of the strokes, and finds the coordinates where there are differences in masks.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/utils/common.py\":124-159",
            "content": "    def reset_masks(self):\n        self.boxes = []\n        self.masks = []\n    # noinspection PyAttributeOutsideInit\n    def update_image(self, image):\n        if image != self.image:\n            # self.reset_state()\n            self.image = image\n    def update_mask(self, mask):\n        if len(self.masks) == 0:\n            last_mask = np.zeros_like(mask)\n        else:\n            last_mask = self.masks[-1]\n        if type(mask) == np.ndarray and mask.size > 1:\n            diff_mask = mask - last_mask\n        else:\n            diff_mask = np.zeros([])\n        # clear all of the strokes\n        if mask.sum() == 0:\n            self.reset_masks()\n            return\n        if (mask.astype(np.float32) - last_mask.astype(np.float32)).sum()<0:\n            self.boxes.pop()\n            self.masks.pop()\n            return\n        if diff_mask.sum() > 0:\n            # noinspection PyArgumentList\n            x1x2 = np.where(diff_mask.max(0) != 0)[0]\n            # noinspection PyArgumentList\n            y1y2 = np.where(diff_mask.max(1) != 0)[0]"
        },
        {
            "comment": "The code is part of a class responsible for handling image boxes and associated masks. The `update_box` method appends new box coordinates to the list of boxes, ensuring they are in the correct order. The `to_model` function prepares the data for conversion into a machine learning model input. The `draw_boxes` function draws the boxes on an image and associates each box with a specific color from a list.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/utils/common.py\":160-185",
            "content": "            y1, y2 = y1y2.min(), y1y2.max()\n            x1, x2 = x1x2.min(), x1x2.max()\n            if (x2 - x1 > 5) and (y2 - y1 > 5):\n                self.masks.append(mask.copy())\n                self.boxes.append(tuple(map(int, (x1, y1, x2, y2))))\n    def update_box(self, box):\n        x1, y1, x2, y2 = box\n        x1, x2 = min(x1, x2), max(x1, x2)\n        y1, y2 = min(y1, y2), max(y1, y2)\n        self.boxes.append(tuple(map(int, (x1, y1, x2, y2))))\n    def to_model(self):\n        pass\n        # if self.image is None:\n        #     return {}\n        # image = expand2square(self.image)\n        # boxes = [box_xyxy_expand2square(box, w=self.image.width, h=self.image.height) for box in self.boxes]\n        # return {'image': image, 'boxes': boxes}\n    def draw_boxes(self):\n        assert self.image is not None\n        grounding_texts = [f'{bid}' for bid in range(len(self.boxes))]\n        def _draw(img, _boxes, texts):\n            assert img is not None\n            colors = [\"red\", \"blue\", \"green\", \"olive\", \"orange\", \"brown\", \"cyan\", \"purple\"]"
        },
        {
            "comment": "The code is drawing bounding boxes and annotated text on images. It uses ImageDraw to create the rectangles for each bounding box and text, using a given font size and color. The function returns the modified image with the annotations.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/utils/common.py\":186-204",
            "content": "            _img_draw = ImageDraw.Draw(img)\n            font = ImageFont.truetype(os.path.join(os.path.dirname(__file__), 'assets/DejaVuSansMono.ttf'), size=18)\n            for bid, box in enumerate(_boxes):\n                _img_draw.rectangle((box[0], box[1], box[2], box[3]), outline=colors[bid % len(colors)], width=4)\n                anno_text = texts[bid]\n                _img_draw.rectangle((box[0], box[3] - int(font.size * 1.2), box[0] + int((len(anno_text) + 0.8) * font.size * 0.6), box[3]),\n                                    outline=colors[bid % len(colors)], fill=colors[bid % len(colors)], width=4)\n                _img_draw.text((box[0] + int(font.size * 0.2), box[3] - int(font.size * 1.2)), anno_text, font=font, fill=(255, 255, 255))\n            return img\n        out_draw = _draw(self.image, self.boxes, grounding_texts)\n        return out_draw\ndef bbox_draw(sketch_pad: dict, state: dict):\n    def binarize(x):\n        return (x != 0).astype('uint8') * 255\n    image = sketch_pad['image']\n    image = open_image(image)"
        },
        {
            "comment": "The code defines a function to open an image, another function to parse boxes from a text string, and contains additional code for image processing. The functions check the type of the input image, convert it to appropriate format, and update the mask and box states. The parsed boxes are validated and appended to a list while replacing them in the original text string.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/utils/common.py\":205-239",
            "content": "    # global count\n    # count += 1\n    # np.save( f\"{count}.npy\", sketch_pad['mask'])\n    mask = sketch_pad['mask'].sum(-1) if sketch_pad['mask'].ndim == 3 else sketch_pad['mask']\n    mask = binarize(mask)\n    ibs = state[\"ibs\"]\n    ibs.update_image(image)\n    ibs.update_mask(mask)\n    out_draw = ibs.draw_boxes()\n    return out_draw, state\ndef open_image(image):\n    if type(image) == np.ndarray:\n        image = Image.fromarray(image)\n    elif type(image) == str:\n        image = Image.open(image).convert(\"RGB\")\n    return image\ndef parse_boxes(text):\n    def is_valid(lst):\n        return all([(type(x) == int) and (x >= 0) for x in lst]) and len(lst)>0\n    text = text.replace(\",]\", \"]\")\n    pat = re.compile(r\"\\[.*?\\]\")\n    matched_boxes = pat.findall(text)\n    ret_boxes = []\n    to_sub_strs = []\n    for box_str in matched_boxes:\n        try:\n            box_seq = json.loads(box_str)\n            if is_valid(box_seq):\n                ret_boxes.append(box_seq)\n                text = text.replace(box_str, \"{}\")\n                # to_sub_strs.append(\" \".join([\"<at> <boxes>\"]*len(box_seq)))"
        },
        {
            "comment": "This code snippet is a part of a larger function. It appends \"<at> <boxes>\" to the 'to_sub_strs' list and then formats the 'text' variable using those substrings. If an exception occurs, it is caught and ignored. Finally, it returns the modified 'text' and 'ret_boxes'.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/utils/common.py\":240-244",
            "content": "                to_sub_strs.append(\"<at> <boxes>\")\n        except Exception as e:\n            pass\n    text = text.format(*to_sub_strs)\n    return text, ret_boxes"
        }
    ]
}