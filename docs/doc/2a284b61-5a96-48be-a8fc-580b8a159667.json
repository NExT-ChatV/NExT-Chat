{
    "summary": "This code contains image and bounding box transformation functions for the NExT-Chat dataset, performing cropping, flipping, area checks, dictionary updates, maintains order consistency, resizes with constraints, preserves integrity, and has custom transforms for resizing, cropping, and flipping.",
    "details": [
        {
            "comment": "This code defines functions for transforming image and bounding box coordinates. It includes functions for normalizing and denormalizing bounding box coordinates, converting between xywh and xyxy formats, and applying common image transforms. The code is part of the NExT-Chat dataset utilities and uses libraries such as PIL and torchvision for image processing.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/utils/transform.py\":0-49",
            "content": "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n\"\"\"\nTransforms and data augmentation for both image + bbox.\n\"\"\"\nfrom typing import Dict, Any, Tuple, Optional\nfrom PIL import Image\nfrom ..root import TRANSFORMS\nimport random\nimport PIL\nimport torch\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as F\nfrom mllm.utils.box_ops import box_xyxy_to_cxcywh\nfrom mllm.utils.box_ops import interpolate\ndef de_norm_box_xyxy(box, *, w, h):\n    x1, y1, x2, y2 = box\n    x1 = x1 * w\n    x2 = x2 * w\n    y1 = y1 * h\n    y2 = y2 * h\n    box = x1, y1, x2, y2\n    return box\ndef box_xywh_to_xyxy(box, *, w=None, h=None):\n    x, y, bw, bh = box\n    x2 = x + bw\n    y2 = y + bh\n    if w is not None:\n        x2 = min(x2, w)\n    if h is not None:\n        y2 = min(y2, h)\n    box = x, y, x2, y2\n    return box\ndef norm_box_xyxy(box, *, w, h):\n    x1, y1, x2, y2 = box\n    # Calculate the normalized coordinates with min-max clamping\n    norm_x1 = max(0.0, min(x1 / w, 1.0))\n    norm_y1 = max(0.0, min(y1 / h, 1.0))"
        },
        {
            "comment": "This code includes functions for normalizing box coordinates, normalizing a point (x, y) and expanding an image to a square shape. The normalized_box function takes in x1, y1, x2, y2 coordinates of a rectangle and normalizes them between 0 and 1. The norm_point_xyxy function normalizes an (x, y) coordinate in the same way. The expand2square function takes an image and expands it to a square shape by either extending the width or height while preserving aspect ratio and centering the image. The box_xyxy_expand2square function checks if w is equal to h, assuming a rectangular image.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/utils/transform.py\":50-82",
            "content": "    norm_x2 = max(0.0, min(x2 / w, 1.0))\n    norm_y2 = max(0.0, min(y2 / h, 1.0))\n    # Return the normalized box coordinates\n    # normalized_box = (round(norm_x1, 3), round(norm_y1, 3), round(norm_x2, 3), round(norm_y2, 3))\n    normalized_box = (norm_x1, norm_y1, norm_x2, norm_y2)\n    return normalized_box\ndef norm_point_xyxy(point, *, w, h):\n    x, y = point\n    norm_x = max(0.0, min(x / w, 1.0))\n    norm_y = max(0.0, min(y / h, 1.0))\n    point = norm_x, norm_y\n    return point\ndef expand2square(pil_img, background_color=(255, 255, 255)):\n    width, height = pil_img.size\n    if width == height:\n        return pil_img\n    elif width > height:\n        result = Image.new(pil_img.mode, (width, width), background_color)\n        result.paste(pil_img, (0, (width - height) // 2))\n        return result\n    else:\n        result = Image.new(pil_img.mode, (height, height), background_color)\n        result.paste(pil_img, ((height - width) // 2, 0))\n        return result\ndef box_xyxy_expand2square(box, *, w, h):\n    if w == h:"
        },
        {
            "comment": "This code defines a transform function that expands an image or bounding box to a square by adjusting the position of its bottom-right and top-left corners. The function can also be applied to individual points, as shown in the point_xy_expand2square function. The Expand2square class is registered as a module transform and takes an optional background color argument.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/utils/transform.py\":83-115",
            "content": "        return box\n    if w > h:\n        x1, y1, x2, y2 = box\n        y1 += (w - h) // 2\n        y2 += (w - h) // 2\n        box = x1, y1, x2, y2\n        return box\n    assert w < h\n    x1, y1, x2, y2 = box\n    x1 += (h - w) // 2\n    x2 += (h - w) // 2\n    box = x1, y1, x2, y2\n    return box\ndef point_xy_expand2square(point, *, w, h):\n    pseudo_box = (point[0], point[1], point[0], point[1])\n    expanded_box = box_xyxy_expand2square(box=pseudo_box, w=w, h=h)\n    expanded_point = (expanded_box[0], expanded_box[1])\n    return expanded_point\n@TRANSFORMS.register_module()\nclass Expand2square:\n    def __init__(self, background_color=(255, 255, 255)):\n        self.background_color = background_color\n    def __call__(self, image: Image.Image, labels: Dict[str, Any] = None) -> Tuple[Image.Image, Optional[Dict[str, Any]]]:\n        width, height = image.size\n        processed_image = expand2square(image, background_color=self.background_color)\n        if labels is None:\n            return processed_image, labels\n        if 'boxes' in labels:"
        },
        {
            "comment": "This function transforms bounding boxes and optionally points in labels, expanding them to square format if necessary. It then returns the processed image and modified labels. The crop() function crops an image based on a given region and target, ensuring that the target's size is correctly updated according to the cropped region. It also checks if the original image should be resized or not.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/utils/transform.py\":116-145",
            "content": "            bboxes = [box_xyxy_expand2square(bbox, w=width, h=height) for bbox in labels['boxes']]\n            labels['boxes'] = bboxes\n        if 'points' in labels:\n            points = [point_xy_expand2square(point, w=width, h=height) for point in labels['points']]\n            labels['points'] = points\n        return processed_image, labels\ndef crop(image, target, region):\n    cropped_image = F.crop(image, *region)\n    if target is None:\n        return cropped_image, None, True\n    target = target.copy()\n    i, j, h, w = region\n    # should we do something wrt the original size?\n    target[\"size\"] = torch.tensor([h, w])\n    fields = [\"labels\", \"area\", \"iscrowd\"]\n    if \"boxes\" in target:\n        boxes = torch.tensor(target[\"boxes\"])\n        max_size = torch.as_tensor([w, h], dtype=torch.float32)\n        cropped_boxes = boxes - torch.as_tensor([j, i, j, i])\n        cropped_boxes = torch.min(cropped_boxes.reshape(-1, 2, 2), max_size)\n        cropped_boxes = cropped_boxes.clamp(min=0)\n        area = (cropped_boxes[:, 1, :] - cropped_boxes[:, 0, :]).prod(dim=1)"
        },
        {
            "comment": "This code performs cropping, horizontal flipping, and removes elements with zero area in images and targets. It also checks if there are boxes or masks and updates the area accordingly. It preserves the order of elements using fields for transforms consistency.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/utils/transform.py\":146-174",
            "content": "        target[\"boxes\"] = cropped_boxes.reshape(-1, 4)\n        target[\"area\"] = area\n        fields.append(\"boxes\")\n    if \"masks\" in target:\n        # FIXME should we update the area here if there are no boxes?\n        target['masks'] = target['masks'][:, i:i + h, j:j + w]\n        fields.append(\"masks\")\n    all_box_reserved = True\n    # remove elements for which the boxes or masks that have zero area\n    if \"boxes\" in target or \"masks\" in target:\n        # favor boxes selection when defining which elements to keep\n        # this is compatible with previous implementation\n        if \"boxes\" in target:\n            cropped_boxes = target['boxes'].reshape(-1, 2, 2)\n            keep = torch.all(cropped_boxes[:, 1, :] > cropped_boxes[:, 0, :], dim=1)\n        else:\n            keep = target['masks'].flatten(1).any(1)\n        # for field in fields:\n        #     target[field] = target[field][keep]\n        all_box_reserved = keep.all()\n    return cropped_image, target, all_box_reserved\ndef hflip(image, target):\n    flipped_image = F.hflip(image)"
        },
        {
            "comment": "This code flips the input image and updates the target dictionary if it contains 'boxes' or 'masks'. It also performs resizing, considering aspect ratio and max size constraints. The function returns the flipped image and updated target.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/utils/transform.py\":176-208",
            "content": "    if target is None:\n        return flipped_image, None\n    w, h = image.size\n    target = target.copy()\n    if \"boxes\" in target:\n        boxes = torch.tensor(target[\"boxes\"])\n        boxes = boxes[:, [2, 1, 0, 3]] * torch.as_tensor([-1, 1, -1, 1]) + torch.as_tensor([w, 0, w, 0])\n        target[\"boxes\"] = boxes\n    if \"masks\" in target:\n        target['masks'] = target['masks'].flip(-1)\n    return flipped_image, target\ndef resize(image, target, size, max_size=None):\n    # size can be min_size (scalar) or (w, h) tuple\n    def get_size_with_aspect_ratio(image_size, size, max_size=None):\n        w, h = image_size\n        if max_size is not None:\n            min_original_size = float(min((w, h)))\n            max_original_size = float(max((w, h)))\n            if max_original_size / min_original_size * size > max_size:\n                size = int(round(max_size * min_original_size / max_original_size))\n        if (w <= h and w == size) or (h <= w and h == size):\n            return (h, w)\n        if w < h:\n            ow = size"
        },
        {
            "comment": "This code resizes an image while maintaining its aspect ratio. It takes the original image size, target size, and maximum size as inputs. If a target is provided, it scales the bounding boxes and area according to the new image dimensions.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/utils/transform.py\":209-242",
            "content": "            oh = int(size * h / w)\n        else:\n            oh = size\n            ow = int(size * w / h)\n        return (oh, ow)\n    def get_size(image_size, size, max_size=None):\n        if isinstance(size, (list, tuple)):\n            return size[::-1]\n        else:\n            return get_size_with_aspect_ratio(image_size, size, max_size)\n    size = get_size(image.size, size, max_size)\n    rescaled_image = F.resize(image, size)\n    if target is None:\n        return rescaled_image, None\n    ratios = tuple(float(s) / float(s_orig) for s, s_orig in zip(rescaled_image.size, image.size))\n    ratio_width, ratio_height = ratios\n    target = target.copy()\n    if \"boxes\" in target:\n        boxes = torch.tensor(target[\"boxes\"])\n        scaled_boxes = boxes * torch.as_tensor([ratio_width, ratio_height, ratio_width, ratio_height])\n        target[\"boxes\"] = scaled_boxes\n    if \"area\" in target:\n        area = target[\"area\"]\n        scaled_area = area * (ratio_width * ratio_height)\n        target[\"area\"] = scaled_area\n    h, w = size"
        },
        {
            "comment": "The code defines a function to resize and pad an image with its target. It includes a RandomSizeCrop class that randomly crops the input image based on given minimum and maximum size constraints. The resize function interpolates the image to a specified size, optionally modifying masks if present. The pad function pads the bottom right corners of the image and adjusts the target size accordingly.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/utils/transform.py\":243-273",
            "content": "    target[\"size\"] = torch.tensor([h, w])\n    if \"masks\" in target:\n        target['masks'] = interpolate(\n            target['masks'][:, None].float(), size, mode=\"nearest\")[:, 0] > 0.5\n    return rescaled_image, target\ndef pad(image, target, padding):\n    # assumes that we only pad on the bottom right corners\n    padded_image = F.pad(image, (0, 0, padding[0], padding[1]))\n    if target is None:\n        return padded_image, None\n    target = target.copy()\n    # should we do something wrt the original size?\n    target[\"size\"] = torch.tensor(padded_image.size[::-1])\n    if \"masks\" in target:\n        target['masks'] = torch.nn.functional.pad(target['masks'], (0, padding[0], 0, padding[1]))\n    return padded_image, target\nclass RandomSizeCrop(object):\n    def __init__(self, min_size: int, max_size: int):\n        self.min_size = min_size\n        self.max_size = max_size\n    def __call__(self, img: PIL.Image.Image, target: dict):\n        w = random.randint(self.min_size, min(img.width, self.max_size))\n        h = random.randint(self.min_size, min(img.height, self.max_size))"
        },
        {
            "comment": "The code defines three classes: `RandomHorizontalFlip`, `RandomResize`, and `RandomSelect`. These classes are used to perform various random transformations on images for data augmentation. The transformations include horizontal flipping, resizing, and selecting between different transformations randomly with given probabilities.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/utils/transform.py\":274-308",
            "content": "        region = T.RandomCrop.get_params(img, [h, w])\n        for i in range(3):\n            rst_img, rst_target, all_box_reserved = crop(img, target, region)\n            if all_box_reserved:\n                return rst_img, rst_target\n        return img, target\nclass RandomHorizontalFlip(object):\n    def __init__(self, p=0.5):\n        self.p = p\n    def __call__(self, img, target):\n        if random.random() < self.p:\n            return hflip(img, target)\n        return img, target\nclass RandomResize(object):\n    def __init__(self, sizes, max_size=None):\n        assert isinstance(sizes, (list, tuple))\n        self.sizes = sizes\n        self.max_size = max_size\n    def __call__(self, img, target=None):\n        size = random.choice(self.sizes)\n        return resize(img, target, size, self.max_size)\nclass RandomSelect(object):\n    \"\"\"\n    Randomly selects between transforms1 and transforms2,\n    with probability p for transforms1 and (1 - p) for transforms2\n    \"\"\"\n    def __init__(self, transforms1, transforms2, p=0.5):"
        },
        {
            "comment": "The code defines a train augmentation class that randomly applies two sets of transformations to an image and its target, with a specified probability. It also includes a composition function for applying multiple transforms sequentially. The transforms1 and transforms2 are instances of the Compose class, which allows for easy addition of different transformation operations like resize, crop, flip, etc. The TrainAug class is registered in TRANSFORMS module for further usage.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/utils/transform.py\":309-342",
            "content": "        self.transforms1 = transforms1\n        self.transforms2 = transforms2\n        self.p = p\n    def __call__(self, img, target):\n        if random.random() < self.p:\n            return self.transforms1(img, target)\n        return self.transforms2(img, target)\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n    def __call__(self, image, target):\n        for t in self.transforms:\n            image, target = t(image, target)\n        return image, target\n    def __repr__(self):\n        format_string = self.__class__.__name__ + \"(\"\n        for t in self.transforms:\n            format_string += \"\\n\"\n            format_string += \"    {0}\".format(t)\n        format_string += \"\\n)\"\n        return format_string\n@TRANSFORMS.register_module()\nclass TrainAug:\n    def __init__(self, background_color=(255, 255, 255)):\n        self.resize_ratio = 0.25\n        self.background_color = background_color\n        self.random_size_crop = Compose(\n                                            ["
        },
        {
            "comment": "This code defines a custom transform that applies random resizing, cropping, and horizontal flipping to an image. It utilizes the RandomResize, RandomSizeCrop, and RandomHorizontalFlip functions for transformation. The resulting processed image is returned along with any labels if provided.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/utils/transform.py\":343-362",
            "content": "                                                RandomResize([400, 500, 600]),\n                                                RandomSizeCrop(384, 1333),\n                                            ]\n                                        )\n        self.random_horizontal = RandomHorizontalFlip()\n    def __call__(self, image: Image.Image, labels: Dict[str, Any] = None) -> Tuple[Image.Image, Optional[Dict[str, Any]]]:\n        # if random.random() > self.resize_ratio:\n        #     image, labels = self.random_size_crop(image, labels)\n        image, labels = self.random_horizontal(image, labels)\n        width, height = image.size\n        processed_image = expand2square(image, background_color=self.background_color)\n        if labels is None:\n            return processed_image, labels\n        if 'boxes' in labels:\n            bboxes = [box_xyxy_expand2square(bbox, w=width, h=height) for bbox in labels['boxes']]\n            labels['boxes'] = bboxes\n        if 'points' in labels:\n            points = [point_xy_expand2square(point, w=width, h=height) for point in labels['points']]"
        },
        {
            "comment": "This code adds the 'points' label to the 'labels' dictionary based on the value of the 'points' variable, and then returns the processed_image and labels as output.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/utils/transform.py\":363-364",
            "content": "            labels['points'] = points\n        return processed_image, labels"
        }
    ]
}