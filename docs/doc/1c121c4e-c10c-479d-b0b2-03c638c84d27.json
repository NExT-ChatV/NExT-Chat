{
    "summary": "This code creates a GUI chatbot with image recognition and response capabilities using GR library, featuring grounding, captioning, explaining, and region captioning tasks. The chatbot includes text prompts, sliders, button functionality for responses, and 2 output areas. It initializes a new demo object, sets inputs, and launches on a specified server with sharing enabled.",
    "details": [
        {
            "comment": "This code imports necessary modules, sets the log level for transformers library, creates a directory for temporary files, and defines argument parsing using argparse. It also initializes the NExT-Chat model with required arguments like load_in_8bit, server_name, server_port, model_path, and vit_path.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/demo/web_demo.py\":0-31",
            "content": "import os, sys\nimport pathlib\nimport logging\nimport argparse\nfrom pathlib import Path\nimport gradio as gr\nimport transformers\nproject_path = pathlib.Path(__file__).parent.parent.parent\nsys.path.append(str(project_path))\nfrom mllm.utils import ImageBoxState, bbox_draw, parse_boxes\nfrom mllm.demo.demo_util import NextChatInference\nimport time\nlog_level = logging.ERROR\ntransformers.logging.set_verbosity(log_level)\ntransformers.logging.enable_default_handler()\ntransformers.logging.enable_explicit_format()\nTEMP_FILE_DIR = Path(__file__).parent / 'temp'\nTEMP_FILE_DIR.mkdir(parents=True, exist_ok=True)\n#########################################\n# mllm model init\n#########################################\nparser = argparse.ArgumentParser(\"NExT-Chat Web Demo\")\nparser.add_argument('--load_in_8bit', action='store_true')\nparser.add_argument('--server_name', default=None)\nparser.add_argument('--server_port', type=int, default=None)\nparser.add_argument('--model_path', type=str, required=True)\nparser.add_argument('--vit_path', type=str, required=True)"
        },
        {
            "comment": "This code initializes a NextChatInference object, defines a post-processing function for responses containing boxes, and a chat_one_turn function that takes input text, temperature, top_p, top_k, an image, history, hidden image, and state as parameters. The code uses the state's \"ibs\" boxes to parse the input text and prepare inputs for the pipe model.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/demo/web_demo.py\":32-66",
            "content": "parser.add_argument('--image_token_len', type=int, default=576)\nargs = parser.parse_args()\nprint(args)\npipe = NextChatInference(args.model_path, args.vit_path, args.image_token_len)\ndef post_process_response(response):\n    if \"<at> <boxes>\" not in response:\n        return response.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n    splits = response.split(\"<at> <boxes>\")\n    to_concat = [f\"[{i}]\" for i in range(len(splits) - 1)]\n    rst = [splits[i // 2] if i % 2 == 0 else to_concat[i // 2]\n           for i in range(len(splits) + len(to_concat))]\n    rst = \"\".join(rst)\n    rst = rst.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n    return rst\ndef chat_one_turn(\n        input_text,\n        temperature,\n        top_p,\n        top_k,\n        input_image,\n        history,\n        hidden_image,\n        state,\n):\n    boxes = state[\"ibs\"].boxes\n    gpt_input_text, boxes_seq = parse_boxes(input_text)\n    inputs = {\"image\":input_image['image'], \"text\": gpt_input_text}\n    response, _, _, img = pipe(inputs, temperature=temperature, top_p=top_p, top_k=top_k,"
        },
        {
            "comment": "The code is creating a chatbot that can recognize images, take user input, and respond accordingly. If an image is provided, it saves it to a temporary folder. The code includes functions for grounding, captioning, explaining, and region captioning tasks based on the user's input. The default chatbox prompts the user to begin the chat.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/demo/web_demo.py\":67-96",
            "content": "                            boxes=boxes, boxes_seq=boxes_seq)\n    ret_text = [(post_process_response(input_text), post_process_response(response))]\n    filename_grounding = None\n    if img is not None:\n        print(\"detection\")\n        timestamp = int(time.time())\n        filename_grounding = f\"tmp/{timestamp}.jpg\"\n        if not os.path.exists(\"tmp/\"):\n            os.makedirs(\"tmp/\")\n        img.save(filename_grounding)\n    if img is not None:\n        ret_text.append((None, (filename_grounding,)))\n    return \"\", ret_text, hidden_image\ndefault_chatbox = [(\"\", \"Please begin the chat.\")]\ndef shortcut_func(task_name, text):\n    task_name = task_name[0]\n    if task_name == \"Grounding\":\n        return \"Where is XXX in the image?\"\n    elif task_name == \"Caption\":\n        return \"Can you provide a description of the image and include the locations for each mentioned object?\"\n    elif task_name == \"Explain\":\n        return text.strip()+\" Please include object locations and explain.\"\n    elif task_name == \"Region Cap\":"
        },
        {
            "comment": "The code defines a function `new_state()` to return the initial image box state, and two functions `clear_fn()` and `clear_fn2()` that return default values for chatbox and states. The main part of the code initializes a GROMACS block with HTML elements for NExT-Chat's user manual, providing instructions on how to use the program for grounding and captioning tasks with images.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/demo/web_demo.py\":97-129",
            "content": "        return \"What is region [0]?\"\n    return \"\"\ndef new_state():\n    return {\"ibs\": ImageBoxState()}\ndef clear_fn(value):\n    return \"\", default_chatbox, None, None, new_state()\ndef clear_fn2(value):\n    return default_chatbox, None, new_state()\nif __name__ == '__main__':\n    # conversation = prepare_interactive(model_args, preprocessor)\n    # predict(model, \"tmp.jpg\", \"Find person bottom left in <image>.\", boxes=None, boxes_seq=None)\n    # import IPython\n    # IPython.embed()\n    with gr.Blocks() as demo:\n        gr.HTML(\n            f\"\"\"\n            <h1 align=\"center\"><font color=\"#966661\">NExT-Chat</font></h1>\n            <p align=\"center\">\n                <a href='' target='_blank'>[Project]</a>\n                <a href='' target='_blank'>[Paper]</a>\n            </p>\n            <h2>User Manual</h2>\n            <ul>\n            <li><p><strong>Grounding:</strong> Where is XXX in the &lt;image&gt;? </p></li>\n            <li><p><strong>Caption with objects: </strong>Can you provide a description of the image &lt;image&gt; and include the locations for each mentioned object? </p></li>"
        },
        {
            "comment": "The code provides a list of prompts to include object locations in the model's responses and explains the process for region understanding using boxes.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/demo/web_demo.py\":130-150",
            "content": "            <li><p><strong>The model is default not to include obj locations at most time.</strong> </p></li>\n            <li><p><strong>To let the model include object locations. You can add prompts like:</strong> </p></li>\n                <ul>\n                <li><p>Please include object locations and explain. </p></li>\n                <li><p>Make sure to include object locations and explain. </p></li>\n                <li><p>Please include object locations as much as possible. </p></li>\n                </ul>\n            <li><p><strong>Region Understanding:</strong> draw boxes and ask like \"what is region [0]?\" </p></li>\n            <ul>\n            \"\"\"\n        )\n        with gr.Row():\n            with gr.Column(scale=6):\n                with gr.Group():\n                    input_shortcuts = gr.Dataset(components=[gr.Textbox(visible=False)], samples=[\n                        [\"Grounding\"],\n                        [\"Caption\"], [\"Explain\"], [\"Region Cap\"]], label=\"Shortcut Dataset\")\n                    input_text = gr.Textbox(label='Input Text',"
        },
        {
            "comment": "The code above sets up a graphical user interface (GUI) for a chatbot using the Gloabal Runtime (GR) library. The UI includes an input box for text prompts, an image mask for parsing sketches, sliders to adjust generation settings like temperature and top K, buttons for generating responses and clearing history, and two output areas: one for displaying text conversation history and another for displaying parsed images.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/demo/web_demo.py\":151-170",
            "content": "                                            placeholder='Please enter text prompt below and press ENTER.')\n                    with gr.Row():\n                        input_image = gr.ImageMask()\n                        out_imagebox = gr.Image(label=\"Parsed Sketch Pad\")\n                    input_image_state = gr.State(new_state())\n                with gr.Row():\n                    temperature = gr.Slider(maximum=1, value=0.8, minimum=0, label='Temperature')\n                    top_p = gr.Slider(maximum=1, value=0.7, minimum=0, label='Top P')\n                    top_k = gr.Slider(maximum=100, value=5, minimum=1, step=1, label='Top K')\n                with gr.Row():\n                    run_button = gr.Button('Generate')\n                    clear_button = gr.Button('Clear')\n            with gr.Column(scale=4):\n                output_text = gr.components.Chatbot(label='Multi-round conversation History',\n                                                    value=default_chatbox).style(height=550)\n                output_image = gr.Textbox(visible=False)"
        },
        {
            "comment": "The code above consists of several function calls to execute chatbot interactions. `input_shortcuts.click()` triggers a shortcut function, `run_button.click()` executes a one-turn chat conversation, `input_text.submit()` also triggers the same one-turn chat, and finally, `clear_button.click()` clears the input text and images while `input_image.upload()` uploads an image. Each function call has different inputs and outputs associated with them for executing the respective tasks.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/demo/web_demo.py\":172-184",
            "content": "        input_shortcuts.click(fn=shortcut_func, inputs=[input_shortcuts, input_text], outputs=[input_text])\n        run_button.click(fn=chat_one_turn, inputs=[input_text, temperature, top_p, top_k,\n                                                   input_image, output_text, output_image,\n                                                   input_image_state],\n                         outputs=[input_text, output_text, output_image])\n        input_text.submit(fn=chat_one_turn, inputs=[input_text, temperature, top_p, top_k,\n                                                    input_image, output_text, output_image,\n                                                    input_image_state],\n                          outputs=[input_text, output_text, output_image])\n        clear_button.click(fn=clear_fn, inputs=clear_button,\n                           outputs=[input_text, output_text, input_image, out_imagebox, input_image_state])\n        input_image.upload(fn=clear_fn2, inputs=clear_button, outputs=[output_text, out_imagebox, input_image_state])"
        },
        {
            "comment": "This code block is creating an example for the GUI application. It sets up input and output variables, uses functions to clear and edit the image, and creates a row with three examples using different images, prompts, and initial states.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/demo/web_demo.py\":185-208",
            "content": "        input_image.clear(fn=clear_fn2, inputs=clear_button, outputs=[output_text, out_imagebox, input_image_state])\n        input_image.edit(\n            fn=bbox_draw,\n            inputs=[input_image, input_image_state],\n            outputs=[out_imagebox, input_image_state],\n            queue=False,\n        )\n        with gr.Row():\n            gr.Examples(\n                examples=[\n                    [\n                        os.path.join(os.path.dirname(__file__), \"assets/dog.jpg\"),\n                        \"Can you describe the image and include object locations?\",\n                        new_state(),\n                    ],\n                    [\n                        os.path.join(os.path.dirname(__file__), \"assets/fishing.jpg\"),\n                        \"A boy is sleeping on bed, is this correct? Please include object locations.\",\n                        new_state(),\n                    ],\n                    [\n                        os.path.join(os.path.dirname(__file__), \"assets/rec_bear.png\"),\n                        \"Where is the bear wearing the red decoration in the image?\","
        },
        {
            "comment": "This code initializes a new demo object, sets its input images and text, and launches the demo on a specified server with sharing enabled.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/demo/web_demo.py\":209-221",
            "content": "                        new_state(),\n                    ],\n                    [\n                        os.path.join(os.path.dirname(__file__), \"assets/woman.jpeg\"),\n                        \"What is the woman doing? Please include object locations.\",\n                        new_state(),\n                    ],\n                ],\n                inputs=[input_image, input_text, input_image_state],\n            )\n    print(\"launching...\")\n    demo.queue().launch(server_name=args.server_name, server_port=args.server_port, share=True)"
        }
    ]
}