{
    "summary": "This code defines the training arguments for a deep learning model, including options for dataset processing, training strategy, distributed data parallelism, logging and saving settings, and evaluation or prediction tasks. The default parameters are set for 5 epochs of training with a batch size of 8, using a learning rate of 2e-5 and a cosine scheduler. It also enables mixed precision training (tf32 and bf16) and gradient checkpointing to improve efficiency on NVIDIA GPUs. The model is wrapped for full-shard distributed training and uses the LlamaDecoderLayer as the layer to wrap.",
    "details": [
        {
            "comment": "This code defines the training arguments for a deep learning model, including options for dataset processing, training strategy, distributed data parallelism, logging and saving settings, and evaluation or prediction tasks. The default parameters are set for 5 epochs of training with a batch size of 8, using a learning rate of 2e-5 and a cosine scheduler. It also enables mixed precision training (tf32 and bf16) and gradient checkpointing to improve efficiency on NVIDIA GPUs. The model is wrapped for full-shard distributed training and uses the LlamaDecoderLayer as the layer to wrap.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/config/_base_/train/nextchat_fsdp.py\":0-40",
            "content": "training_args = dict(\n    # run\n    output_dir=None,  # required. must be filled by derived configs.\n    overwrite_output_dir=True,\n    report_to='none',\n    seed=42,\n    # datasets\n    remove_unused_columns=False,\n    # train\n    do_train=True,\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=1,\n    num_train_epochs=5,\n    learning_rate=2e-5,\n    lr_scheduler_type='cosine',\n    weight_decay=0.,\n    warmup_ratio=0.03,\n    evaluation_strategy='no',\n    # train ddp\n    tf32=True,\n    bf16=True,\n    gradient_checkpointing=True,\n    fsdp=\"full_shard auto_wrap\",\n    fsdp_transformer_layer_cls_to_wrap='LlamaDecoderLayer',\n    # train logging\n    logging_steps=10,\n    save_strategy='steps',\n    save_steps=3000,\n    save_total_limit=10,\n    # eval and predict\n    do_eval=False,\n    do_predict=False,\n    predict_with_generate=True,\n    per_device_eval_batch_size=8,\n    dataloader_num_workers=16,\n)"
        }
    ]
}