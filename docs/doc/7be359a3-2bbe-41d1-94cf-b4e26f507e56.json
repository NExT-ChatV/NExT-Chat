{
    "summary": "This code initializes vision and language models for AI chat, handling multimodal inputs, performing bounding box regression and object detection. It uses pretrained weights, encodes embeddings, and checks input shapes. The mean of the box_iou matrix is printed as output.",
    "details": [
        {
            "comment": "This code imports necessary libraries, defines a NextChatConfig class which inherits from LlamaConfig and extends it with sam_path and mm_depth parameters. It also defines a NextChatLlamaModel class that inherits from LlamaModel and includes additional parameters for vision tower and hidden size.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_base.py\":0-33",
            "content": "import logging\nfrom typing import List, Optional, Tuple, Union\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom transformers.models.bert.modeling_bert import BertEncoder, BertConfig\nfrom torch.nn import CrossEntropyLoss\nimport torch.nn.functional as F\nfrom mllm.utils.box_ops import generalized_box_iou, box_cxcywh_to_xyxy, box_xyxy_to_cxcywh, box_iou\nfrom transformers import LlamaConfig, LlamaModel, LlamaForCausalLM, CLIPVisionModel, CLIPImageProcessor, \\\n    CLIPVisionConfig\nfrom transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\nDEFAULT_IMAGE_TOKEN = \"<image>\"\nDEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\nDEFAULT_BOXES_TOKEN = \"<boxes>\"\nDEFAULT_AT_TOKEN = \"<at>\"\nDEFAULT_IM_START_TOKEN = \"<im_start>\"\nDEFAULT_IM_END_TOKEN = \"<im_end>\"\nclass NextChatConfig(LlamaConfig):\n    model_type = \"nextchat\"\n    sam_path = None\n    mm_depth = 2\nclass NextChatLlamaModel(LlamaModel):\n    config_class = NextChatConfig\n    def __init__(self, config: LlamaConfig, mm_vision_tower=None, mm_hidden_size=None):"
        },
        {
            "comment": "Initializing the vision modules of NextChatLlamaModel by setting vision tower, image processor, and checking if mm projector is present. If not, it builds an mm_projector layer with specified dimensions.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_base.py\":34-53",
            "content": "        super(NextChatLlamaModel, self).__init__(config)\n        # if hasattr(config, \"mm_vision_tower\"):\n        #     vcfg = CLIPVisionConfig.from_pretrained(config.mm_vision_tower)\n        #     self.vision_tower = CLIPVisionModel(vcfg)\n        if hasattr(config, \"use_mm_proj\"):\n            self.mm_projector = self._build_mm_projector(config.mm_depth,\n                                                         config.mm_hidden_size, config.hidden_size) # nn.Linear(config.mm_hidden_size, config.hidden_size)\n    def initialize_vision_modules(self, vision_tower, mm_vision_select_layer, mm_depth=1,\n                                  pretrained_mm_projector=None, fsdp=None):\n        self.config.mm_vision_tower = vision_tower\n        image_processor = CLIPImageProcessor.from_pretrained(vision_tower)\n        if not hasattr(self, 'vision_tower'):\n            vision_tower = CLIPVisionModel.from_pretrained(vision_tower)\n        else:\n            vision_tower = self.vision_tower\n        vision_tower.requires_grad_(False)"
        },
        {
            "comment": "Initializing and configuring the mm_projector for the vision tower model based on input parameters, with potential loading of pretrained projector weights if provided.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_base.py\":54-71",
            "content": "        vision_tower = vision_tower.to(torch.float16) # TODO remove\n        self.vision_tower = [vision_tower] if fsdp is not None and len(fsdp)>0 else vision_tower\n        vision_config = vision_tower.config\n        num_patches = (vision_config.image_size // vision_config.patch_size) ** 2\n        self.config.use_mm_proj = True\n        self.config.mm_hidden_size = vision_config.hidden_size\n        self.config.mm_vision_select_layer = mm_vision_select_layer\n        if not hasattr(self, 'mm_projector'):\n            self.mm_projector = self._build_mm_projector(mm_depth,\n                                                         vision_config.hidden_size, self.config.hidden_size)\n            self.config.mm_depth=mm_depth\n            # self.mm_projector = nn.Linear(vision_config.hidden_size, self.config.hidden_size)\n        if pretrained_mm_projector is not None:\n            logging.info(f\"loading mm_projector from {pretrained_mm_projector}\")\n            mm_projector_weights = torch.load(pretrained_mm_projector, map_location='cpu')"
        },
        {
            "comment": "Line 72-96: This code defines a method `load_state_dict()` in a class that loads state dictionary of the model's mm_projector, updates the state of the projector and returns image_processor, image_token_len (number of patches), and vision_config. It also includes methods to get the vision tower, build an MM projector with specified depth, and encode input embeddings from input ids, images, location embeds, and original embeds parameters.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_base.py\":72-96",
            "content": "            info = self.mm_projector.load_state_dict({k.replace(\"model.mm_projector.\", \"\"): v for k, v in mm_projector_weights.items()})\n            logging.info(info)\n        return dict(\n            image_processor=image_processor,\n            image_token_len=num_patches,\n            vision_config=vision_config\n        )\n    def get_vision_tower(self):\n        vision_tower = self.vision_tower[0] if type(self.vision_tower) is list else self.vision_tower\n        return vision_tower\n    def _build_mm_projector(self, depth, in_size, out_size):\n        if depth is None or depth<=1:\n            return nn.Linear(in_size, out_size)\n        modules = [nn.Linear(in_size, out_size)]\n        for _ in range(1, depth):\n            modules.append(nn.GELU())\n            modules.append(nn.Linear(out_size, out_size))\n        return nn.Sequential(*modules)\n    def encode_input_embeds(self, input_ids, images, loc_embeds, orig_embeds_params, inputs_embeds=None):\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)"
        },
        {
            "comment": "This code retrieves the vision tower from the model and checks if it's not None, input_ids shape is not 1 or training mode is active, and images are provided. If these conditions are met, it gets the vision tower, extracts image features using a for loop for variable length images, and selects the hidden state at a specific layer to create the image feature.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_base.py\":98-113",
            "content": "        vision_tower = getattr(self, 'vision_tower', None)\n        if vision_tower is not None and (input_ids.shape[1] != 1 or self.training) and images is not None:\n            # TODO: this is a modified multimodal LLM -- Haotian Liu\n            vision_tower = self.get_vision_tower()  # HACK: for FSDP\n            with torch.no_grad():\n                if type(images) is list:\n                    # variable length images\n                    image_features = []\n                    for image in images:\n                        image_forward_out = vision_tower(image.unsqueeze(0), output_hidden_states=True)\n                        select_hidden_state_layer = getattr(self.config, \"mm_vision_select_layer\", -1)\n                        select_hidden_state = image_forward_out.hidden_states[select_hidden_state_layer]\n                        image_feature = select_hidden_state[:, 1:]\n                        image_features.append(image_feature)\n                else:\n                    image_forward_outs = vision_tower(images, output_hidden_states=True)"
        },
        {
            "comment": "The code handles multimodal input in a language model by extracting image features and merging them with textual embeddings. It checks if the sample is multimodal, and if not, replaces the image features with dummy values before combining the inputs.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_base.py\":114-129",
            "content": "                    select_hidden_state_layer = getattr(self.config, \"mm_vision_select_layer\", -1)\n                    select_hidden_state = image_forward_outs.hidden_states[select_hidden_state_layer]\n                    image_features = select_hidden_state[:, 1:]\n            if type(images) is list:\n                image_features = [self.mm_projector(image_feature)[0] for image_feature in image_features]\n            else:\n                image_features = self.mm_projector(image_features)\n            dummy_image_features = torch.zeros(256, 1024, device=inputs_embeds.device, dtype=inputs_embeds.dtype)\n            dummy_image_features = self.mm_projector(dummy_image_features)\n            new_input_embeds = []\n            cur_image_idx = 0\n            for cur_input_ids, cur_input_embeds in zip(input_ids, inputs_embeds):\n                if (cur_input_ids == vision_tower.config.im_patch_token).sum() == 0:\n                    # multimodal LLM, but the current sample is not multimodal\n                    cur_input_embeds = cur_input_embeds + (0. * dummy_image_features).sum()"
        },
        {
            "comment": "This code segment is for handling image start and end tokens in the input sequence. It checks if the number of start and end tokens is equal, raises a ValueError if not. Then, it iterates through the image start token positions, extracts the corresponding image features, and checks if the next token after the image is an image end token. If not, it raises a ValueError.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_base.py\":130-142",
            "content": "                    new_input_embeds.append(cur_input_embeds)\n                    continue\n                if vision_tower.config.use_im_start_end:\n                    cur_image_features = image_features[cur_image_idx]\n                    num_patches = cur_image_features.shape[0]\n                    if (cur_input_ids == vision_tower.config.im_start_token).sum() != (\n                            cur_input_ids == vision_tower.config.im_end_token).sum():\n                        raise ValueError(\"The number of image start tokens and image end tokens should be the same.\")\n                    image_start_tokens = torch.where(cur_input_ids == vision_tower.config.im_start_token)[0]\n                    for image_start_token_pos in image_start_tokens:\n                        cur_image_features = image_features[cur_image_idx].to(device=cur_input_embeds.device)\n                        num_patches = cur_image_features.shape[0]\n                        if cur_input_ids[image_start_token_pos + num_patches + 1] != vision_tower.config.im_end_token:"
        },
        {
            "comment": "Raises a ValueError if the image end token is not following the start token. If orig_embeds_params is not None, combines input embeds and image features to create cur_new_input_embeds. Otherwise, only combines specific parts of cur_input_embeds with cur_image_features.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_base.py\":143-152",
            "content": "                            raise ValueError(\"The image end token should follow the image start token.\")\n                        if orig_embeds_params is not None:\n                            cur_new_input_embeds = torch.cat((cur_input_embeds[:image_start_token_pos].detach(),\n                                                              cur_input_embeds[image_start_token_pos:image_start_token_pos + 1],\n                                                              cur_image_features, cur_input_embeds[\n                                                                                  image_start_token_pos + num_patches + 1:image_start_token_pos + num_patches + 2],\n                                                              cur_input_embeds[image_start_token_pos + num_patches + 2:].detach()), dim=0)\n                        else:\n                            cur_new_input_embeds = torch.cat((cur_input_embeds[:image_start_token_pos + 1], cur_image_features,\n                                                              cur_input_embeds[image_start_token_pos + num_patches + 1:]), dim=0)"
        },
        {
            "comment": "This code block checks if the number of image patch tokens in the input matches the number of image patches and ensures they are consecutive. It raises a ValueError if these conditions are not met. The orig_embeds_params parameter is checked for existence, but no action is taken based on its value.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_base.py\":153-165",
            "content": "                        cur_image_idx += 1\n                    new_input_embeds.append(cur_new_input_embeds)\n                else:\n                    cur_image_features = image_features[cur_image_idx]\n                    num_patches = cur_image_features.shape[0]\n                    if (cur_input_ids == vision_tower.config.im_patch_token).sum() != num_patches:\n                        raise ValueError(\"The number of image patch tokens should be the same as the number of image patches.\")\n                    masked_indices = torch.where(cur_input_ids == vision_tower.config.im_patch_token)[0]\n                    mask_index_start = masked_indices[0]\n                    if (masked_indices != torch.arange(mask_index_start, mask_index_start + num_patches, device=masked_indices.device,\n                                                       dtype=masked_indices.dtype)).any():\n                        raise ValueError(\"The image patch tokens should be consecutive.\")\n                    if orig_embeds_params is not None:"
        },
        {
            "comment": "This code concatenates input embeddings, image features, and (optionally) location embeddings into a single tensor, then returns the result. The function takes in `input_ids`, `attention_mask`, and other parameters for a model's forward pass.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_base.py\":166-184",
            "content": "                        cur_new_input_embeds = torch.cat((cur_input_embeds[:mask_index_start].detach(), cur_image_features,\n                                                          cur_input_embeds[mask_index_start + num_patches:].detach()), dim=0)\n                    else:\n                        cur_new_input_embeds = torch.cat(\n                            (cur_input_embeds[:mask_index_start], cur_image_features, cur_input_embeds[mask_index_start + num_patches:]),\n                            dim=0)\n                    new_input_embeds.append(cur_new_input_embeds)\n            inputs_embeds = torch.stack(new_input_embeds, dim=0)\n        # add the loc embeddings into the input_embeds\n        if (input_ids == vision_tower.config.box_token).sum() > 0 and loc_embeds is not None:\n            inputs_embeds[input_ids == vision_tower.config.box_token] = loc_embeds.type(inputs_embeds.dtype)\n        return inputs_embeds\n    def forward(\n            self,\n            input_ids: torch.LongTensor = None,\n            attention_mask: Optional[torch.Tensor] = None,"
        },
        {
            "comment": "This function defines a model for the NextChatLlama language generation. It takes various parameters such as past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, images, and loc_embeds. If no inputs_embeds are provided, it encodes the input embeddings using input_ids, images, loc_embeds, orig_embeds_params, and inputs_embeds. It then calls a superclass method to forward the computation.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_base.py\":185-205",
            "content": "            past_key_values: Optional[List[torch.FloatTensor]] = None,\n            inputs_embeds: Optional[torch.FloatTensor] = None,\n            use_cache: Optional[bool] = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            images: Optional[torch.FloatTensor] = None,\n            loc_embeds: Optional[torch.FloatTensor] = None,\n            return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPast]:\n        orig_embeds_params = getattr(self, 'orig_embeds_params', None)\n        # if orig_embeds_params is not None:\n        #     orig_embeds_params = orig_embeds_params[0]\n        #     with torch.no_grad():\n        #         self.get_input_embeddings().weight.data[:-2] = orig_embeds_params[:-2].data\n        if inputs_embeds is None:\n            inputs_embeds = self.encode_input_embeds(input_ids, images, loc_embeds, orig_embeds_params, inputs_embeds)\n        return super(NextChatLlamaModel, self).forward(\n            input_ids=None, attention_mask=attention_mask, past_key_values=past_key_values,"
        },
        {
            "comment": "The code defines a class \"NextChatForCausalLM\" which inherits from \"LlamaForCausalLM\". It initializes an instance of \"NextChatLlamaModel\" with a specified configuration and adds a linear layer to transform the output to match the vocabulary size. The code also includes location encoder and decoder layers for some specific functionality, and calls the \"post_init\" method to initialize weights and apply final processing in the forward function.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_base.py\":206-237",
            "content": "            inputs_embeds=inputs_embeds, use_cache=use_cache,\n            output_attentions=output_attentions, output_hidden_states=output_hidden_states,\n            return_dict=return_dict\n        )\nclass NextChatForCausalLM(LlamaForCausalLM):\n    config_class = NextChatConfig\n    def __init__(self, config: NextChatConfig):\n        super(LlamaForCausalLM, self).__init__(config)\n        self.model = NextChatLlamaModel(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        self.loc_encoder = nn.Sequential(\n            nn.Linear(4, config.hidden_size // 2),\n            nn.ReLU(),\n            nn.Linear(config.hidden_size // 2, config.hidden_size),\n        )\n        self.loc_decoder = nn.Sequential(\n            nn.Linear(config.hidden_size, config.hidden_size // 2),\n            nn.ReLU(),\n            nn.Linear(config.hidden_size // 2, 4)\n        )\n        # Initialize weights and apply final processing\n        self.post_init()\n    def forward(\n            self,\n            input_ids: torch.LongTensor = None,"
        },
        {
            "comment": "This code defines a method that takes input arguments and returns the output of a model. It initializes a vision tower from the model, handles labels by setting specific values to -100 if they match a box token, and sets output_attentions and output_hidden_states based on their provided values or default config settings. The method returns either a tuple or an object of CausalLMOutputWithPast depending on the input argument \"return_dict\".",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_base.py\":238-257",
            "content": "            attention_mask: Optional[torch.Tensor] = None,\n            past_key_values: Optional[List[torch.FloatTensor]] = None,\n            inputs_embeds: Optional[torch.FloatTensor] = None,\n            labels: Optional[torch.LongTensor] = None,\n            use_cache: Optional[bool] = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            images: Optional[torch.FloatTensor] = None,\n            return_dict: Optional[bool] = None,\n            loc_inputs=None,\n            loc_targets=None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        vision_tower = self.model.get_vision_tower()\n        if labels is not None:\n            labels[labels==vision_tower.config.box_token] = -100\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )"
        },
        {
            "comment": "This code snippet is part of the NextChat model. It takes input features, past key values, attention mask, and location embeds (if provided) as parameters to create outputs from a decoder. The hidden states are extracted from these outputs and passed through the language model head to obtain logits. If labels are given, it performs shift operations on logits and labels for loss calculation.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_base.py\":258-287",
            "content": "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        # TODO change to loc_inputs\n        loc_embeds = None\n        if loc_inputs is not None and len(loc_inputs) > 0:\n            loc_embeds = self.loc_encoder(loc_inputs)\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=True,\n            return_dict=return_dict,\n            images=images,\n            loc_embeds=loc_embeds,\n        )\n        hidden_states = outputs[0]\n        logits = self.lm_head(hidden_states)\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[:-1][..., :-1, :].contiguous()\n            shift_labels = labels[:-1][..., 1:].contiguous()"
        },
        {
            "comment": "Computing shift_logits and shift_labels using CrossEntropyLoss for classification task. The code flattens the tokens, enables parallelism by moving labels to device, and checks for loc_targets to compute cycle loss if present.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_base.py\":288-307",
            "content": "            # x, y = shift_labels[shift_labels > 29871], shift_logits.argmax(-1)[shift_labels > 29871]\n            # print((x==y).sum()/len(x))\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n            shift_labels = shift_labels.view(-1)\n            # Enable model/pipeline parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n        pred_locs = None\n        cycle_loss1= None\n        if loc_targets is not None and len(loc_targets)>0:\n            last_hidden_states = outputs.hidden_states[-1]\n            last_hidden_states = last_hidden_states.view(-1, last_hidden_states.size(-1))\n            loc_positions = ( (input_ids.flatten() == vision_tower.config.at_token)\n                             & (labels.flatten()>0) ).nonzero().flatten()\n            selected_hidden_states = last_hidden_states[loc_positions]\n            # pred_locs = self.loc_mlp(selected_hidden_states)"
        },
        {
            "comment": "The code snippet defines a model for object detection. It calculates the box loss between predicted and actual locations, and computes the cycle loss by comparing encoder outputs with input embeddings. The masked_loss function is applied to cycle losses. If loc_embeds are not None, it also calculates another cycle loss between decoded loc embeddings and input locations. Finally, it prints the mean of the diagonal values from box_iou matrix.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_base.py\":308-328",
            "content": "            pred_locs = self.loc_decoder(selected_hidden_states)\n            # pred_locs = F.relu(pred_locs)\n            # loc_targets_cxcywh = box_xyxy_to_cxcywh(loc_targets)\n            if len(pred_locs) != len(loc_targets):\n                torch.save([input_ids, labels, attention_mask, loc_inputs, loc_targets], \"tmp.pth\")\n            box_loss = self.box_loss(pred_locs, loc_targets)\n            loss += box_loss\n            print(torch.diag(box_iou(pred_locs, loc_targets)[0]).mean())\n            # cycle loss\n            pred_output_embeds = self.loc_encoder(pred_locs)\n            cycle_loss1 = F.mse_loss(pred_output_embeds, selected_hidden_states, reduction=\"none\")\n            cycle_loss1 = self.masked_loss(cycle_loss1, 1)\n            loss += cycle_loss1\n            # print()\n        # cycle loss\n        if loc_embeds is not None:\n            pred_input_locs = self.loc_decoder(loc_embeds)\n            cycle_loss2 = F.l1_loss(pred_input_locs, loc_inputs, reduction=\"none\")\n            cycle_loss2 = self.masked_loss(cycle_loss2, 1)"
        },
        {
            "comment": "This code defines a model that calculates loss functions for bounding box regression and object detection. The \"nextchat_base.py\" section updates the loss based on cycle_loss2, and returns the updated loss and other outputs depending on the return_dict parameter. The \"box_loss\" function computes the box loss by combining L1 loss and generalized intersection over union (GIoU) loss for bounding boxes. It also performs masked loss calculations, where only non-empty regions contribute to the total loss.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_base.py\":329-359",
            "content": "            loss += cycle_loss2\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n    def box_loss(self, src_boxes, target_boxes):\n        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction='none')\n        loss_bbox = self.masked_loss(loss_bbox, 1)\n        mask = (src_boxes[:, 2:] >= src_boxes[:, :2]).all(-1)\n        src_boxes = src_boxes[mask]\n        target_boxes = target_boxes[mask]\n        # if not mask.all():\n        #     print(len(mask)-mask.sum())\n        loss_giou = 1 - torch.diag(generalized_box_iou(\n            src_boxes,\n            target_boxes))\n        loss_giou = self.masked_loss(loss_giou, 1)\n        return loss_bbox*2 + loss_giou/5\n    def masked_loss(self, loss, n):"
        },
        {
            "comment": "This code defines a method to generate and append additional tokens for the input_ids and attention_mask, then passes it into a model for processing. This allows the model to process images along with text inputs by appending image-specific token ids to the input sequence. The output hidden states are also returned.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_base.py\":360-387",
            "content": "        mask = torch.ones_like(loss)\n        mask[-n:] = 1e-10\n        loss = (loss*mask).sum()/(mask.sum())\n        return loss\n    def generate_rec(\n        self,\n        **kwargs,\n    ):\n        input_ids = kwargs[\"input_ids\"]\n        attention_mask = kwargs[\"attention_mask\"]\n        use_cache = kwargs[\"use_cache\"]\n        images = kwargs[\"images\"]\n        to_append = torch.tensor([673, 29901, 32003, 32004,   29889,     2], dtype=input_ids.dtype, device=input_ids.device)\n        input_ids = torch.cat([input_ids, to_append.repeat(len(input_ids), 1)], 1)\n        to_append_attn = torch.ones([len(attention_mask), len(to_append)], dtype=attention_mask.dtype, device=attention_mask.device)\n        attention_mask = torch.cat([attention_mask, to_append_attn], 1)\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            past_key_values=None,\n            inputs_embeds=None,\n            use_cache=use_cache,\n            output_attentions=False,\n            output_hidden_states=True,"
        },
        {
            "comment": "This code is part of a model for an AI chat system. It prepares inputs for the generation step, handling image and text inputs. It retrieves hidden states from previous layers, selects relevant information, predicts locations based on this data, and returns these predictions as the output.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_base.py\":388-410",
            "content": "            return_dict=True,\n            images=images.type(self.model.vision_tower.vision_model.embeddings.patch_embedding.weight.dtype)\n        )\n        last_hidden_states = outputs.hidden_states[-1]\n        last_hidden_states = last_hidden_states.view(-1, last_hidden_states.size(-1))\n        vision_tower = self.model.get_vision_tower()\n        loc_positions = (input_ids.flatten() == vision_tower.config.at_token).nonzero().flatten()\n        selected_hidden_states = last_hidden_states[loc_positions]\n        pred_locs = self.loc_decoder(selected_hidden_states)\n        pred_locs = pred_locs\n        return pred_locs\n    # def prepare_inputs_for_generation(\n    #         self, input_ids, loc_inputs=None, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n    # ):\n    #     if past_key_values:\n    #         loc_ids = None\n    #         if input_ids.size(-1)>=2:\n    #             loc_ids = input_ids[:, -2]\n    #         input_ids = input_ids[:, -1:]\n    #\n    #     # if `inputs_embeds` are passed, we only want to use them in the 1st generation step"
        },
        {
            "comment": "This code segment is responsible for preparing model inputs for the NextChat base model. If `inputs_embeds` is not None and `past_key_values` is None, it creates a dictionary `model_inputs` containing only \"inputs_embeds\". Otherwise, it embeds tokens using the model's embedding layer, incorporates location information if `loc_ids` is not None and the token is an attention token, and updates the `inputs_embeds`. Finally, it updates `model_inputs` with both `past_key_values` and `use_cache` from the input kwargs.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_base.py\":411-430",
            "content": "    #     if inputs_embeds is not None and past_key_values is None:\n    #         model_inputs = {\"inputs_embeds\": inputs_embeds}\n    #     else:\n    #         inputs_embeds = self.model.embed_tokens(input_ids)\n    #         hidden_states = kwargs.pop(\"hidden_states\", None)\n    #         vision_tower = self.model.get_vision_tower()\n    #         # need to incorporate location information\n    #         if loc_ids is not None and (loc_ids==vision_tower.config.at_token).any():\n    #             mask = loc_ids==vision_tower.config.at_token\n    #             loc_embeds = hidden_states[-1][mask][:, -1:, :]\n    #             loc_embeds = loc_embeds.type(inputs_embeds.dtype)\n    #             pred_locs = self.loc_decoder(loc_embeds)\n    #             loc_embeds = self.loc_encoder(pred_locs)\n    #             inputs_embeds[mask] = loc_embeds\n    #         model_inputs = {\"inputs_embeds\": inputs_embeds}\n    #\n    #     model_inputs.update(\n    #         {\n    #             \"past_key_values\": past_key_values,\n    #             \"use_cache\": kwargs.get(\"use_cache\"),"
        },
        {
            "comment": "This function prepares the model inputs for generation. If `inputs_embeds` is provided and there are no past key values, it uses the `inputs_embeds`. Otherwise, it uses `input_ids`. It also updates the `model_inputs` with `past_key_values`, `use_cache`, `attention_mask`, and `images`.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_base.py\":431-460",
            "content": "    #             \"attention_mask\": attention_mask,\n    #             \"images\": kwargs.get(\"images\", None),\n    #             \"loc_inputs\": loc_inputs,\n    #         }\n    #     )\n    #     return model_inputs\n    def prepare_inputs_for_generation(\n            self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n    ):\n        if past_key_values:\n            input_ids = input_ids[:, -1:]\n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            model_inputs = {\"input_ids\": input_ids}\n        model_inputs.update(\n            {\n                \"past_key_values\": past_key_values,\n                \"use_cache\": kwargs.get(\"use_cache\"),\n                \"attention_mask\": attention_mask,\n                \"images\": kwargs.get(\"images\", None),\n            }\n        )\n        return model_inputs\n    def _update_model_kwargs_for_generation("
        },
        {
            "comment": "This code defines a function for initializing the vision tokenizer in a model. It sets up a vision tower, updates its configuration based on mm_use_im_start_end, and adds a default image patch token to the tokenizer. This is used in models that incorporate visual information.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_base.py\":461-479",
            "content": "        self,\n        outputs,\n        model_kwargs,\n        is_encoder_decoder=False,\n        standardize_cache_format=False,\n    ):\n        model_kwargs = super(NextChatForCausalLM, self)._update_model_kwargs_for_generation(outputs,\n                                                                                model_kwargs,\n                                                                                is_encoder_decoder,\n                                                                                standardize_cache_format)\n        model_kwargs.update({\"hidden_states\": outputs.hidden_states})\n        return model_kwargs\n    def initialize_vision_tokenizer(self, mm_use_im_start_end, tokenizer, device,\n                                    tune_mm_mlp_adapter=False, pretrain_mm_mlp_adapter=None):\n        vision_tower = self.model.get_vision_tower()\n        vision_config = vision_tower.config\n        vision_config.use_im_start_end = mm_use_im_start_end\n        tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)"
        },
        {
            "comment": "This code resizes the token embeddings to accommodate new tokens added by the tokenizer. It sets the vision config values for IM start, end, at, and box tokens. If new tokens are added, it calculates the average input and output embeddings for existing tokens and assigns these averages to the newly added tokens.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_base.py\":480-497",
            "content": "        self.resize_token_embeddings(len(tokenizer))\n        if mm_use_im_start_end:\n            num_new_tokens = tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, DEFAULT_AT_TOKEN, DEFAULT_BOXES_TOKEN], special_tokens=True)\n            self.resize_token_embeddings(len(tokenizer))\n            vision_config.im_start_token, vision_config.im_end_token, vision_config.at_token, vision_config.box_token = tokenizer.convert_tokens_to_ids(\n                [DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, DEFAULT_AT_TOKEN, DEFAULT_BOXES_TOKEN])\n            if num_new_tokens > 0:\n                input_embeddings = self.get_input_embeddings().weight.data\n                output_embeddings = self.get_output_embeddings().weight.data\n                input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n                    dim=0, keepdim=True)\n                output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n                    dim=0, keepdim=True)\n                input_embeddings[-num_new_tokens:] = input_embeddings_avg"
        },
        {
            "comment": "The code is initializing input embeddings with pre-trained ones, if available. If not, it sets requires_grad to True for input embeddings and averages output embeddings for the new tokens. Also, it tunes MMMLP adapter parameters if specified.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_base.py\":498-514",
            "content": "                output_embeddings[-num_new_tokens:] = output_embeddings_avg\n            if tune_mm_mlp_adapter:\n                self.model.orig_embeds_params = [self.get_input_embeddings().weight.data.clone().to(device=device)]\n                for p in self.get_input_embeddings().parameters():\n                    p.requires_grad = True\n                for p in self.get_output_embeddings().parameters():\n                    p.requires_grad = False\n            if pretrain_mm_mlp_adapter:\n                mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')\n                embed_tokens_weight = mm_projector_weights['model.embed_tokens.weight']\n                assert num_new_tokens == 3\n                if input_embeddings.shape == embed_tokens_weight.shape:\n                    input_embeddings[-num_new_tokens:] = embed_tokens_weight[-num_new_tokens:]\n                elif embed_tokens_weight.shape[0] == num_new_tokens:\n                    input_embeddings[-num_new_tokens:] = embed_tokens_weight"
        },
        {
            "comment": "This code segment checks the shape of embed_tokens_weight, input_embeddings and counts the number of new tokens. If the shapes are unexpected, it raises a ValueError with information about the unexpected shapes and number of new tokens. Then, it assigns the ID for the default image patch token to vision_config's im_patch_token variable.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/nextchat/nextchat_base.py\":515-519",
            "content": "                else:\n                    raise ValueError(\n                        f\"Unexpected embed_tokens_weight shape. Pretrained: {embed_tokens_weight.shape}. Current: {input_embeddings.shape}. Numer of new tokens: {num_new_tokens}.\")\n        vision_config.im_patch_token = tokenizer.convert_tokens_to_ids([DEFAULT_IMAGE_PATCH_TOKEN])[0]"
        }
    ]
}