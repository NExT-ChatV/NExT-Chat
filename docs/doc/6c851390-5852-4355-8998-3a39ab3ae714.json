{
    "summary": "The code prepares the environment for training a machine learning model, handles training and evaluation tasks, logs metrics, saves models, manages errors, and performs multi-predict if enabled.",
    "details": [
        {
            "comment": "This code is importing necessary modules and preparing the training environment for a machine learning model. It sets up the logging, loads the pre-trained model, and processes the data for training. The main function handles arguments and initializes the model and data processor.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/pipeline/finetune.py\":0-35",
            "content": "import os\nimport sys\nimport logging\nimport pathlib\nimport typing\nimport warnings\nSLURM_ENV = {k: v for k, v in os.environ.items() if 'SLURM' in k}\nif SLURM_ENV:\n    print(f\"SLURM_ENV: {SLURM_ENV}\")\nproject_path = pathlib.Path(__file__).parent.parent.parent\nsys.path.append(str(project_path))\nimport torch\nimport torch.cuda\nfrom mllm.config import prepare_args\nfrom mllm.models import load_pretrained\nfrom mllm.utils import print_trainable_params\nfrom mllm.engine import prepare_trainer_collator\nfrom mllm.dataset import prepare_data, prepare_target_processor\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\nlogging.basicConfig(\n    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n    datefmt=\"%m/%d/%Y %H:%M:%S\",\n    handlers=[logging.StreamHandler(sys.stdout), ],\n)\ndef main():\n    cfg, training_args = prepare_args()\n    model, preprocessor = load_pretrained(cfg.model_args, training_args)\n    # Some ugly codes to inject target_processor into preprocessor.\n    # maybe effect model. (e.g. add special token; resize embedding)"
        },
        {
            "comment": "The code initializes a trainer for finetuning a model using the provided arguments, preprocessor, dataset, and compute metrics. It handles both training and evaluation tasks depending on the specified parameters, ensuring proper data handling with a data collator.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/pipeline/finetune.py\":36-58",
            "content": "    model, preprocessor = prepare_target_processor(model, preprocessor, cfg.model_args, training_args)\n    print_trainable_params(model)\n    # Prepare data_collator\n    collator_kwargs = cfg.data_args.collator_kwargs\n    trainer_cls, data_collator_dict = prepare_trainer_collator(cfg.model_args, preprocessor, collator_kwargs)\n    dataset, compute_metrics = prepare_data(cfg.data_args, cfg.model_args, training_args, preprocessor)\n    # Initialize Trainer\n    trainer = trainer_cls(\n        model=model,\n        args=training_args,\n        tokenizer=preprocessor['text'],\n        train_dataset=dataset['train'] if training_args.do_train else None,\n        eval_dataset=dataset['validation'] if training_args.do_eval else None,\n        compute_metrics=compute_metrics if training_args.predict_with_generate else None,\n        **data_collator_dict,\n    )\n    # Training\n    if training_args.do_train:\n        try:\n            if (not training_args.overwrite_output_dir) and list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\")):"
        },
        {
            "comment": "This code snippet trains a model using a trainer, logging metrics, saving the model, and handling potential RuntimeErrors. If a RuntimeError occurs, it prints the error message, attempts to display GPU memory summary, re-raises the exception, saves the trainer's state, and plots loss. It also tries to save the configuration file (cfg) to the output directory.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/pipeline/finetune.py\":59-82",
            "content": "                train_result = trainer.train(resume_from_checkpoint=True)\n            else:\n                train_result = trainer.train()\n            trainer.log_metrics(\"train\", train_result.metrics)  # noqa\n            trainer.save_metrics(\"train\", train_result.metrics)  # noqa\n            trainer.save_model()\n        except RuntimeError as e:\n            print(f\"got RuntimeError: {e.args}\")\n            try:\n                print(f\"#### device {training_args.local_rank} summary ####\\n{torch.cuda.memory_summary(training_args.local_rank)}\")\n            except Exception as inner_e:\n                print(f\"get Exception when show cuda summary: {inner_e.args}\")\n            raise e\n        finally:\n            trainer.save_state()  # noqa\n            trainer.plot_loss()\n    # save cfg to output_dir\n    try:\n        output_dir = training_args.output_dir\n        pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n        cfg.dump(os.path.join(output_dir, \"cfg.py\"))\n    except Exception as e:\n        warnings.warn(f'try to save cfg to output_dir, but get exception {e.args}')"
        },
        {
            "comment": "This code segment is setting default values for model.generate function arguments and handling specific token IDs based on the provided configuration. It also checks if evaluation is enabled, comparing test and eval collators in the trainer object.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/pipeline/finetune.py\":84-99",
            "content": "    # Keyword arguments for `model.generate`\n    gen_kwargs = dict(cfg.data_args.gen_kwargs)\n    gen_kwargs.setdefault('use_cache', True)\n    # important for use model.generate in batch mode. some model config with wrong special_token_id\n    # (e.g. shikra generationConfig set pad_token_id to -1)\n    if hasattr(cfg.model_args, 'gen_kwargs_set_pad_token_id') and cfg.model_args.gen_kwargs_set_pad_token_id:\n        gen_kwargs['pad_token_id'] = preprocessor['text'].pad_token_id\n    if hasattr(cfg.model_args, 'gen_kwargs_set_bos_token_id') and cfg.model_args.gen_kwargs_set_bos_token_id:\n        gen_kwargs['bos_token_id'] = preprocessor['text'].bos_token_id\n    if hasattr(cfg.model_args, 'gen_kwargs_set_eos_token_id') and cfg.model_args.gen_kwargs_set_eos_token_id:\n        gen_kwargs['eos_token_id'] = preprocessor['text'].eos_token_id\n    # Evaluation\n    if training_args.do_eval:\n        if hasattr(trainer, '_test_collator') and hasattr(trainer, '_eval_collator') \\\n                and trainer._test_collator != trainer._eval_collator:  # noqa"
        },
        {
            "comment": "The code snippet handles evaluation and prediction in a machine learning training process. It logs metrics, saves results, and performs multi-predict if enabled. However, it warns about using the same collator for eval and test since trainer.predict only uses test_collator.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/pipeline/finetune.py\":100-117",
            "content": "            warnings.warn('[WARNING!!!] use different collator for eval and test. but do_eval and '\n                          'do_predict both use trainer.predict (i.e. only test_collator is used.)')\n        eval_results = trainer.predict(dataset['validation'], metric_key_prefix=\"eval\", **gen_kwargs)\n        trainer.log_metrics(\"eval\", eval_results.metrics)  # noqa\n        trainer.save_metrics(\"eval\", eval_results.metrics)  # noqa\n        trainer.save_prediction(eval_results, file_key_prefix='eval')\n    # Predict\n    if training_args.do_predict:\n        predict_results = trainer.predict(dataset['test'], metric_key_prefix=\"test\", **gen_kwargs)\n        trainer.log_metrics(\"test\", predict_results.metrics)  # noqa\n        trainer.save_metrics(\"test\", predict_results.metrics)  # noqa\n        trainer.save_prediction(predict_results, file_key_prefix='test')\n    # Multi Predict\n    if training_args.do_multi_predict:\n        old_compute_metrics = trainer.compute_metrics\n        multitest = dataset['multitest']"
        },
        {
            "comment": "Code is iterating over the 'multitest' dictionary, processing each set by calling the trainer's predict method, logging and saving metrics and predictions. It also temporarily changes the compute_metrics for each iteration before returning to the original value.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/pipeline/finetune.py\":118-140",
            "content": "        multitest = typing.cast(dict, multitest)\n        for _idx, (k, item) in enumerate(multitest.items()):\n            print(f'processing multitest set {_idx}/{len(multitest)}: {k}')\n            _ds = item['dataset']\n            _compute_metrics = item['compute_metric']\n            _prefix = f\"multitest_{k}\"\n            trainer.compute_metrics = _compute_metrics\n            _pred_results = trainer.predict(_ds, metric_key_prefix=_prefix, **gen_kwargs)\n            trainer.log_metrics(_prefix, _pred_results.metrics)  # noqa\n            trainer.save_metrics(_prefix, _pred_results.metrics)  # noqa\n            trainer.save_prediction(_pred_results, file_key_prefix=_prefix)\n        trainer.compute_metrics = old_compute_metrics\n# noinspection PyUnusedLocal\ndef _mp_fn(index):\n    # For xla_spawn (TPUs)\n    main()\nif __name__ == \"__main__\":\n    main()"
        }
    ]
}