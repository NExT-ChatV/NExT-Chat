{
    "summary": "The code creates a Transformer model for image attending with Multi-Head Attention, window-based attention, and optional relative position embeddings. It utilizes 'Sam' model and 'PositionEmbeddingRandom' to create embeddings for points, boxes, and masks. The model performs mask token generation, upscaling, and IOU prediction heads through hypernetworks MLPs and multiple masks using embedding point prompts in image encoding.",
    "details": [
        {
            "comment": "This code defines a module for a Multilayer Perceptron (MLP) block with input and output embedding dimensions, and an activation function. It consists of two linear layers and applies the specified activation function to the output of the first layer before passing it through the second layer.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":0-34",
            "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Optional, Tuple, Type, List, Any, Dict\nfrom torch import Tensor, nn\nimport numpy as np\nimport math\nclass MLPBlock(nn.Module):\n    def __init__(\n        self,\n        embedding_dim: int,\n        mlp_dim: int,\n        act: Type[nn.Module] = nn.GELU,\n    ) -> None:\n        super().__init__()\n        self.lin1 = nn.Linear(embedding_dim, mlp_dim)\n        self.lin2 = nn.Linear(mlp_dim, embedding_dim)\n        self.act = act()\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.lin2(self.act(self.lin1(x)))\n# From https://github.com/facebookresearch/detectron2/blob/main/detectron2/layers/batch_norm.py # noqa\n# Itself from https://github.com/facebookresearch/ConvNeXt/blob/d1fa8f6fef0a165b27399986cc2bdacc92777e40/models/convnext.py#L119  # noqa"
        },
        {
            "comment": "The LayerNorm2d class is a layer normalization operation applied to 2D tensors, with adjustable weight and bias parameters. The TwoWayTransformer class is a transformer decoder for attending to an input image, featuring configurable depth, embedding dimension, number of heads, MLP dimension, activation function (default ReLU), and attention downsample rate.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":35-69",
            "content": "class LayerNorm2d(nn.Module):\n    def __init__(self, num_channels: int, eps: float = 1e-6) -> None:\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(num_channels))\n        self.bias = nn.Parameter(torch.zeros(num_channels))\n        self.eps = eps\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        u = x.mean(1, keepdim=True)\n        s = (x - u).pow(2).mean(1, keepdim=True)\n        x = (x - u) / torch.sqrt(s + self.eps)\n        x = self.weight[:, None, None] * x + self.bias[:, None, None]\n        return x\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nclass TwoWayTransformer(nn.Module):\n    def __init__(\n        self,\n        depth: int,\n        embedding_dim: int,\n        num_heads: int,\n        mlp_dim: int,\n        activation: Type[nn.Module] = nn.ReLU,\n        attention_downsample_rate: int = 2,\n    ) -> None:\n        \"\"\"\n        A transformer decoder that attends to an input image using"
        },
        {
            "comment": "This code defines a Transformer model with a specific architecture, including the number of layers (depth), embedding dimension, number of attention heads, and MLP block dimensions. The `TwoWayAttentionBlock` class is used for each layer, which may or may not require positional embedding depending on the current layer's position.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":70-95",
            "content": "        queries whose positional embedding is supplied.\n        Args:\n          depth (int): number of layers in the transformer\n          embedding_dim (int): the channel dimension for the input embeddings\n          num_heads (int): the number of heads for multihead attention. Must\n            divide embedding_dim\n          mlp_dim (int): the channel dimension internal to the MLP block\n          activation (nn.Module): the activation to use in the MLP block\n        \"\"\"\n        super().__init__()\n        self.depth = depth\n        self.embedding_dim = embedding_dim\n        self.num_heads = num_heads\n        self.mlp_dim = mlp_dim\n        self.layers = nn.ModuleList()\n        for i in range(depth):\n            self.layers.append(\n                TwoWayAttentionBlock(\n                    embedding_dim=embedding_dim,\n                    num_heads=num_heads,\n                    mlp_dim=mlp_dim,\n                    activation=activation,\n                    attention_downsample_rate=attention_downsample_rate,\n                    skip_first_layer_pe=(i == 0),"
        },
        {
            "comment": "This code defines a class for a model that takes in an image embedding, image positional encoding, and point embeddings as input. It applies attention to the image and then processes both the point and image embeddings using layer normalization. The resulting processed point_embedding and image_embedding are returned.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":96-123",
            "content": "                )\n            )\n        self.final_attn_token_to_image = Attention1(\n            embedding_dim, num_heads, downsample_rate=attention_downsample_rate\n        )\n        self.norm_final_attn = nn.LayerNorm(embedding_dim)\n    def forward(\n        self,\n        image_embedding: Tensor,\n        image_pe: Tensor,\n        point_embedding: Tensor,\n    ) -> Tuple[Tensor, Tensor]:\n        \"\"\"\n        Args:\n          image_embedding (torch.Tensor): image to attend to. Should be shape\n            B x embedding_dim x h x w for any h and w.\n          image_pe (torch.Tensor): the positional encoding to add to the image. Must\n            have the same shape as image_embedding.\n          point_embedding (torch.Tensor): the embedding to add to the query points.\n            Must have shape B x N_points x embedding_dim for any N_points.\n        Returns:\n          torch.Tensor: the processed point_embedding\n          torch.Tensor: the processed image_embedding\n        \"\"\"\n        # BxCxHxW -> BxHWxC == B x N_image_tokens x C"
        },
        {
            "comment": "The code defines a TwoWayAttentionBlock class that applies transformer blocks to perform attention between points and images. It flattens the input, prepares queries and keys, iterates over the defined layers in the block, performs attention from points to image, adds the result back to queries and normalizes them before returning.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":124-157",
            "content": "        bs, c, h, w = image_embedding.shape\n        image_embedding = image_embedding.flatten(2).permute(0, 2, 1)\n        image_pe = image_pe.flatten(2).permute(0, 2, 1)\n        # Prepare queries\n        queries = point_embedding\n        keys = image_embedding\n        # Apply transformer blocks and final layernorm\n        for layer in self.layers:\n            queries, keys = layer(\n                queries=queries,\n                keys=keys,\n                query_pe=point_embedding,\n                key_pe=image_pe,\n            )\n        # Apply the final attention layer from the points to the image\n        q = queries + point_embedding\n        k = keys + image_pe\n        attn_out = self.final_attn_token_to_image(q=q, k=k, v=keys)\n        queries = queries + attn_out\n        queries = self.norm_final_attn(queries)\n        return queries, keys\nclass TwoWayAttentionBlock(nn.Module):\n    def __init__(\n        self,\n        embedding_dim: int,\n        num_heads: int,\n        mlp_dim: int = 2048,\n        activation: Type[nn.Module] = nn.ReLU,"
        },
        {
            "comment": "This code defines a transformer block with four layers: (1) self-attention on sparse inputs, (2) cross attention from sparse to dense inputs, (3) mlp block on sparse inputs, and (4) cross attention from dense to sparse inputs. It takes in arguments for embedding dimension, number of heads, MLP dimension, activation function, and whether to skip the first layer PE.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":158-181",
            "content": "        attention_downsample_rate: int = 2,\n        skip_first_layer_pe: bool = False,\n    ) -> None:\n        \"\"\"\n        A transformer block with four layers: (1) self-attention of sparse\n        inputs, (2) cross attention of sparse inputs to dense inputs, (3) mlp\n        block on sparse inputs, and (4) cross attention of dense inputs to sparse\n        inputs.\n        Arguments:\n          embedding_dim (int): the channel dimension of the embeddings\n          num_heads (int): the number of heads in the attention layers\n          mlp_dim (int): the hidden dimension of the mlp block\n          activation (nn.Module): the activation of the mlp block\n          skip_first_layer_pe (bool): skip the PE on the first layer\n        \"\"\"\n        super().__init__()\n        self.self_attn = Attention1(embedding_dim, num_heads)\n        self.norm1 = nn.LayerNorm(embedding_dim)\n        self.cross_attn_token_to_image = Attention1(\n            embedding_dim, num_heads, downsample_rate=attention_downsample_rate\n        )\n        self.norm2 = nn.LayerNorm(embedding_dim)"
        },
        {
            "comment": "The code initializes a model with several layers, including an MLPBlock, two LayerNorm layers, and an Attention1 layer. The forward method performs self-attention on the queries, followed by cross-attention between queries and keys. If skip_first_layer_pe is true, it skips the first layer's PE addition; otherwise, it adds query position embeddings and performs attention before adding the result back to the queries. The queries are then passed through LayerNorm layers. This code seems to be part of a transformer model for processing tokens and image embeddings.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":183-209",
            "content": "        self.mlp = MLPBlock(embedding_dim, mlp_dim, activation)\n        self.norm3 = nn.LayerNorm(embedding_dim)\n        self.norm4 = nn.LayerNorm(embedding_dim)\n        self.cross_attn_image_to_token = Attention1(\n            embedding_dim, num_heads, downsample_rate=attention_downsample_rate\n        )\n        self.skip_first_layer_pe = skip_first_layer_pe\n    def forward(\n        self, queries: Tensor, keys: Tensor, query_pe: Tensor, key_pe: Tensor\n    ) -> Tuple[Tensor, Tensor]:\n        # Self attention block\n        if self.skip_first_layer_pe:\n            queries = self.self_attn(q=queries, k=queries, v=queries)\n        else:\n            q = queries + query_pe\n            attn_out = self.self_attn(q=q, k=q, v=queries)\n            queries = queries + attn_out\n        queries = self.norm1(queries)\n        # Cross attention block, tokens attending to image embedding\n        q = queries + query_pe\n        k = keys + key_pe\n        attn_out = self.cross_attn_token_to_image(q=q, k=k, v=keys)\n        queries = queries + attn_out"
        },
        {
            "comment": "The code defines an attention layer that downscales the size of the embedding after projection to queries, keys, and values. It initializes the attention layer with given dimensions, number of heads, and a downsample rate. The internal_dim is calculated by dividing the embedding_dim by the downsample_rate. It also includes a linear projection (q_proj) for the queries.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":210-245",
            "content": "        queries = self.norm2(queries)\n        # MLP block\n        mlp_out = self.mlp(queries)\n        queries = queries + mlp_out\n        queries = self.norm3(queries)\n        # Cross attention block, image embedding attending to tokens\n        q = queries + query_pe\n        k = keys + key_pe\n        attn_out = self.cross_attn_image_to_token(q=k, k=q, v=queries)\n        keys = keys + attn_out\n        keys = self.norm4(keys)\n        return queries, keys\nclass Attention1(nn.Module):\n    \"\"\"\n    An attention layer that allows for downscaling the size of the embedding\n    after projection to queries, keys, and values.\n    \"\"\"\n    def __init__(\n        self,\n        embedding_dim: int,\n        num_heads: int,\n        downsample_rate: int = 1,\n    ) -> None:\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.internal_dim = embedding_dim // downsample_rate\n        self.num_heads = num_heads\n        assert self.internal_dim % num_heads == 0, \"num_heads must divide embedding_dim.\"\n        self.q_proj = nn.Linear(embedding_dim, self.internal_dim)"
        },
        {
            "comment": "This code initializes two linear layers for key and value projections, as well as an output projection layer. The `_separate_heads` function reshapes the input tensor into multiple heads, while the `_recombine_heads` function reverses this process. In the forward pass, it performs input projections, separates inputs into heads, and computes attention before recombining and returning the result.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":246-272",
            "content": "        self.k_proj = nn.Linear(embedding_dim, self.internal_dim)\n        self.v_proj = nn.Linear(embedding_dim, self.internal_dim)\n        self.out_proj = nn.Linear(self.internal_dim, embedding_dim)\n    def _separate_heads(self, x: Tensor, num_heads: int) -> Tensor:\n        b, n, c = x.shape\n        x = x.reshape(b, n, num_heads, c // num_heads)\n        return x.transpose(1, 2)  # B x N_heads x N_tokens x C_per_head\n    def _recombine_heads(self, x: Tensor) -> Tensor:\n        b, n_heads, n_tokens, c_per_head = x.shape\n        x = x.transpose(1, 2)\n        return x.reshape(b, n_tokens, n_heads * c_per_head)  # B x N_tokens x C\n    def forward(self, q: Tensor, k: Tensor, v: Tensor) -> Tensor:\n        # Input projections\n        q = self.q_proj(q)\n        k = self.k_proj(k)\n        v = self.v_proj(v)\n        # Separate into heads\n        q = self._separate_heads(q, self.num_heads)\n        k = self._separate_heads(k, self.num_heads)\n        v = self._separate_heads(v, self.num_heads)\n        # Attention\n        _, _, _, c_per_head = q.shape"
        },
        {
            "comment": "This code snippet defines a class called `ImageEncoderViT` which inherits from the `nn.Module` class in PyTorch. This module is used to encode an image using the Vision Transformer architecture. It takes in parameters such as image size, patch size, number of input channels, embedding dimension, depth, number of heads, mlp ratio, and output channels. The code contains a method that performs multi-head attention on patches extracted from the input image and returns the encoded output.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":273-305",
            "content": "        attn = q @ k.permute(0, 1, 3, 2)  # B x N_heads x N_tokens x N_tokens\n        attn = attn / math.sqrt(c_per_head)\n        attn = torch.softmax(attn, dim=-1)\n        # Get output\n        out = attn @ v\n        out = self._recombine_heads(out)\n        out = self.out_proj(out)\n        return out\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n# This class and its supporting functions below lightly adapted from the ViTDet backbone available at: https://github.com/facebookresearch/detectron2/blob/main/detectron2/modeling/backbone/vit.py # noqa\nclass ImageEncoderViT(nn.Module):\n    def __init__(\n        self,\n        img_size: int = 1024,\n        patch_size: int = 16,\n        in_chans: int = 3,\n        embed_dim: int = 768,\n        depth: int = 12,\n        num_heads: int = 12,\n        mlp_ratio: float = 4.0,\n        out_chans: int = 256,\n        qkv_bias: bool = True,"
        },
        {
            "comment": "This function initializes the parameters for a vision transformer model, including input image size, patch size, number of input channels, embedding dimension, depth of ViT, number of attention heads, and more. It also defines the normalization layer as LayerNorm and activation layer as GELU.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":306-327",
            "content": "        norm_layer: Type[nn.Module] = nn.LayerNorm,\n        act_layer: Type[nn.Module] = nn.GELU,\n        use_abs_pos: bool = True,\n        use_rel_pos: bool = False,\n        rel_pos_zero_init: bool = True,\n        window_size: int = 0,\n        global_attn_indexes: Tuple[int, ...] = (),\n    ) -> None:\n        \"\"\"\n        Args:\n            img_size (int): Input image size.\n            patch_size (int): Patch size.\n            in_chans (int): Number of input image channels.\n            embed_dim (int): Patch embedding dimension.\n            depth (int): Depth of ViT.\n            num_heads (int): Number of attention heads in each ViT block.\n            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n            norm_layer (nn.Module): Normalization layer.\n            act_layer (nn.Module): Activation layer.\n            use_abs_pos (bool): If True, use absolute positional embeddings.\n            use_rel_pos (bool): If True, add relative positional embeddings to the attention map."
        },
        {
            "comment": "The code initializes a model with specified parameters, such as image size, patch size, embedding dimensions, and block depth. It creates an optional absolute positional embedding and stores each block in a module list for later use.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":328-354",
            "content": "            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n            window_size (int): Window size for window attention blocks.\n            global_attn_indexes (list): Indexes for blocks using global attention.\n        \"\"\"\n        super().__init__()\n        self.img_size = img_size\n        self.patch_embed = PatchEmbed(\n            kernel_size=(patch_size, patch_size),\n            stride=(patch_size, patch_size),\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n        )\n        self.pos_embed: Optional[nn.Parameter] = None\n        if use_abs_pos:\n            # Initialize absolute positional embedding with pretrain image size.\n            self.pos_embed = nn.Parameter(\n                torch.zeros(1, img_size // patch_size, img_size // patch_size, embed_dim)\n            )\n        self.blocks = nn.ModuleList()\n        for i in range(depth):\n            block = Block(\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,"
        },
        {
            "comment": "This code initializes a Vision Transformer model with patch embedding, multiple attention blocks, and a neck. The blocks are created with specified parameters including patch size, embedding dimension, number of heads, and window size. The neck consists of convolutional layers for feature extraction and normalization.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":355-388",
            "content": "                qkv_bias=qkv_bias,\n                norm_layer=norm_layer,\n                act_layer=act_layer,\n                use_rel_pos=use_rel_pos,\n                rel_pos_zero_init=rel_pos_zero_init,\n                window_size=window_size if i not in global_attn_indexes else 0,\n                input_size=(img_size // patch_size, img_size // patch_size),\n            )\n            self.blocks.append(block)\n        self.neck = nn.Sequential(\n            nn.Conv2d(\n                embed_dim,\n                out_chans,\n                kernel_size=1,\n                bias=False,\n            ),\n            LayerNorm2d(out_chans),\n            nn.Conv2d(\n                out_chans,\n                out_chans,\n                kernel_size=3,\n                padding=1,\n                bias=False,\n            ),\n            LayerNorm2d(out_chans),\n        )\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.patch_embed(x)\n        if self.pos_embed is not None:\n            x = x + self.pos_embed\n        for blk in self.blocks:"
        },
        {
            "comment": "This code defines a transformer block class that supports window attention and residual propagation blocks. It takes in parameters such as the number of input channels, attention heads, mlp ratio, etc., and returns an instance of the transformer block with optional features like relative position embeddings and window-based attention.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":389-420",
            "content": "            x = blk(x)\n        x = self.neck(x.permute(0, 3, 1, 2))\n        return x\nclass Block(nn.Module):\n    \"\"\"Transformer blocks with support of window attention and residual propagation blocks\"\"\"\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = True,\n        norm_layer: Type[nn.Module] = nn.LayerNorm,\n        act_layer: Type[nn.Module] = nn.GELU,\n        use_rel_pos: bool = False,\n        rel_pos_zero_init: bool = True,\n        window_size: int = 0,\n        input_size: Optional[Tuple[int, int]] = None,\n    ) -> None:\n        \"\"\"\n        Args:\n            dim (int): Number of input channels.\n            num_heads (int): Number of attention heads in each ViT block.\n            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n            norm_layer (nn.Module): Normalization layer.\n            act_layer (nn.Module): Activation layer.\n            use_rel_pos (bool): If True, add relative positional embeddings to the attention map."
        },
        {
            "comment": "This code initializes a new class that extends the `nn.Module` and contains an attention block, normalization layers, and MLP block. The attention block uses relative positional parameters if specified, and partitions input into windows of size determined by `window_size`. The shortcut variable holds the input for use in residual connections during forward pass.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":421-446",
            "content": "            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n            window_size (int): Window size for window attention blocks. If it equals 0, then\n                use global attention.\n            input_size (tuple(int, int) or None): Input resolution for calculating the relative\n                positional parameter size.\n        \"\"\"\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            use_rel_pos=use_rel_pos,\n            rel_pos_zero_init=rel_pos_zero_init,\n            input_size=input_size if window_size == 0 else (window_size, window_size),\n        )\n        self.norm2 = norm_layer(dim)\n        self.mlp = MLPBlock(embedding_dim=dim, mlp_dim=int(dim * mlp_ratio), act=act_layer)\n        self.window_size = window_size\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        shortcut = x\n        x = self.norm1(x)\n        # Window partition"
        },
        {
            "comment": "This code defines a multi-head attention block with relative position embeddings. It takes an input tensor and applies window partitioning for the attention mechanism if the window size is greater than 0. It then performs multi-layer perceptron (MLP) processing, adds shortcut connection, and returns the resulting output. The Attention class initializes the attention block with specified dimensions, number of heads, and other parameters.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":447-479",
            "content": "        if self.window_size > 0:\n            H, W = x.shape[1], x.shape[2]\n            x, pad_hw = window_partition(x, self.window_size)\n        x = self.attn(x)\n        # Reverse window partition\n        if self.window_size > 0:\n            x = window_unpartition(x, self.window_size, pad_hw, (H, W))\n        x = shortcut + x\n        x = x + self.mlp(self.norm2(x))\n        return x\nclass Attention(nn.Module):\n    \"\"\"Multi-head Attention block with relative position embeddings.\"\"\"\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 8,\n        qkv_bias: bool = True,\n        use_rel_pos: bool = False,\n        rel_pos_zero_init: bool = True,\n        input_size: Optional[Tuple[int, int]] = None,\n    ) -> None:\n        \"\"\"\n        Args:\n            dim (int): Number of input channels.\n            num_heads (int): Number of attention heads.\n            qkv_bias (bool):  If True, add a learnable bias to query, key, value.\n            rel_pos (bool): If True, add relative positional embeddings to the attention map."
        },
        {
            "comment": "This function initializes the Multi-Head Attention layer for a Transformer model. It takes in parameters such as num_heads, dim, qkv_bias, and input_size. The function also includes the use_rel_pos parameter to determine if relative positional encoding should be used. If so, it initializes the relative positional embeddings (rel_pos_h and rel_pos_w) with zero values. The forward function performs the multi-head attention operation on the input tensor x.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":480-503",
            "content": "            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n            input_size (tuple(int, int) or None): Input resolution for calculating the relative\n                positional parameter size.\n        \"\"\"\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim**-0.5\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim)\n        self.use_rel_pos = use_rel_pos\n        if self.use_rel_pos:\n            assert (\n                input_size is not None\n            ), \"Input size must be provided if using relative positional encoding.\"\n            # initialize relative positional embeddings\n            self.rel_pos_h = nn.Parameter(torch.zeros(2 * input_size[0] - 1, head_dim))\n            self.rel_pos_w = nn.Parameter(torch.zeros(2 * input_size[1] - 1, head_dim))\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        B, H, W, _ = x.shape\n        # qkv with shape (3, B, nHead, H * W, C)"
        },
        {
            "comment": "This code defines a function for partitioning input tokens into non-overlapping windows with padding if needed. It first computes the query, key, and value matrices from input features. Then, it performs attention calculation using scaled dot product attention and optionally adds relative position information. Finally, it applies a projection to the output before returning it.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":504-530",
            "content": "        qkv = self.qkv(x).reshape(B, H * W, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n        # q, k, v with shape (B * nHead, H * W, C)\n        q, k, v = qkv.reshape(3, B * self.num_heads, H * W, -1).unbind(0)\n        attn = (q * self.scale) @ k.transpose(-2, -1)\n        if self.use_rel_pos:\n            attn = add_decomposed_rel_pos(attn, q, self.rel_pos_h, self.rel_pos_w, (H, W), (H, W))\n        attn = attn.softmax(dim=-1)\n        x = (attn @ v).view(B, self.num_heads, H, W, -1).permute(0, 2, 3, 1, 4).reshape(B, H, W, -1)\n        x = self.proj(x)\n        return x\ndef window_partition(x: torch.Tensor, window_size: int) -> Tuple[torch.Tensor, Tuple[int, int]]:\n    \"\"\"\n    Partition into non-overlapping windows with padding if needed.\n    Args:\n        x (tensor): input tokens with [B, H, W, C].\n        window_size (int): window size.\n    Returns:\n        windows: windows after partition with [B * num_windows, window_size, window_size, C].\n        (Hp, Wp): padded height and width before partition\n    \"\"\""
        },
        {
            "comment": "This function takes in a tensor `x` and reshapes it into windows of size `window_size` for each sequence. It also adds padding to the height and width of the input tensor, ensuring that the window sizes are consistent across sequences. The function returns the reshaped windows along with the original unpadded height and width of the input tensor.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":531-558",
            "content": "    B, H, W, C = x.shape\n    pad_h = (window_size - H % window_size) % window_size\n    pad_w = (window_size - W % window_size) % window_size\n    if pad_h > 0 or pad_w > 0:\n        x = F.pad(x, (0, 0, 0, pad_w, 0, pad_h))\n    Hp, Wp = H + pad_h, W + pad_w\n    x = x.view(B, Hp // window_size, window_size, Wp // window_size, window_size, C)\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n    return windows, (Hp, Wp)\ndef window_unpartition(\n    windows: torch.Tensor, window_size: int, pad_hw: Tuple[int, int], hw: Tuple[int, int]\n) -> torch.Tensor:\n    \"\"\"\n    Window unpartition into original sequences and removing padding.\n    Args:\n        windows (tensor): input tokens with [B * num_windows, window_size, window_size, C].\n        window_size (int): window size.\n        pad_hw (Tuple): padded height and width (Hp, Wp).\n        hw (Tuple): original height and width (H, W) before padding.\n    Returns:\n        x: unpartitioned sequences with [B, H, W, C].\n    \"\"\"\n    Hp, Wp = pad_hw"
        },
        {
            "comment": "The code defines functions for handling windows and positional embeddings in a model. The first function, `get_windows`, resizes the input, calculates the window size, and rearranges the dimensions of the tensor. The second function, `get_rel_pos`, extracts relative positional embeddings based on query and key sizes. It also interpolates the relative position embeddings if necessary.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":559-586",
            "content": "    H, W = hw\n    B = windows.shape[0] // (Hp * Wp // window_size // window_size)\n    x = windows.view(B, Hp // window_size, Wp // window_size, window_size, window_size, -1)\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, Hp, Wp, -1)\n    if Hp > H or Wp > W:\n        x = x[:, :H, :W, :].contiguous()\n    return x\ndef get_rel_pos(q_size: int, k_size: int, rel_pos: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Get relative positional embeddings according to the relative positions of\n        query and key sizes.\n    Args:\n        q_size (int): size of query q.\n        k_size (int): size of key k.\n        rel_pos (Tensor): relative position embeddings (L, C).\n    Returns:\n        Extracted positional embeddings according to relative positions.\n    \"\"\"\n    max_rel_dist = int(2 * max(q_size, k_size) - 1)\n    # Interpolate rel pos if needed.\n    if rel_pos.shape[0] != max_rel_dist:\n        # Interpolate rel pos.\n        rel_pos_resized = F.interpolate(\n            rel_pos.reshape(1, rel_pos.shape[0], -1).permute(0, 2, 1),"
        },
        {
            "comment": "This function calculates decomposed Relative Positional Embeddings based on the attention map, query tensor, and relative positioning information for height and width. It resizes and scales the relative positions according to the shapes of the query (q) and key (k) tensors, then returns the resized embeddings.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":587-614",
            "content": "            size=max_rel_dist,\n            mode=\"linear\",\n        )\n        rel_pos_resized = rel_pos_resized.reshape(-1, max_rel_dist).permute(1, 0)\n    else:\n        rel_pos_resized = rel_pos\n    # Scale the coords with short length if shapes for q and k are different.\n    q_coords = torch.arange(q_size)[:, None] * max(k_size / q_size, 1.0)\n    k_coords = torch.arange(k_size)[None, :] * max(q_size / k_size, 1.0)\n    relative_coords = (q_coords - k_coords) + (k_size - 1) * max(q_size / k_size, 1.0)\n    return rel_pos_resized[relative_coords.long()]\ndef add_decomposed_rel_pos(\n    attn: torch.Tensor,\n    q: torch.Tensor,\n    rel_pos_h: torch.Tensor,\n    rel_pos_w: torch.Tensor,\n    q_size: Tuple[int, int],\n    k_size: Tuple[int, int],\n) -> torch.Tensor:\n    \"\"\"\n    Calculate decomposed Relative Positional Embeddings from :paper:`mvitv2`.\n    https://github.com/facebookresearch/mvit/blob/19786631e330df9f3622e5402b4a419a263a2c80/mvit/models/attention.py   # noqa B950\n    Args:\n        attn (Tensor): attention map."
        },
        {
            "comment": "This code is a part of the Self-Attention mechanism in the SAM (Spatio-Temporal Attentive Memory) model. The function calculates and adds relative positional embeddings to the attention map, considering both height and width axes. The PatchEmbed class performs image embedding by flattening image patches into a 2D feature grid for further processing.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":615-646",
            "content": "        q (Tensor): query q in the attention layer with shape (B, q_h * q_w, C).\n        rel_pos_h (Tensor): relative position embeddings (Lh, C) for height axis.\n        rel_pos_w (Tensor): relative position embeddings (Lw, C) for width axis.\n        q_size (Tuple): spatial sequence size of query q with (q_h, q_w).\n        k_size (Tuple): spatial sequence size of key k with (k_h, k_w).\n    Returns:\n        attn (Tensor): attention map with added relative positional embeddings.\n    \"\"\"\n    q_h, q_w = q_size\n    k_h, k_w = k_size\n    Rh = get_rel_pos(q_h, k_h, rel_pos_h)\n    Rw = get_rel_pos(q_w, k_w, rel_pos_w)\n    B, _, dim = q.shape\n    r_q = q.reshape(B, q_h, q_w, dim)\n    rel_h = torch.einsum(\"bhwc,hkc->bhwk\", r_q, Rh)\n    rel_w = torch.einsum(\"bhwc,wkc->bhwk\", r_q, Rw)\n    attn = (\n        attn.view(B, q_h, q_w, k_h, k_w) + rel_h[:, :, :, :, None] + rel_w[:, :, :, None, :]\n    ).view(B, q_h * q_w, k_h * k_w)\n    return attn\nclass PatchEmbed(nn.Module):\n    \"\"\"\n    Image to Patch Embedding.\n    \"\"\"\n    def __init__("
        },
        {
            "comment": "This class initializes a Conv2d layer for patch embedding with specified kernel size, stride, padding, input channels, and embedding dimension. The forward pass simply applies the initialized convolution and reorders the dimensions of the output tensor.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":647-679",
            "content": "        self,\n        kernel_size: Tuple[int, int] = (16, 16),\n        stride: Tuple[int, int] = (16, 16),\n        padding: Tuple[int, int] = (0, 0),\n        in_chans: int = 3,\n        embed_dim: int = 768,\n    ) -> None:\n        \"\"\"\n        Args:\n            kernel_size (Tuple): kernel size of the projection layer.\n            stride (Tuple): stride of the projection layer.\n            padding (Tuple): padding size of the projection layer.\n            in_chans (int): Number of input image channels.\n            embed_dim (int): Patch embedding dimension.\n        \"\"\"\n        super().__init__()\n        self.proj = nn.Conv2d(\n            in_chans, embed_dim, kernel_size=kernel_size, stride=stride, padding=padding\n        )\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.proj(x)\n        # B C H W -> B H W C\n        x = x.permute(0, 2, 3, 1)\n        return x\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n# This source code is licensed under the license found in the"
        },
        {
            "comment": "This code defines a MaskDecoder class for predicting masks given an image and prompt embeddings using a transformer architecture. It takes in parameters like transformer_dim, transformer, num_multimask_outputs, activation, iou_head_depth, and iou_head_hidden_dim.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":680-708",
            "content": "# LICENSE file in the root directory of this source tree.\nclass MaskDecoder(nn.Module):\n    def __init__(\n        self,\n        *,\n        transformer_dim: int,\n        transformer: nn.Module,\n        num_multimask_outputs: int = 3,\n        activation: Type[nn.Module] = nn.GELU,\n        iou_head_depth: int = 3,\n        iou_head_hidden_dim: int = 256,\n    ) -> None:\n        \"\"\"\n        Predicts masks given an image and prompt embeddings, using a\n        transformer architecture.\n        Arguments:\n          transformer_dim (int): the channel dimension of the transformer\n          transformer (nn.Module): the transformer used to predict masks\n          num_multimask_outputs (int): the number of masks to predict\n            when disambiguating masks\n          activation (nn.Module): the type of activation to use when\n            upscaling masks\n          iou_head_depth (int): the depth of the MLP used to predict\n            mask quality\n          iou_head_hidden_dim (int): the hidden dimension of the MLP\n            used to predict mask quality"
        },
        {
            "comment": "This code defines a model for image segmentation, consisting of a transformer network and output layers. The model takes in an input image and outputs a masked version with the ability to generate multiple masks. It uses an IOU token embedding, mask tokens embedding, output upscaling convolutions, hypernetworks MLPs, and an IOU prediction head.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":709-735",
            "content": "        \"\"\"\n        super().__init__()\n        self.transformer_dim = transformer_dim\n        self.transformer = transformer\n        self.num_multimask_outputs = num_multimask_outputs\n        self.iou_token = nn.Embedding(1, transformer_dim)\n        self.num_mask_tokens = num_multimask_outputs + 1\n        self.mask_tokens = nn.Embedding(self.num_mask_tokens, transformer_dim)\n        self.output_upscaling = nn.Sequential(\n            nn.ConvTranspose2d(transformer_dim, transformer_dim // 4, kernel_size=2, stride=2),\n            LayerNorm2d(transformer_dim // 4),\n            activation(),\n            nn.ConvTranspose2d(transformer_dim // 4, transformer_dim // 8, kernel_size=2, stride=2),\n            activation(),\n        )\n        self.output_hypernetworks_mlps = nn.ModuleList(\n            [\n                MLP(transformer_dim, transformer_dim, transformer_dim // 8, 3)\n                for i in range(self.num_mask_tokens)\n            ]\n        )\n        self.iou_prediction_head = MLP(\n            transformer_dim, iou_head_hidden_dim, self.num_mask_tokens, iou_head_depth"
        },
        {
            "comment": "This function takes image embeddings, image positional encoding, sparse prompt embeddings, dense prompt embeddings, and a boolean indicating whether to return multiple or single masks as input. It returns predicted masks and mask quality scores for each input case.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":736-762",
            "content": "        )\n    def forward(\n        self,\n        image_embeddings: torch.Tensor,\n        image_pe: torch.Tensor,\n        sparse_prompt_embeddings: torch.Tensor,\n        dense_prompt_embeddings: torch.Tensor,\n        multimask_output: bool,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Predict masks given image and prompt embeddings.\n        Arguments:\n          image_embeddings (torch.Tensor): the embeddings from the image encoder\n          image_pe (torch.Tensor): positional encoding with the shape of image_embeddings\n          sparse_prompt_embeddings (torch.Tensor): the embeddings of the points and boxes\n          dense_prompt_embeddings (torch.Tensor): the embeddings of the mask inputs\n          multimask_output (bool): Whether to return multiple masks or a single\n            mask.\n        Returns:\n          torch.Tensor: batched predicted masks\n          torch.Tensor: batched predictions of mask quality\n        \"\"\"\n        masks, iou_pred = self.predict_masks(\n            image_embeddings=image_embeddings,"
        },
        {
            "comment": "This code snippet defines a model for predicting masks based on image embeddings, image position encoding (image_pe), sparse prompt embeddings, and dense prompt embeddings. It uses IOU token and mask tokens to concatenate output tokens and then expands them accordingly. The function returns the predicted masks and intersection over union (IOU) predictions.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":763-790",
            "content": "            image_pe=image_pe,\n            sparse_prompt_embeddings=sparse_prompt_embeddings,\n            dense_prompt_embeddings=dense_prompt_embeddings,\n        )\n        # Select the correct mask or masks for output\n        if multimask_output:\n            mask_slice = slice(1, None)\n        else:\n            mask_slice = slice(0, 1)\n        masks = masks[:, mask_slice, :, :]\n        iou_pred = iou_pred[:, mask_slice]\n        # Prepare output\n        return masks, iou_pred\n    def predict_masks(\n        self,\n        image_embeddings: torch.Tensor,\n        image_pe: torch.Tensor,\n        sparse_prompt_embeddings: torch.Tensor,\n        dense_prompt_embeddings: torch.Tensor,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Predicts masks. See 'forward' for more details.\"\"\"\n        # Concatenate output tokens\n        output_tokens = torch.cat([self.iou_token.weight, self.mask_tokens.weight], dim=0)\n        output_tokens = output_tokens.unsqueeze(0).expand(sparse_prompt_embeddings.size(0), -1, -1)\n        tokens = torch.cat((output_tokens, sparse_prompt_embeddings), dim=1)"
        },
        {
            "comment": "This code is performing mask token generation and mask upscaling using a transformer model. It first expands image embeddings to match the number of tokens, then concatenates them with dense prompt embeddings. The transformed input is passed through the transformer to generate mask tokens and iou values. Finally, the image embeddings are upscaled and used in combination with mask tokens to produce masks using a hypernetwork.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":792-813",
            "content": "        # Expand per-image data in batch direction to be per-mask\n        if image_embeddings.shape[0] != tokens.shape[0]:\n            src = torch.repeat_interleave(image_embeddings, tokens.shape[0], dim=0)\n        else:\n            src = image_embeddings\n        src = src + dense_prompt_embeddings\n        pos_src = torch.repeat_interleave(image_pe, tokens.shape[0], dim=0)\n        b, c, h, w = src.shape\n        # Run the transformer\n        hs, src = self.transformer(src, pos_src, tokens)\n        iou_token_out = hs[:, 0, :]\n        mask_tokens_out = hs[:, 1 : (1 + self.num_mask_tokens), :]\n        # Upscale mask embeddings and predict masks using the mask tokens\n        src = src.transpose(1, 2).view(b, c, h, w)\n        upscaled_embedding = self.output_upscaling(src)\n        hyper_in_list: List[torch.Tensor] = []\n        for i in range(self.num_mask_tokens):\n            hyper_in_list.append(self.output_hypernetworks_mlps[i](mask_tokens_out[:, i, :]))\n        hyper_in = torch.stack(hyper_in_list, dim=1)\n        b, c, h, w = upscaled_embedding.shape"
        },
        {
            "comment": "This code defines a Multi-Layer Perceptron (MLP) with optional sigmoid output. It takes an input dimension, hidden dimension, output dimension, and number of layers as parameters. The MLP consists of a series of linear layers with ReLU activation functions between them, followed by a final layer without activation. If sigmoid_output is set to True, the output is passed through a sigmoid function. This code is adapted from the MaskFormer model's TransformerPredictor class.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":814-844",
            "content": "        masks = (hyper_in @ upscaled_embedding.view(b, c, h * w)).view(b, -1, h, w)\n        # Generate mask quality predictions\n        iou_pred = self.iou_prediction_head(iou_token_out)\n        return masks, iou_pred\n# Lightly adapted from\n# https://github.com/facebookresearch/MaskFormer/blob/main/mask_former/modeling/transformer/transformer_predictor.py # noqa\nclass MLP(nn.Module):\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int,\n        output_dim: int,\n        num_layers: int,\n        sigmoid_output: bool = False,\n    ) -> None:\n        super().__init__()\n        self.num_layers = num_layers\n        h = [hidden_dim] * (num_layers - 1)\n        self.layers = nn.ModuleList(\n            nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim])\n        )\n        self.sigmoid_output = sigmoid_output\n    def forward(self, x):\n        for i, layer in enumerate(self.layers):\n            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n        if self.sigmoid_output:"
        },
        {
            "comment": "This code defines the PromptEncoder class, which encodes prompts for input to SAM's mask decoder. It takes arguments such as embed_dim (embedding dimension), image_embedding_size, input_image_size, and mask_in_chans. The activation function can also be specified.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":845-878",
            "content": "            x = F.sigmoid(x)\n        return x\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nclass PromptEncoder(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int,\n        image_embedding_size: Tuple[int, int],\n        input_image_size: Tuple[int, int],\n        mask_in_chans: int,\n        activation: Type[nn.Module] = nn.GELU,\n    ) -> None:\n        \"\"\"\n        Encodes prompts for input to SAM's mask decoder.\n        Arguments:\n          embed_dim (int): The prompts' embedding dimension\n          image_embedding_size (tuple(int, int)): The spatial size of the\n            image embedding, as (H, W).\n          input_image_size (int): The padded size of the image as input\n            to the image encoder, as (H, W).\n          mask_in_chans (int): The number of hidden channels used for\n            encoding input masks.\n          activation (nn.Module): The activation to use when encoding"
        },
        {
            "comment": "The code initializes an object with specified dimensions for embedding, input image size, and image embedding. It creates a position embedding layer and a list of point embeddings, followed by a not-a-point embedding. The mask input size is also defined. Finally, it sets up a series of convolutional layers to scale the mask inputs to the appropriate size.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":879-900",
            "content": "            input masks.\n        \"\"\"\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.input_image_size = input_image_size\n        self.image_embedding_size = image_embedding_size\n        self.pe_layer = PositionEmbeddingRandom(embed_dim // 2)\n        self.num_point_embeddings: int = 4  # pos/neg point + 2 box corners\n        point_embeddings = [nn.Embedding(1, embed_dim) for i in range(self.num_point_embeddings)]\n        self.point_embeddings = nn.ModuleList(point_embeddings)\n        self.not_a_point_embed = nn.Embedding(1, embed_dim)\n        self.mask_input_size = (4 * image_embedding_size[0], 4 * image_embedding_size[1])\n        self.mask_downscaling = nn.Sequential(\n            nn.Conv2d(1, mask_in_chans // 4, kernel_size=2, stride=2),\n            LayerNorm2d(mask_in_chans // 4),\n            activation(),\n            nn.Conv2d(mask_in_chans // 4, mask_in_chans, kernel_size=2, stride=2),\n            LayerNorm2d(mask_in_chans),\n            activation(),\n            nn.Conv2d(mask_in_chans, embed_dim, kernel_size=1),"
        },
        {
            "comment": "This code defines a model for embedding point prompts in an image encoding. It includes functions to generate positional encodings, embed points, and handle padding if necessary. The model uses an embedding layer, positional encoding layer (pe_layer), and input image size for embedding the points.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":901-928",
            "content": "        )\n        self.no_mask_embed = nn.Embedding(1, embed_dim)\n    def get_dense_pe(self) -> torch.Tensor:\n        \"\"\"\n        Returns the positional encoding used to encode point prompts,\n        applied to a dense set of points the shape of the image encoding.\n        Returns:\n          torch.Tensor: Positional encoding with shape\n            1x(embed_dim)x(embedding_h)x(embedding_w)\n        \"\"\"\n        return self.pe_layer(self.image_embedding_size).unsqueeze(0)\n    def _embed_points(\n        self,\n        points: torch.Tensor,\n        labels: torch.Tensor,\n        pad: bool,\n    ) -> torch.Tensor:\n        \"\"\"Embeds point prompts.\"\"\"\n        points = points + 0.5  # Shift to center of pixel\n        if pad:\n            padding_point = torch.zeros((points.shape[0], 1, 2), device=points.device)\n            padding_label = -torch.ones((labels.shape[0], 1), device=labels.device)\n            points = torch.cat([points, padding_point], dim=1)\n            labels = torch.cat([labels, padding_label], dim=1)\n        point_embedding = self.pe_layer.forward_with_coords(points, self.input_image_size)"
        },
        {
            "comment": "This code is for a model that embeds points, boxes, and masks as input features. The `_embed_points` function assigns default embeddings to labels -1, 0, and 1, then adds additional embeddings if necessary. The `_embed_boxes` function shifts box prompts to the center of pixels, applies a position encoding layer, and adds specific embeddings for each corner. The `_embed_masks` function downscales mask inputs before returning their embedding. The `_get_batch_size` function retrieves the batch size from a tuple of optional points.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":929-951",
            "content": "        point_embedding[labels == -1] = 0.0\n        point_embedding[labels == -1] += self.not_a_point_embed.weight\n        point_embedding[labels == 0] += self.point_embeddings[0].weight\n        point_embedding[labels == 1] += self.point_embeddings[1].weight\n        return point_embedding\n    def _embed_boxes(self, boxes: torch.Tensor) -> torch.Tensor:\n        \"\"\"Embeds box prompts.\"\"\"\n        boxes = boxes + 0.5  # Shift to center of pixel\n        coords = boxes.reshape(-1, 2, 2)\n        corner_embedding = self.pe_layer.forward_with_coords(coords, self.input_image_size)\n        corner_embedding[:, 0, :] += self.point_embeddings[2].weight\n        corner_embedding[:, 1, :] += self.point_embeddings[3].weight\n        return corner_embedding\n    def _embed_masks(self, masks: torch.Tensor) -> torch.Tensor:\n        \"\"\"Embeds mask inputs.\"\"\"\n        mask_embedding = self.mask_downscaling(masks)\n        return mask_embedding\n    def _get_batch_size(\n        self,\n        points: Optional[Tuple[torch.Tensor, torch.Tensor]],"
        },
        {
            "comment": "The function returns the batch size based on the input type, sets the device for point embeddings, and performs embedding for points, boxes, or masks in a single forward pass.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":952-983",
            "content": "        boxes: Optional[torch.Tensor],\n        masks: Optional[torch.Tensor],\n    ) -> int:\n        \"\"\"\n        Gets the batch size of the output given the batch size of the input prompts.\n        \"\"\"\n        if points is not None:\n            return points[0].shape[0]\n        elif boxes is not None:\n            return boxes.shape[0]\n        elif masks is not None:\n            return masks.shape[0]\n        else:\n            return 1\n    def _get_device(self) -> torch.device:\n        return self.point_embeddings[0].weight.device\n    def forward(\n        self,\n        points: Optional[Tuple[torch.Tensor, torch.Tensor]],\n        boxes: Optional[torch.Tensor],\n        masks: Optional[torch.Tensor],\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Embeds different types of prompts, returning both sparse and dense\n        embeddings.\n        Arguments:\n          points (tuple(torch.Tensor, torch.Tensor) or none): point coordinates\n            and labels to embed.\n          boxes (torch.Tensor or none): boxes to embed"
        },
        {
            "comment": "The function takes points, boxes, and masks as inputs and returns sparse embeddings for the points and boxes, and dense embeddings for the masks. It checks the input batch size and creates an empty tensor for storing the sparse embeddings. If points are provided, it embeds them and appends the result to the sparse_embeddings tensor. Similarly, if boxes are provided, they are embedded and appended to the same tensor. The function then returns the combined sparse_embeddings and dense masks.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":984-1002",
            "content": "          masks (torch.Tensor or none): masks to embed\n        Returns:\n          torch.Tensor: sparse embeddings for the points and boxes, with shape\n            BxNx(embed_dim), where N is determined by the number of input points\n            and boxes.\n          torch.Tensor: dense embeddings for the masks, in the shape\n            Bx(embed_dim)x(embed_H)x(embed_W)\n        \"\"\"\n        bs = self._get_batch_size(points, boxes, masks)\n        sparse_embeddings = torch.empty((bs, 0, self.embed_dim), device=self._get_device(),\n                                        dtype=self.no_mask_embed.weight.dtype)\n        if points is not None:\n            coords, labels = points\n            point_embeddings = self._embed_points(coords, labels, pad=(boxes is None))\n            sparse_embeddings = torch.cat([sparse_embeddings, point_embeddings], dim=1)\n        if boxes is not None:\n            box_embeddings = self._embed_boxes(boxes)\n            sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=1)"
        },
        {
            "comment": "This code appears to be from a model called 'Sam'. It checks if there are any masks present, and if so, it embeds them. If no masks are present, it uses a default no-mask embedding. The code also defines a class 'PositionEmbeddingRandom' which utilizes random spatial frequencies for positional encoding.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":1004-1031",
            "content": "        if masks is not None:\n            dense_embeddings = self._embed_masks(masks)\n        else:\n            dense_embeddings = self.no_mask_embed.weight.reshape(1, -1, 1, 1).expand(\n                bs, -1, self.image_embedding_size[0], self.image_embedding_size[1]\n            )\n        return sparse_embeddings, dense_embeddings\nclass PositionEmbeddingRandom(nn.Module):\n    \"\"\"\n    Positional encoding using random spatial frequencies.\n    \"\"\"\n    def __init__(self, num_pos_feats: int = 64, scale: Optional[float] = None) -> None:\n        super().__init__()\n        if scale is None or scale <= 0.0:\n            scale = 1.0\n        self.register_buffer(\n            \"positional_encoding_gaussian_matrix\",\n            scale * torch.randn((2, num_pos_feats)),\n        )\n    def _pe_encoding(self, coords: torch.Tensor) -> torch.Tensor:\n        \"\"\"Positionally encode points that are normalized to [0,1].\"\"\"\n        # assuming coords are in [0, 1]^2 square and have d_1 x ... x d_n x 2 shape\n        coords = coords.type(self.positional_encoding_gaussian_matrix.dtype)"
        },
        {
            "comment": "This code defines a class for positional encoding generation. The `forward` method generates a grid-based positional encoding of size (H, W) and returns it. The `forward_with_coords` method takes coordinate input and generates positional encoding based on those coordinates. The `_pe_encoding` method is not shown here but is responsible for generating the actual encoding matrix.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":1032-1054",
            "content": "        coords = 2 * coords - 1\n        coords = coords @ self.positional_encoding_gaussian_matrix\n        coords = 2 * np.pi * coords\n        # outputs d_1 x ... x d_n x C shape\n        return torch.cat([torch.sin(coords), torch.cos(coords)], dim=-1)\n    def forward(self, size: Tuple[int, int]) -> torch.Tensor:\n        \"\"\"Generate positional encoding for a grid of the specified size.\"\"\"\n        h, w = size\n        device: Any = self.positional_encoding_gaussian_matrix.device\n        grid = torch.ones((h, w), device=device, dtype=self.positional_encoding_gaussian_matrix.dtype)\n        y_embed = grid.cumsum(dim=0) - 0.5\n        x_embed = grid.cumsum(dim=1) - 0.5\n        y_embed = y_embed / h\n        x_embed = x_embed / w\n        pe = self._pe_encoding(torch.stack([x_embed, y_embed], dim=-1))\n        return pe.permute(2, 0, 1)  # C x H x W\n    def forward_with_coords(\n        self, coords_input: torch.Tensor, image_size: Tuple[int, int]\n    ) -> torch.Tensor:\n        \"\"\"Positionally encode points that are not normalized to [0,1].\"\"\""
        },
        {
            "comment": "This code snippet is part of the SAM (Spatio-Attentional Memory) model in the NExT-Chat/mllm repository. It defines a class called Sam and initializes its attributes like image_encoder, prompt_encoder, mask_decoder, pixel_mean, and pixel_std. The method _pe_encoding is used to encode coordinates in the image space which are then returned after being converted to float type. The SAM model predicts object masks from an image and input prompts.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":1055-1087",
            "content": "        coords = coords_input.clone()\n        coords[:, :, 0] = coords[:, :, 0] / image_size[1]\n        coords[:, :, 1] = coords[:, :, 1] / image_size[0]\n        return self._pe_encoding(coords.to(torch.float))  # B x N x C\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nclass Sam(nn.Module):\n    mask_threshold: float = 0.0\n    image_format: str = \"RGB\"\n    def __init__(\n        self,\n        image_encoder: ImageEncoderViT,\n        prompt_encoder: PromptEncoder,\n        mask_decoder: MaskDecoder,\n        pixel_mean: List[float] = [123.675, 116.28, 103.53],\n        pixel_std: List[float] = [58.395, 57.12, 57.375],\n    ) -> None:\n        \"\"\"\n        SAM predicts object masks from an image and input prompts.\n        Arguments:\n          image_encoder (ImageEncoderViT): The backbone used to encode the\n            image into image embeddings that allow for efficient mask prediction."
        },
        {
            "comment": "This code initializes a class that takes in an image encoder, prompt encoder, and mask decoder as inputs. It also registers buffer for pixel mean and pixel std values. The device property returns the device of the pixel_mean tensor. The forward method predicts masks from images and prompts end-to-end.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":1088-1112",
            "content": "          prompt_encoder (PromptEncoder): Encodes various types of input prompts.\n          mask_decoder (MaskDecoder): Predicts masks from the image embeddings\n            and encoded prompts.\n          pixel_mean (list(float)): Mean values for normalizing pixels in the input image.\n          pixel_std (list(float)): Std values for normalizing pixels in the input image.\n        \"\"\"\n        super().__init__()\n        self.image_encoder = image_encoder\n        self.prompt_encoder = prompt_encoder\n        self.mask_decoder = mask_decoder\n        self.register_buffer(\"pixel_mean\", torch.Tensor(pixel_mean).view(-1, 1, 1), False)\n        self.register_buffer(\"pixel_std\", torch.Tensor(pixel_std).view(-1, 1, 1), False)\n    @property\n    def device(self) -> Any:\n        return self.pixel_mean.device\n    @torch.no_grad()\n    def forward(\n        self,\n        batched_input: List[Dict[str, Any]],\n        multimask_output: bool,\n    ) -> List[Dict[str, torch.Tensor]]:\n        \"\"\"\n        Predicts masks end-to-end from provided images and prompts."
        },
        {
            "comment": "This code describes the function signature for the SamPredictor model in the NExT-Chat codebase. It accepts a list of input dictionaries, each containing keys like 'image', 'original_size', 'point_coords', 'point_labels', and 'boxes'. These inputs are already transformed to match the input frame of the model, making it easier for users who don't know prompts in advance.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":1113-1130",
            "content": "        If prompts are not known in advance, using SamPredictor is\n        recommended over calling the model directly.\n        Arguments:\n          batched_input (list(dict)): A list over input images, each a\n            dictionary with the following keys. A prompt key can be\n            excluded if it is not present.\n              'image': The image as a torch tensor in 3xHxW format,\n                already transformed for input to the model.\n              'original_size': (tuple(int, int)) The original size of\n                the image before transformation, as (H, W).\n              'point_coords': (torch.Tensor) Batched point prompts for\n                this image, with shape BxNx2. Already transformed to the\n                input frame of the model.\n              'point_labels': (torch.Tensor) Batched labels for point prompts,\n                with shape BxN.\n              'boxes': (torch.Tensor) Batched box inputs, with shape Bx4.\n                Already transformed to the input frame of the model."
        },
        {
            "comment": "This function takes batched input images and preprocesses them using the model's 'preprocess' method. The result is a stack of processed input images along dimension 0, with shape Bx3xHxW where B is the batch size, H and W are the original image dimensions, and C=3 for RGB images. This function serves as part of a larger model that performs some type of mask prediction or disambiguation task on the processed input images.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":1131-1149",
            "content": "              'mask_inputs': (torch.Tensor) Batched mask inputs to the model,\n                in the form Bx1xHxW.\n          multimask_output (bool): Whether the model should predict multiple\n            disambiguating masks, or return a single mask.\n        Returns:\n          (list(dict)): A list over input images, where each element is\n            as dictionary with the following keys.\n              'masks': (torch.Tensor) Batched binary mask predictions,\n                with shape BxCxHxW, where B is the number of input prompts,\n                C is determined by multimask_output, and (H, W) is the\n                original size of the image.\n              'iou_predictions': (torch.Tensor) The model's predictions\n                of mask quality, in shape BxC.\n              'low_res_logits': (torch.Tensor) Low resolution logits with\n                shape BxCxHxW, where H=W=256. Can be passed as mask input\n                to subsequent iterations of prediction.\n        \"\"\"\n        input_images = torch.stack([self.preprocess(x[\"image\"]) for x in batched_input], dim=0)"
        },
        {
            "comment": "This code is for image-based text generation using a model with separate encoders and decoders. It takes input images, extracts their embeddings, then uses these embeddings along with other inputs like point coordinates, bounding boxes, and masks to generate low-resolution masks and IOU predictions. The masks are then post-processed for the final output.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":1150-1172",
            "content": "        image_embeddings = self.image_encoder(input_images)\n        outputs = []\n        for image_record, curr_embedding in zip(batched_input, image_embeddings):\n            if \"point_coords\" in image_record:\n                points = (image_record[\"point_coords\"], image_record[\"point_labels\"])\n            else:\n                points = None\n            sparse_embeddings, dense_embeddings = self.prompt_encoder(\n                points=points,\n                boxes=image_record.get(\"boxes\", None),\n                masks=image_record.get(\"mask_inputs\", None),\n            )\n            low_res_masks, iou_predictions = self.mask_decoder(\n                image_embeddings=curr_embedding.unsqueeze(0),\n                image_pe=self.prompt_encoder.get_dense_pe(),\n                sparse_prompt_embeddings=sparse_embeddings,\n                dense_prompt_embeddings=dense_embeddings,\n                multimask_output=multimask_output,\n            )\n            masks = self.postprocess_masks(\n                low_res_masks,\n                input_size=image_record[\"image\"].shape[-2:],"
        },
        {
            "comment": "This code contains a model that takes in masks, input and original sizes as inputs and returns outputs. The 'masks' are processed to remove padding and upscale to the original image size using the mask_decoder. The result is a batched mask, where each batch element represents an image, with dimensions BxCxHxW format.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":1173-1202",
            "content": "                original_size=image_record[\"original_size\"],\n            )\n            masks = masks > self.mask_threshold\n            outputs.append(\n                {\n                    \"masks\": masks,\n                    \"iou_predictions\": iou_predictions,\n                    \"low_res_logits\": low_res_masks,\n                }\n            )\n        return outputs\n    def postprocess_masks(\n        self,\n        masks: torch.Tensor,\n        input_size: Tuple[int, ...],\n        original_size: Tuple[int, ...],\n    ) -> torch.Tensor:\n        \"\"\"\n        Remove padding and upscale masks to the original image size.\n        Arguments:\n          masks (torch.Tensor): Batched masks from the mask_decoder,\n            in BxCxHxW format.\n          input_size (tuple(int, int)): The size of the image input to the\n            model, in (H, W) format. Used to remove padding.\n          original_size (tuple(int, int)): The original size of the image\n            before resizing for input to the model, in (H, W) format.\n        Returns:"
        },
        {
            "comment": "This code is from a machine learning model, specifically the Sam model. It includes functions for generating masks and preprocessing inputs. The `generate_masks` function interpolates and pads masks to match the input size. The `preprocess` function normalizes pixel values and pads inputs to a square shape. This model is likely used in image processing tasks.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":1203-1230",
            "content": "          (torch.Tensor): Batched masks in BxCxHxW format, where (H, W)\n            is given by original_size.\n        \"\"\"\n        masks = F.interpolate(\n            masks,\n            (self.image_encoder.img_size, self.image_encoder.img_size),\n            mode=\"bilinear\",\n            align_corners=False,\n        )\n        h, w = input_size[0], input_size[1]\n        max_dim = max(w, h)\n        pad_w = (max_dim - w) // 2\n        pad_h = (max_dim - h) // 2\n        masks = masks[..., pad_h: input_size[0]+pad_h, pad_w: pad_w+input_size[1]]\n        masks = F.interpolate(masks, original_size, mode=\"bilinear\", align_corners=False)\n        return masks\n    def preprocess(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Normalize pixel values and pad to a square input.\"\"\"\n        # Normalize colors\n        x = (x - self.pixel_mean) / self.pixel_std\n        # Pad\n        h, w = x.shape[-2:]\n        padh = self.image_encoder.img_size - h\n        padw = self.image_encoder.img_size - w\n        x = F.pad(x, (0, padw, 0, padh))"
        },
        {
            "comment": "The code defines three functions build_sam_vit_h, build_sam_vit_l, and build_sam_vit_b that return different implementations of the SAM model using different configurations. The SAM model is a transformer-based architecture with global attention mechanisms. The functions accept an optional checkpoint parameter. The sam_model_registry dictionary maps \"default\" to build_sam_vit_h, which appears to be the default implementation.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":1231-1278",
            "content": "        return x\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nfrom functools import partial\ndef build_sam_vit_h(checkpoint=None):\n    return _build_sam(\n        encoder_embed_dim=1280,\n        encoder_depth=32,\n        encoder_num_heads=16,\n        encoder_global_attn_indexes=[7, 15, 23, 31],\n        checkpoint=checkpoint,\n    )\nbuild_sam = build_sam_vit_h\ndef build_sam_vit_l(checkpoint=None):\n    return _build_sam(\n        encoder_embed_dim=1024,\n        encoder_depth=24,\n        encoder_num_heads=16,\n        encoder_global_attn_indexes=[5, 11, 17, 23],\n        checkpoint=checkpoint,\n    )\ndef build_sam_vit_b(checkpoint=None):\n    return _build_sam(\n        encoder_embed_dim=768,\n        encoder_depth=12,\n        encoder_num_heads=12,\n        encoder_global_attn_indexes=[2, 5, 8, 11],\n        checkpoint=checkpoint,\n    )\nsam_model_registry = {\n    \"default\": build_sam_vit_h,"
        },
        {
            "comment": "This code is defining a function `_build_sam` that builds a SAM (Sampling Model) using different Vision Transformers (ViT) of varying sizes (\"vit_h\", \"vit_l\", and \"vit_b\"). The SAM consists of an image encoder, which is a ViT, and a prompt encoder. It takes in parameters such as the embedding dimension, depth, number of heads, global attention indexes for the image encoder, and optional checkpointing. The prompt encoder has an embed_dim (prompt embedding size) and an image_embedding_size matching the image encoder's output channel count.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":1279-1313",
            "content": "    \"vit_h\": build_sam_vit_h,\n    \"vit_l\": build_sam_vit_l,\n    \"vit_b\": build_sam_vit_b,\n}\ndef _build_sam(\n    encoder_embed_dim,\n    encoder_depth,\n    encoder_num_heads,\n    encoder_global_attn_indexes,\n    checkpoint=None,\n):\n    prompt_embed_dim = 256\n    image_size = 1024\n    vit_patch_size = 16\n    image_embedding_size = image_size // vit_patch_size\n    sam = Sam(\n        image_encoder=ImageEncoderViT(\n            depth=encoder_depth,\n            embed_dim=encoder_embed_dim,\n            img_size=image_size,\n            mlp_ratio=4,\n            norm_layer=partial(torch.nn.LayerNorm, eps=1e-6),\n            num_heads=encoder_num_heads,\n            patch_size=vit_patch_size,\n            qkv_bias=True,\n            use_rel_pos=True,\n            global_attn_indexes=encoder_global_attn_indexes,\n            window_size=14,\n            out_chans=prompt_embed_dim,\n        ),\n        prompt_encoder=PromptEncoder(\n            embed_dim=prompt_embed_dim,\n            image_embedding_size=(image_embedding_size, image_embedding_size),"
        },
        {
            "comment": "This code defines a class `SamForLMSeg` which takes in `model_type` and `ckpt`, initializes an instance of the model specified by `model_type` from the `sam_model_registry`, and provides a `forward` method for running inference on images, sparse embeddings, and boxes. The model is initialized with optional checkpoint `ckpt`.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":1314-1347",
            "content": "            input_image_size=(image_size, image_size),\n            mask_in_chans=16,\n        ),\n        mask_decoder=MaskDecoder(\n            num_multimask_outputs=3,\n            transformer=TwoWayTransformer(\n                depth=2,\n                embedding_dim=prompt_embed_dim,\n                mlp_dim=2048,\n                num_heads=8,\n            ),\n            transformer_dim=prompt_embed_dim,\n            iou_head_depth=3,\n            iou_head_hidden_dim=256,\n        ),\n        pixel_mean=[123.675, 116.28, 103.53],\n        pixel_std=[58.395, 57.12, 57.375],\n    )\n    sam.eval()\n    if checkpoint is not None:\n        with open(checkpoint, \"rb\") as f:\n            state_dict = torch.load(f)\n        sam.load_state_dict(state_dict)\n    return sam\nfrom mllm.models.sam.transforms import ResizeAndPad, ResizeLongestSide\nclass SamForLMSeg(nn.Module):\n    def __init__(self, model_type, ckpt):\n        super().__init__()\n        self.model = sam_model_registry[model_type](checkpoint=ckpt)\n    def forward(self, images, sparse_embeddings, boxes):"
        },
        {
            "comment": "This code defines a model for generating mask predictions using image and prompt embeddings. It involves image encoding, prompt encoding, and mask decoding. The predicted masks are interpolated to match the input image's shape. The device property is used to specify the hardware device for computations.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":1348-1379",
            "content": "        _, _, H, W = images.shape\n        image_embeddings = self.model.image_encoder(images)\n        pred_masks = []\n        ious = []\n        device = image_embeddings.device\n        box_embeddings, dense_embeddings = self.model.prompt_encoder(\n            points=None,\n            boxes=boxes,\n            masks=None,\n        )\n        sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], 1)\n        low_res_masks, iou_predictions = self.model.mask_decoder(\n            image_embeddings=image_embeddings,\n            image_pe=self.model.prompt_encoder.get_dense_pe(),\n            sparse_prompt_embeddings=sparse_embeddings,\n            dense_prompt_embeddings=dense_embeddings,\n            multimask_output=False,\n        )\n        pred_masks = F.interpolate(\n            low_res_masks.float(),\n            (H, W),\n            mode=\"bilinear\",\n            align_corners=False,\n        )\n        return pred_masks, iou_predictions\n    @property\n    def device(self):\n        return self.model.image_encoder.patch_embed.proj.weight.device"
        },
        {
            "comment": "This function defines a predict method for a model. It resizes and pads input images to a specific size, extracts features from the image, and transforms point coords, labels, box and mask_input inputs if not None. The dtype is set according to the weight of the projection matrix in the image encoder, and the original and input sizes are stored for reference.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":1381-1403",
            "content": "    def predict(self,\n                image,\n                sparse_embedding=None,\n                point_coords: Optional[np.ndarray] = None,\n                point_labels: Optional[np.ndarray] = None,\n                box: Optional[np.ndarray] = None,\n                mask_input: Optional[np.ndarray] = None,\n                multimask_output: bool = False,\n                return_logits: bool = False,\n                ):\n        dtype = self.model.image_encoder.patch_embed.proj.weight.dtype\n        self.original_size = [image.height, image.width]\n        input_image, _, hw = ResizeAndPad(1024)(image, None)\n        self.input_size = [int(hw[0]), int(hw[1])]\n        self.features = self.model.image_encoder(input_image.to(self.device).unsqueeze(0).type(dtype))\n        self.transform = ResizeAndPad(1024) # TODO modify this\n        # Transform input prompts\n        coords_torch, labels_torch, box_torch, mask_input_torch = None, None, None, None\n        if point_coords is not None:\n            assert (\n                    point_labels is not None"
        },
        {
            "comment": "This code prepares and transforms inputs for the model's predict method. It applies coordinate transformation, converts input to tensors, adjusts dimensions, and handles optional box and mask_input parameters. The model then makes predictions using these prepared inputs.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":1404-1421",
            "content": "            ), \"point_labels must be supplied if point_coords is supplied.\"\n            point_coords = self.transform.apply_coords(point_coords, self.original_size)\n            coords_torch = torch.as_tensor(point_coords, dtype=torch.float, device=self.device)\n            labels_torch = torch.as_tensor(point_labels, dtype=torch.int, device=self.device)\n            coords_torch, labels_torch = coords_torch[None, :, :], labels_torch[None, :]\n        if box is not None:\n            # box = self.transform.apply_boxes(box, self.original_size)\n            box_torch = torch.as_tensor(box, dtype=dtype, device=self.device)\n            box_torch = box_torch[None, :]\n        if mask_input is not None:\n            mask_input_torch = torch.as_tensor(mask_input, dtype=torch.float, device=self.device)\n            mask_input_torch = mask_input_torch[None, :, :, :]\n        masks, iou_predictions, low_res_masks = self.predict_torch(\n            sparse_embedding,\n            coords_torch,\n            labels_torch,\n            box_torch,"
        },
        {
            "comment": "This function takes input sparse embedding, point coordinates and labels, boxes, mask input, multimask output, and return logits as arguments. It returns three tensors of masks_np, iou_predictions_np, and low_res_masks_np after converting them into numpy format and moving them to CPU. The function also defines another function called predict_torch that takes similar arguments and embeds prompts using the prompt_encoder method from self.model.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":1422-1448",
            "content": "            mask_input_torch,\n            multimask_output,\n            return_logits=return_logits,\n        )\n        masks_np = masks[0].detach().float().cpu().numpy()\n        iou_predictions_np = iou_predictions[0].detach().float().cpu().numpy()\n        low_res_masks_np = low_res_masks[0].detach().float().cpu().numpy()\n        return masks_np, iou_predictions_np, low_res_masks_np\n    @torch.no_grad()\n    def predict_torch(\n        self,\n        input_sparse_embedding=None,\n        point_coords: Optional[torch.Tensor]=None,\n        point_labels: Optional[torch.Tensor]=None,\n        boxes: Optional[torch.Tensor] = None,\n        mask_input: Optional[torch.Tensor] = None,\n        multimask_output: bool = True,\n        return_logits: bool = False,\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        if point_coords is not None:\n            points = (point_coords, point_labels)\n        else:\n            points = None\n        # Embed prompts\n        sparse_embeddings, dense_embeddings = self.model.prompt_encoder("
        },
        {
            "comment": "This code predicts masks and calculates IOU predictions from image embeddings, prompt embeddings, and multimask output. The masks are then upscaled to the original image resolution and returned based on a threshold.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/sam/modeling_sam.py\":1449-1472",
            "content": "            points=points,\n            boxes=boxes,\n            masks=mask_input,\n        )\n        if input_sparse_embedding is not None:\n            sparse_embeddings = input_sparse_embedding\n        # Predict masks\n        low_res_masks, iou_predictions = self.model.mask_decoder(\n            image_embeddings=self.features,\n            image_pe=self.model.prompt_encoder.get_dense_pe(),\n            sparse_prompt_embeddings=sparse_embeddings,\n            dense_prompt_embeddings=dense_embeddings,\n            multimask_output=multimask_output,\n        )\n        # Upscale the masks to the original image resolution\n        masks = self.model.postprocess_masks(low_res_masks, self.input_size, self.original_size)\n        if not return_logits:\n            masks = masks > self.model.mask_threshold\n        return masks, iou_predictions, low_res_masks"
        }
    ]
}