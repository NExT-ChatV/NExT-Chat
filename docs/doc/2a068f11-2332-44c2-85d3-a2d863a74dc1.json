{
    "summary": "The code initializes and configures NextChat model, tokenizer, vision model, and sets device, image tokenization options. It also modifies model parameters, resizes tokenizer, and updates input/output embeddings.",
    "details": [
        {
            "comment": "This code imports necessary libraries and defines a function that loads a pre-trained NextChat model and tokenizer. It creates an instance of the NextChatForCausalLM class from a specified model path, sets use_cache to False, and creates a tokenizer from the same model path while setting the padding side to right and use_fast to False. The code also includes an assertion that the model_args version is 'v1' and if it is not, it checks if the tokenizer has a pad_token set.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/builder/build_nextchat.py\":0-38",
            "content": "import json\nfrom typing import Dict, Any, Tuple\nimport torch\nimport transformers\nfrom torch import nn\nfrom ..nextchat.nextchat_seg import NextChatForSegLM\nfrom ..nextchat.nextchat_base import NextChatForCausalLM\nPREPROCESSOR = Dict[str, Any]\nDEFAULT_PAD_TOKEN = \"[PAD]\"\nDEFAULT_EOS_TOKEN = \"</s>\"\nDEFAULT_BOS_TOKEN = \"<s>\"\nDEFAULT_UNK_TOKEN = \"<unk>\"\ndef load_pretrained_nextchat_base(model_args, training_args, **kwargs) -> Tuple[nn.Module, PREPROCESSOR]:\n    model = NextChatForCausalLM.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=model_args.cache_dir,\n        _fast_init=False,\n        **kwargs\n    )\n    model.config.use_cache = False\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=model_args.cache_dir,\n        model_max_length=model_args.model_max_length,\n        padding_side=\"right\",\n        use_fast=False,\n    )\n    assert model_args.version == 'v1'\n    if model_args.version == \"v0\":\n        if tokenizer.pad_token is None:"
        },
        {
            "comment": "This code initializes a model for the NExT-Chat application. It tokenizes and resizes embeddings, adds special tokens if using LLAMA, and configures the vision modules. It also sets the appropriate data types based on whether to use 16-bit or 8-bit floating-point precision for training. The last line seems to be a workaround for a potential quantization issue involving the device 'meta'.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/builder/build_nextchat.py\":39-66",
            "content": "            smart_tokenizer_and_embedding_resize(\n                special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n                tokenizer=tokenizer,\n                model=model,\n            )\n        if \"llama\" in model_args.model_name_or_path:\n            tokenizer.add_special_tokens({\n                \"eos_token\": DEFAULT_EOS_TOKEN,\n                \"bos_token\": DEFAULT_BOS_TOKEN,\n                \"unk_token\": DEFAULT_UNK_TOKEN,\n            })\n    else:\n        tokenizer.pad_token = tokenizer.unk_token\n    model_vision_dict = model.model.initialize_vision_modules(\n        mm_depth=model_args.get(\"mm_projector_depth\", 1),\n        vision_tower=model_args.vision_tower,\n        mm_vision_select_layer=model_args.mm_vision_select_layer,\n        pretrained_mm_projector=model_args.pretrained_mm_projector,\n        fsdp=training_args.fsdp,\n    )\n    dtype = torch.float32\n    if training_args.fp16:\n        dtype = torch.float16\n    if training_args.bf16:\n        dtype = torch.bfloat16\n    # HACK for quantization\n    if model.model.get_vision_tower().device != torch.device('meta'):"
        },
        {
            "comment": "This code is initializing and configuring the vision tower for a model. If it exists, it moves the vision tower to the specified device and dtype. Otherwise, it creates a new CLIPVisionModel from pre-trained weights. The code also sets configuration options related to image start and end tokens. Finally, it initializes the vision tokenizer and determines which parameters should not require gradients.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/builder/build_nextchat.py\":67-84",
            "content": "        model.model.get_vision_tower().to(dtype=dtype, device=training_args.device)\n    else:\n        from transformers import CLIPVisionModel\n        model.model.vision_tower = CLIPVisionModel.from_pretrained(model_args.vision_tower)  # not quantize clip\n        # model.model.vision_tower = CLIPVisionModel.from_pretrained(model_args.vision_tower, **kwargs)  # quantize clip\u3001\n    vision_config = model_vision_dict['vision_config']\n    model.config.mm_use_im_start_end = model_args.mm_use_im_start_end\n    vision_config.use_im_start_end = model_args.mm_use_im_start_end\n    model.initialize_vision_tokenizer(mm_use_im_start_end=model_args.mm_use_im_start_end,\n                                      tokenizer=tokenizer,\n                                      device=training_args.device,\n                                      tune_mm_mlp_adapter=model_args.tune_mm_mlp_adapter,\n                                      pretrain_mm_mlp_adapter=model_args.pretrain_mm_mlp_adapter)\n    params_no_grad = [n for n, p in model.named_parameters() if not p.requires_grad]"
        },
        {
            "comment": "This code checks if there are any parameters without gradients while using FSDP (Fully Sharded Data Parallel) and provides a warning message if the count is less than 10. It also mentions that this feature requires PyTorch-nightly build and points to a specific repository for details. The purpose is to save memory during pretraining.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/builder/build_nextchat.py\":85-97",
            "content": "    if len(params_no_grad) > 0:\n        if training_args.fsdp is not None and len(training_args.fsdp) > 0:\n            if len(params_no_grad) < 10:\n                print('[WARNING] Attempting to use FSDP while {} parameters do not require gradients: {}'.format(len(params_no_grad),\n                                                                                                                 params_no_grad))\n            else:\n                print('[WARNING] Attempting to use FSDP while {} parameters do not require gradients: {}...(omitted)'.format(\n                    len(params_no_grad), ', '.join(params_no_grad[:10])))\n            print(\"[WARNING] Attempting to use FSDP with partially frozen parameters, this is experimental.\")\n            print(\n                \"[WARNING] As of 4/30/23, this feature requires PyTorch-nightly build.  See here for details: https://github.com/haotian-liu/LLaVA#experimental-use-fsdp-to-save-memory-in-pretraining\")\n            from torch.distributed.fsdp.fully_sharded_data_parallel import FullyShardedDataParallel as FSDP"
        },
        {
            "comment": "This code defines a function `patch_FSDP_use_orig_params` that wraps the `__init__` method of the `FSDP` class. The main function, `load_pretrained_nextchat`, takes arguments `model_args` and `training_args` and returns a model and preprocessor. It also writes a JSON file containing information about which parameters require gradients in the model.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/builder/build_nextchat.py\":99-125",
            "content": "            def patch_FSDP_use_orig_params(func):\n                def wrap_func(*args, **kwargs):\n                    use_orig_params = kwargs.pop('use_orig_params', True)\n                    return func(*args, **kwargs, use_orig_params=use_orig_params)\n                return wrap_func\n            FSDP.__init__ = patch_FSDP_use_orig_params(FSDP.__init__)\n    preprocessor = dict(\n        image=model_vision_dict['image_processor'],\n        text=tokenizer,\n        conv=dict(\n            image_token_len=model_args.image_token_len,\n            sep_image_conv_front=model_args.sep_image_conv_front,\n            use_im_start_end=model_args.mm_use_im_start_end,\n        )\n    )\n    # for k, v in model.named_parameters():\n    #     if v.requires_grad:\n    #         print(k)\n    # TODO peft lora_model\n    import json\n    json.dump({k: bool(v.requires_grad) for k, v in model.named_parameters()}, open(\"param.json\", \"w\"))\n    return model, preprocessor\ndef load_pretrained_nextchat(model_args, training_args, **kwargs) -> Tuple[nn.Module, PREPROCESSOR]:"
        },
        {
            "comment": "This code initializes a NextChat model and tokenizer using the specified configuration. The model is initialized with `from_pretrained`, caching disabled, and an optional SAM path included. The tokenizer is also initialized using the same method and configuration. If the version is 'v0', the code checks if the pad token exists in the tokenizer, resizes the embedding, and adds EOS and BOS tokens if the model includes LLAMA.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/builder/build_nextchat.py\":126-155",
            "content": "    model = NextChatForSegLM.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=model_args.cache_dir,\n        _fast_init=False,\n        sam_path=model_args.sam_path,\n        # mm_vision_tower=model_args.vision_tower,\n        **kwargs\n    )\n    model.config.use_cache = False\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=model_args.cache_dir,\n        model_max_length=model_args.model_max_length,\n        padding_side=\"right\",\n        use_fast=False,\n    )\n    assert model_args.version == 'v1'\n    if model_args.version == \"v0\":\n        if tokenizer.pad_token is None:\n            smart_tokenizer_and_embedding_resize(\n                special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n                tokenizer=tokenizer,\n                model=model,\n            )\n        if \"llama\" in model_args.model_name_or_path:\n            tokenizer.add_special_tokens({\n                \"eos_token\": DEFAULT_EOS_TOKEN,\n                \"bos_token\": DEFAULT_BOS_TOKEN,"
        },
        {
            "comment": "This code initializes the vision modules for a model in NextChat, depending on the provided arguments. It sets the unk_token if necessary and handles the data type based on training arguments. If the device is not Meta, it moves the vision tower to the specified device and data type.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/builder/build_nextchat.py\":156-182",
            "content": "                \"unk_token\": DEFAULT_UNK_TOKEN,\n            })\n    else:\n        tokenizer.pad_token = tokenizer.unk_token\n    # model_vision_dict = model.model.initialize_vision_modules(\n    #     vision_tower=model_args.vision_tower,\n    #     mm_vision_select_layer=model_args.mm_vision_select_layer,\n    #     fsdp=training_args.fsdp,\n    # )\n    model_vision_dict = model.model.initialize_vision_modules(\n        mm_depth=model_args.get(\"mm_projector_depth\", 1),\n        vision_tower=model_args.vision_tower,\n        mm_vision_select_layer=model_args.mm_vision_select_layer,\n        pretrained_mm_projector=None,\n        fsdp=training_args.fsdp,\n    )\n    dtype = torch.float32\n    if training_args.fp16:\n        dtype = torch.float16\n    if training_args.bf16:\n        dtype = torch.bfloat16\n    # HACK for quantization\n    if model.model.get_vision_tower().device != torch.device('meta'):\n        model.model.get_vision_tower().to(dtype=dtype, device=training_args.device)\n    else:\n        from transformers import CLIPVisionModel"
        },
        {
            "comment": "This code initializes a CLIP Vision Model, sets model parameters, and configures the vision tokenizer. It then disables gradient tracking for most components except the seg_prompt_mlp, prompt_encoder, and sam.model.prompt_encoder.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/builder/build_nextchat.py\":183-200",
            "content": "        model.model.vision_tower = CLIPVisionModel.from_pretrained(model_args.vision_tower)  # not quantize clip\n        # model.model.vision_tower = CLIPVisionModel.from_pretrained(model_args.vision_tower, **kwargs)  # quantize clip\u3001\n    vision_config = model_vision_dict['vision_config']\n    model.config.mm_use_im_start_end = model_args.mm_use_im_start_end\n    vision_config.use_im_start_end = model_args.mm_use_im_start_end\n    model.initialize_vision_tokenizer(mm_use_im_start_end=model_args.mm_use_im_start_end,\n                                      tokenizer=tokenizer,\n                                      device=training_args.device,\n                                      tune_mm_mlp_adapter=model_args.tune_mm_mlp_adapter,\n                                      pretrain_mm_mlp_adapter=model_args.pretrain_mm_mlp_adapter)\n    # grad check\n    model.requires_grad_(False)\n    # model.model.vision_tower.requires_grad_(False)\n    model.seg_prompt_mlp.requires_grad_(True)\n    # model.sam.model.prompt_encoder.requires_grad_(True)"
        },
        {
            "comment": "This code is setting the requires_grad attribute of a model's sam, mask_decoder to True while turning off gradients for other parameters. It also warns if there are less than 10 parameters without gradients when using FSDP (Fully Sharded Data Parallel), which is experimental and requires PyTorch-nightly.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/builder/build_nextchat.py\":201-215",
            "content": "    # model.sam.requires_grad_(False)\n    model.sam.model.mask_decoder.requires_grad_(True)\n    params_no_grad = [n for n, p in model.named_parameters() if not p.requires_grad]\n    if len(params_no_grad) > 0:\n        if training_args.fsdp is not None and len(training_args.fsdp) > 0:\n            if len(params_no_grad) < 10:\n                print('[WARNING] Attempting to use FSDP while {} parameters do not require gradients: {}'.format(len(params_no_grad),\n                                                                                                                 params_no_grad))\n            else:\n                print('[WARNING] Attempting to use FSDP while {} parameters do not require gradients: {}...(omitted)'.format(\n                    len(params_no_grad), ', '.join(params_no_grad[:10])))\n            print(\"[WARNING] Attempting to use FSDP with partially frozen parameters, this is experimental.\")\n            print(\n                \"[WARNING] As of 4/30/23, this feature requires PyTorch-night"
        },
        {
            "comment": "This code imports FullyShardedDataParallel (FSDP) from torch.distributed and patches its __init__ method to allow using original parameters. It then defines a preprocessor dictionary containing image, text, and conv fields with various arguments. Finally, it dumps a JSON file containing whether each model parameter requires gradients or not.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/builder/build_nextchat.py\":215-239",
            "content": "ly build.  See here for details: https://github.com/haotian-liu/LLaVA#experimental-use-fsdp-to-save-memory-in-pretraining\")\n            from torch.distributed.fsdp.fully_sharded_data_parallel import FullyShardedDataParallel as FSDP\n            def patch_FSDP_use_orig_params(func):\n                def wrap_func(*args, **kwargs):\n                    use_orig_params = kwargs.pop('use_orig_params', True)\n                    return func(*args, **kwargs, use_orig_params=use_orig_params)\n                return wrap_func\n            FSDP.__init__ = patch_FSDP_use_orig_params(FSDP.__init__)\n    preprocessor = dict(\n        image=model_vision_dict['image_processor'],\n        text=tokenizer,\n        conv=dict(\n            image_token_len=model_args.image_token_len,\n            sep_image_conv_front=model_args.sep_image_conv_front,\n            use_im_start_end=model_args.mm_use_im_start_end,\n        )\n    )\n    # TODO peft lora_model\n    import json\n    json.dump({k: bool(v.requires_grad) for k, v in model.named_parameters()}, open(\"param.json\", \"w\"))"
        },
        {
            "comment": "This function resizes the tokenizer and embedding in a model by adding special tokens, updating the input and output embeddings, and averaging new token embeddings to ensure they are divisible by 64.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/models/builder/build_nextchat.py\":240-263",
            "content": "    return model, preprocessor\ndef smart_tokenizer_and_embedding_resize(\n        special_tokens_dict: Dict,\n        tokenizer: transformers.PreTrainedTokenizer,\n        model: transformers.PreTrainedModel,\n):\n    \"\"\"Resize tokenizer and embedding.\n    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n    \"\"\"\n    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n    model.resize_token_embeddings(len(tokenizer))\n    if num_new_tokens > 0:\n        input_embeddings = model.get_input_embeddings().weight.data\n        output_embeddings = model.get_output_embeddings().weight.data\n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n        output_embeddings[-num_new_tokens:] = output_embeddings_avg"
        }
    ]
}