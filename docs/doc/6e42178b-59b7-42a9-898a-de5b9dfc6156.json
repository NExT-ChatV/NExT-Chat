{
    "summary": "The code imports libraries, sets up NextChat model, defines ML-LM parameters, prepares preprocessor for text and images, handles quantization, configures devices, includes image processing tasks, and allows customization, utilizing image processing techniques and pre-built models to generate AI chat system responses including response images. It lacks output image saving model implementation details.",
    "details": [
        {
            "comment": "The code is importing necessary libraries and modules, setting the logging level, creating a temporary directory for files, and loading pre-trained NextChat model. It also defines some directories and configurations for using ML-LM (Masked Language Model). The code seems to be part of an AI or machine learning application related to image processing and natural language processing tasks.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/demo/demo_util.py\":0-37",
            "content": "import json\nimport numbers\nimport os\nimport re\nimport sys\nimport logging\nimport time\nimport argparse\nimport tempfile\nfrom pathlib import Path\nfrom typing import List, Any, Union\nimport torch\nimport numpy as np\nimport gradio as gr\nfrom PIL import Image\nfrom PIL import ImageDraw, ImageFont\nfrom mmengine import Config\nimport transformers\n# from transformers import BitsAndBytesConfig\nimport torch.nn.functional as F\nsys.path.append(str(Path(__file__).parent.parent.parent))\nfrom mllm.dataset.process_function import PlainBoxFormatter\nfrom mllm.dataset.builder import prepare_interactive\nfrom mllm.utils import draw_bounding_boxes, ImageBoxState, bbox_draw, open_image, parse_boxes\nfrom mllm.models.builder.build_nextchat import load_pretrained_nextchat\nlog_level = logging.ERROR\ntransformers.logging.set_verbosity(log_level)\ntransformers.logging.enable_default_handler()\ntransformers.logging.enable_explicit_format()\nTEMP_FILE_DIR = Path(__file__).parent / 'temp'\nTEMP_FILE_DIR.mkdir(parents=True, exist_ok=True)\n#########################################"
        },
        {
            "comment": "This function builds an ML model using the specified `model_name_or_path` and `vit_model_path`. It allows for customization with various parameters including image token length, 8-bit model loading, and other configuration options related to backbone, MLP adapter tuning, data processing, etc.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/demo/demo_util.py\":38-73",
            "content": "# mllm model init\n#########################################\ndef build_model(model_name_or_path, vit_model_path, image_token_len=256, load_in_8bit=False):\n    model_args = Config(dict(\n        type='nextchat',\n        version='v1',\n        # checkpoint config\n        cache_dir=None,\n        model_name_or_path=model_name_or_path,\n        vision_tower=vit_model_path,\n        pretrain_mm_mlp_adapter=None,\n        sam_path=None,\n        # model config\n        mm_vision_select_layer=-2,\n        model_max_length=2048,\n        # finetune config\n        freeze_backbone=False,\n        tune_mm_mlp_adapter=False,\n        freeze_mm_mlp_adapter=False,\n        # data process config\n        is_multimodal=True,\n        sep_image_conv_front=False,\n        image_token_len=image_token_len,\n        mm_use_im_start_end=True,\n        target_processor=dict(\n            boxes=dict(type='PlainBoxFormatter'),\n        ),\n        process_func_args=dict(\n            conv=dict(type='ChatConvProcess'),\n            target=dict(type='BoxFormatProcess'),"
        },
        {
            "comment": "This code is loading and configuring a NextChat model with specific arguments, training arguments, and quantization kwargs. The model is loaded using the `load_pretrained_nextchat` function, and if necessary, converted to bfloat16 for efficient inference on GPUs.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/demo/demo_util.py\":74-108",
            "content": "            text=dict(type='ChatTextProcess'),\n            image=dict(type='ChatImageProcessor'),\n        ),\n        conv_args=dict(\n            conv_template='vicuna_v1.1',\n            transforms=dict(type='Expand2square'),\n            tokenize_kwargs=dict(truncation_size=None),\n        ),\n        gen_kwargs_set_pad_token_id=True,\n        gen_kwargs_set_bos_token_id=True,\n        gen_kwargs_set_eos_token_id=True,\n    ))\n    training_args = Config(dict(\n        bf16=True,\n        fp16=False,\n        device='cuda',\n        fsdp=None,\n    ))\n    # if load_in_8bit:\n    #     quantization_kwargs = dict(\n    #         quantization_config=BitsAndBytesConfig(\n    #             load_in_8bit=True,\n    #         )\n    #     )\n    # else:\n    #     quantization_kwargs = dict()\n    quantization_kwargs = dict()\n    model, preprocessor = load_pretrained_nextchat(model_args, training_args, **quantization_kwargs)\n    if not getattr(model, 'is_quantized', False):\n        model.to(dtype=torch.bfloat16, device=torch.device('cuda'))\n    if not getattr(model.model.get_vision_tower(), 'is_quantized', False):"
        },
        {
            "comment": "This code sets the LLM and vision model devices, checks if they are quantized or loaded in specific bit formats, and then prepares a preprocessor for text and images. The parse_text function normalizes image bounding boxes and de_norm_box_xyxy function denormalizes bounding boxes.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/demo/demo_util.py\":109-130",
            "content": "        model.model.get_vision_tower().to(dtype=torch.bfloat16, device=torch.device('cuda'))\n    print(f\"LLM device: {model.device}, is_quantized: {getattr(model, 'is_quantized', False)}, is_loaded_in_4bit: {getattr(model, 'is_loaded_in_4bit', False)}, is_loaded_in_8bit: {getattr(model, 'is_loaded_in_8bit', False)}\")\n    print(f\"vision device: {model.model.get_vision_tower().device}, is_quantized: {getattr(model.model.get_vision_tower(), 'is_quantized', False)}, is_loaded_in_4bit: {getattr(model, 'is_loaded_in_4bit', False)}, is_loaded_in_8bit: {getattr(model, 'is_loaded_in_8bit', False)}\")\n    preprocessor['target'] = {'boxes': PlainBoxFormatter()}\n    tokenizer = preprocessor['text']\n    return model, model_args, preprocessor, tokenizer\n#########################################\n# demo utils\n#########################################\ndef parse_text(text):\n    text = text.replace(\"<image>\", \"&lt;image&gt;\")\n    return text\ndef de_norm_box_xyxy(box, *, w, h):\n    x1, y1, x2, y2 = box\n    x1 = x1 * w\n    x2 = x2 * w"
        },
        {
            "comment": "`resize_pil_img` defines a function that takes in a PIL image and resizes it to the specified width and height. It first checks the current aspect ratio of the image, then determines the new dimensions based on either the width or height being provided as arguments. `expand2square` is a function that expands an image to a square shape by creating a new image with the larger dimension matching the width, then pasting the original image into the center. `box_xyxy_expand2square` adjusts the bounding box of an object in an image based on whether the image is being expanded to a square aspect ratio. If the width becomes greater than height, it adjusts the y coordinates; if the height becomes greater, it adjusts the x coordinates.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/demo/demo_util.py\":131-170",
            "content": "    y1 = y1 * h\n    y2 = y2 * h\n    box = x1, y1, x2, y2\n    return box\ndef expand2square(pil_img, background_color=(255, 255, 255)):\n    width, height = pil_img.size\n    if width == height:\n        return pil_img\n    elif width > height:\n        result = Image.new(pil_img.mode, (width, width), background_color)\n        result.paste(pil_img, (0, (width - height) // 2))\n        return result\n    else:\n        result = Image.new(pil_img.mode, (height, height), background_color)\n        result.paste(pil_img, ((height - width) // 2, 0))\n        return result\ndef box_xyxy_expand2square(box, *, w, h):\n    if w == h:\n        return box\n    if w > h:\n        x1, y1, x2, y2 = box\n        y1 += (w - h) // 2\n        y2 += (w - h) // 2\n        box = x1, y1, x2, y2\n        return box\n    assert w < h\n    x1, y1, x2, y2 = box\n    x1 += (h - w) // 2\n    x2 += (h - w) // 2\n    box = x1, y1, x2, y2\n    return box\ndef resize_pil_img(pil_img: Image.Image, *, w, h):\n    old_height, old_width = pil_img.height, pil_img.width\n    new_height, new_width = (h, w)"
        },
        {
            "comment": "Code snippet resizes images, adjusts bounding box coordinates, binarizes pixels, and transforms masks. This code seems to be part of an image processing library for object detection tasks.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/demo/demo_util.py\":171-207",
            "content": "    if (new_height, new_width) == (old_height, old_width):\n        return pil_img\n    return pil_img.resize((new_width, new_height))\ndef resize_box_xyxy(boxes, *, w, h, ow, oh):\n    old_height, old_width = (oh, ow)\n    new_height, new_width = (h, w)\n    if (new_height, new_width) == (old_height, old_width):\n        return boxes\n    w_ratio = new_width / old_width\n    h_ratio = new_height / old_height\n    out_boxes = []\n    for box in boxes:\n        x1, y1, x2, y2 = box\n        x1 = x1 * w_ratio\n        x2 = x2 * w_ratio\n        y1 = y1 * h_ratio\n        y2 = y2 * h_ratio\n        nb = (x1, y1, x2, y2)\n        out_boxes.append(nb)\n    return out_boxes\ndef binarize(x):\n    return (x != 0).astype('uint8') * 255\ndef de_transform_mask(orgw, orgh, mask):\n    long_side = max(orgw, orgh)\n    short_side = min(orgw, orgh)\n    pad = (long_side - short_side) // 2\n    mask = F.interpolate(mask, [long_side, long_side], mode=\"bilinear\", align_corners=False)\n    mask = mask > 0\n    mask[mask > 0] = 255\n    if orgw < orgh:\n        mask = mask[..., :, pad: short_side + pad]"
        },
        {
            "comment": "Code snippet from NExT-Chat/mllm/demo/demo_util.py:208-243 includes functions for handling mask and bounding box manipulations. The code contains a function that handles mask transformations based on image dimensions, occludes masks by union operation, and a function to draw boxes with texts on an image. It is related to image processing and computer vision tasks.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/demo/demo_util.py\":208-243",
            "content": "    else:\n        mask = mask[..., pad: short_side + pad, :]\n    # mask = mask.transpose(2, 3)\n    # print(mask.shape)\n    return mask.squeeze(1)\ndef de_occlude_masks(masks):\n    union_mask = torch.zeros_like(masks[0]).int()\n    for i, m in enumerate(masks):\n        m[m.int() + union_mask.int() >= 2] = 0\n        print((m.int() + union_mask.int() >= 2).sum())\n        union_mask = m.int() + union_mask\n    return masks\ndef de_transform_box(orgw, orgh, boxes):\n    long_side = max(orgw, orgh)\n    short_side = min(orgw, orgh)\n    pad = (long_side - short_side) // 2\n    boxes = boxes * long_side\n    if orgw < orgh:\n        boxes[:, 0] -= pad\n        boxes[:, 2] -= pad\n    else:\n        boxes[:, 1] -= pad\n        boxes[:, 3] -= pad\n    return boxes\ndef draw_boxes(img, _boxes, texts, colors):\n    assert img is not None\n    _img_draw = ImageDraw.Draw(img)\n    font = ImageFont.truetype(os.path.join(os.path.dirname(__file__), 'assets/DejaVuSansMono.ttf'), size=36)\n    for bid, box in enumerate(_boxes):\n        _img_draw.rectangle((box[0], box[1], box[2], box[3]), outline=colors[bid % len(colors)], width=8)"
        },
        {
            "comment": "Code snippet from \"NExT-Chat/mllm/demo/demo_util.py\":244-266 defines a function that takes an image, text, and bounding boxes as input and adds the text within the bounding boxes to the image using the provided font and color. The post_process_response function processes responses containing \"<at> <boxes>\", splitting them into separate parts and joining them back together after HTML encoding. The model_predict function takes a model, model_args, tokenizer, preprocessor, image, text, temperature, top_p, top_k, boxes, boxes_seq, and iou_thres as input and returns the predicted output for the provided image and text.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/demo/demo_util.py\":244-266",
            "content": "        anno_text = texts[bid]\n        _img_draw.rectangle(\n            (box[0], box[3] - int(font.size * 1.2), box[0] + int((len(anno_text) + 0.8) * font.size * 0.6), box[3]),\n            outline=colors[bid % len(colors)], fill=colors[bid % len(colors)], width=8)\n        _img_draw.text((box[0] + int(font.size * 0.2), box[3] - int(font.size * 1.2)), anno_text, font=font,\n                       fill=(255, 255, 255))\n    return img\ndef post_process_response(response):\n    if \"<at> <boxes>\" not in response:\n        return response.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n    splits = response.split(\"<at> <boxes>\")\n    to_concat = [f\"[{i}]\" for i in range(len(splits) - 1)]\n    rst = [splits[i // 2] if i % 2 == 0 else to_concat[i // 2]\n           for i in range(len(splits) + len(to_concat))]\n    rst = \"\".join(rst)\n    rst = rst.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n    return rst\ndef model_predict(model, model_args, tokenizer, preprocessor, image, text,\n                  temperature=0.75, top_p=0.7, top_k=5, boxes=None, boxes_seq=None, iou_thres=0.3):"
        },
        {
            "comment": "This code segment opens an image, prepares a conversation for interactive model input, generates output and masks from the model's response, removes low-quality masks, deformats boxes and masks, and performs grounding by assigning the generated response to specific regions of the image.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/demo/demo_util.py\":267-291",
            "content": "    image = open_image(image)\n    orgw, orgh = image.width, image.height\n    conversation = prepare_interactive(model_args, preprocessor)\n    conversation.set_image(image)\n    conversation.append_message(role=conversation.roles[0],\n                                message=text, boxes=boxes, boxes_seq=boxes_seq)\n    inputs = conversation.to_model_input()\n    inputs.update({\"temperature\": temperature, \"top_p\": top_p, \"top_k\": top_k})\n    output_ids, masks, iou_predictions, boxes = model.generate(**inputs)\n    response = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n    # print(response)\n    # print(boxes)\n    filename_grounding = None\n    ret_image = None\n    if boxes is not None:\n        # remove low quality masks\n        # print(masks.shape, iou_predictions.squeeze().shape)\n        for idx, iou in enumerate(iou_predictions.squeeze(1)):\n            if iou < iou_thres:\n                masks[idx, :] = 0\n        boxes = de_transform_box(orgw, orgh, boxes)\n        masks = de_transform_mask(orgw, orgh, masks)"
        },
        {
            "comment": "Code segment demonstrates the application of image processing and visualization techniques to generate a response image for an AI chat system. It utilizes PILToTensor and ToPILImage transforms to convert image data, applies color-coded masks and boxes using draw_segmentation_masks and draw_boxes functions, but does not save the final output image as intended due to missing model implementation details in the provided code snippet.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/demo/demo_util.py\":292-317",
            "content": "        # masks = de_occlude_masks(masks)\n        colors = [\"#F76566\", \"#18ACBA\", \"#9400D3\", \"#454926\", \"#4E72B8\"]\n        if len(colors) < len(boxes):\n            colors += [\"#F76566\"] * (len(boxes) - len(colors))\n        from torchvision.transforms import PILToTensor, ToPILImage\n        Timage = PILToTensor()(image)\n        from torchvision.utils import draw_segmentation_masks\n        res = draw_segmentation_masks(Timage, masks, colors=colors, alpha=0.5)\n        res = ToPILImage()(res)\n        res = draw_boxes(res, boxes, [str(i) for i in range(len(boxes))], colors)\n        ret_image = res\n        # timestamp = int(time.time())\n        # filename_grounding = f\"tmp/sever_imgs/{timestamp}.jpg\"\n        # if not os.path.exists(\"tmp/sever_imgs/\"):\n        #     os.makedirs(\"tmp/sever_imgs/\")\n        # res.save(filename_grounding)\n    return response, boxes, masks, ret_image\nclass NextChatInference(object):\n    def __init__(self, model_path, vit_path, image_token_len=576, **kwargs):\n        self.model, self."
        },
        {
            "comment": "This code creates an object that takes input tensors and uses a pre-built model to generate responses based on an image and text. It also allows for temperature, top_p, and top_k parameters as well as boxes and boxes_seq lists to filter results by IOU threshold. The method calls the 'model_predict' function with the necessary arguments to get the response, boxes, masks, and potentially return the original image.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/demo/demo_util.py\":317-334",
            "content": "model_args, self.preprocessor, self.tokenizer = build_model(model_path, vit_path, image_token_len=image_token_len)\n    def __call__(self, input_tensor, **forward_params):\n        forward_params = {}\n        temperature = forward_params.pop(\"temperature\", 0.8)\n        top_p = forward_params.pop(\"top_p\", 0.7)\n        top_k = forward_params.pop(\"top_k\", 5)\n        boxes = forward_params.pop(\"boxes\", [])\n        boxes_seq = forward_params.pop(\"boxes_seq\", [])\n        iou_thres = forward_params.pop(\"iou_thres\", 0.3)\n        image, text = input_tensor[\"image\"], input_tensor[\"text\"]\n        response, boxes, masks, ret_img = model_predict(self.model, self.model_args,\n                                          self.tokenizer, self.preprocessor,\n                                          image, text,\n                                          temperature=temperature,\n                                          top_p=top_p,\n                                          top_k=top_k,\n                                          iou_thres=iou_thres,"
        },
        {
            "comment": "These lines are returning a response, bounding boxes, masks, and a possibly retouched image from an image classification function.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/demo/demo_util.py\":335-336",
            "content": "                                          boxes=boxes, boxes_seq=boxes_seq)\n        return response, boxes, masks, ret_img"
        }
    ]
}