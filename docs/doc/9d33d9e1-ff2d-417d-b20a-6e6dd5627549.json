{
    "summary": "The code creates a class for single image datasets, validates inputs and applies transformations before storing them. It provides methods to manage data storage, retrieval, and conversion to Gradio Chatbot format, making it suitable for chat apps with images.",
    "details": [
        {
            "comment": "This code defines a class for interactive single image dataset, which allows appending messages with roles, boxes, and points. It initializes attributes and includes methods to set the image and append messages. The ResizeAndPad transform is used for image resizing.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/single_image_interactive.py\":0-32",
            "content": "import copy\nfrom typing import Optional\nfrom PIL import Image\nimport torch\nfrom .single_image_convsation import SingleImageConvDatasetMixin\nfrom mllm.models.sam.transforms import ResizeAndPad\nclass SingleImageInteractive(SingleImageConvDatasetMixin):\n    _printed_sample = True\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.image: Optional[Image.Image] = None\n        self.roles = ('human', 'gpt')\n        self.boxes = []\n        self.points = []\n        self.raw_conv = []\n        self.conversations = []\n        self.sam_transform = ResizeAndPad(1024)\n    def set_image(self, image: Image.Image):\n        assert self.image is None, f\"{image}\"\n        self.image = image\n    def append_message(self, role: str, message: str, *, boxes=None, points=None, boxes_seq=None, points_seq=None):\n        \"\"\"Append a new message.\"\"\"\n        assert role in self.roles\n        def convert_idx(objs_seq, objs_value, get_obj_idx_func):\n            if objs_seq is None:\n                return None"
        },
        {
            "comment": "This function converts object indices and applies the conversion to box and point sequences. It then checks if the previous message had an image placeholder, and modifies the current message accordingly before appending a new conversation entry with the specified 'from' role and message value.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/single_image_interactive.py\":33-57",
            "content": "            ret = []\n            for objs_idx in objs_seq:\n                new_objs_idx = []\n                for idx in objs_idx:\n                    new_idx = get_obj_idx_func(objs_value[idx])\n                    new_objs_idx.append(new_idx)\n                ret.append(tuple(new_objs_idx))\n            return tuple(ret)\n        boxes_seq = convert_idx(boxes_seq, boxes, self._get_box_idx)\n        points_seq = convert_idx(points_seq, points, self._get_point_idx)\n        if self.image is not None:\n            previous_message_has_image_placeholder = any(\n                '<image>' in item['value'] for item in self.conversations\n            )\n            if not previous_message_has_image_placeholder and '<image>' not in message:\n                message = '<image> ' + message\n            if previous_message_has_image_placeholder and '<image>' in message:\n                message = message.replace('<image>', '')\n        self.conversations.append(\n            {\n                'from': role,\n                'value': message,"
        },
        {
            "comment": "This code is part of a dataset for an interactive image chatbot. It defines classes for storing data related to images, boxes, points, and conversations. The `get_raw_item` method returns a copy of the raw item data, ensuring that the 'conversations' list always ends with a response from the second role. The `to_model_input` method converts an item into model input format, including input IDs, image(s), and optional location inputs if present.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/single_image_interactive.py\":58-90",
            "content": "                'boxes_seq': copy.deepcopy(boxes_seq),\n                'points_seq': copy.deepcopy(points_seq),\n            }\n        )\n    def get_raw_item(self, index=None):\n        ret = copy.deepcopy({\n            'image': self.image,\n            'target': {\n                'boxes': self.boxes,\n                'points': self.points,\n            },\n            'conversations': self.conversations,\n        })\n        assert ret['conversations'][0]['from'] == self.roles[0]\n        if ret['conversations'][-1]['from'] == self.roles[0]:\n            ret['conversations'].append(\n                {\n                    'from': self.roles[1],\n                    'value': '',\n                }\n            )\n        return ret\n    def to_model_input(self):\n        item = self.__getitem__(0)\n        ret = {'input_ids': item['input_ids'].unsqueeze(0).cuda()}\n        if 'image' in item and item['image'] is not None:\n            ret['images'] = item['image'].unsqueeze(0).cuda()\n        else:\n            ret['images'] = None\n        if item.get(\"loc_inputs\", None) is not None:"
        },
        {
            "comment": "The code includes a function that processes a dataset for single image interactive tasks. It assigns tensorized 'loc_inputs' if available, and creates 'images_sam' tensor if the corresponding data exists. The function also sets 'attention_mask' to None. Another function converts new messages into the Gradio Chatbot format by stripping unnecessary tags from the messages. A method is used to retrieve an index of a box or point given its coordinates. It checks if the box is in the boxes list and returns its index, or adds it to the list and returns the index.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/single_image_interactive.py\":91-117",
            "content": "            ret['loc_inputs'] = torch.tensor(item['loc_inputs']).cuda()\n        if item.get(\"images_sam\", None) is not None:\n            ret['images_sam'] = item['images_sam'].unsqueeze(0).cuda()\n        ret['attention_mask'] = None\n        return ret\n    def to_gradio_chatbot_new_messages(self):\n        conv = self.__getitem__(0, return_conv=True)\n        new_messages = conv.messages[-2:]\n        ret_messages = []\n        for r, m in new_messages:\n            nm = m.replace('<im_patch>', '').replace('<im_end>', '').replace('<im_start>', '<image>')\n            ret_messages.append((r, nm))\n        return ret_messages\n    def _get_box_idx(self, box):\n        assert isinstance(box, (tuple, list)), f\"{type(box)}\"\n        assert isinstance(box[0], (int, float)), f\"{type(box[0])}\"\n        assert len(box) == 4\n        box = tuple(box)\n        if box not in self.boxes:\n            self.boxes.append(box)\n            return len(self.boxes) - 1\n        else:\n            return self.boxes.index(box)\n    def _get_point_idx(self, point):"
        },
        {
            "comment": "This code seems to be part of a class that handles interactive single-image data for a chat application. It validates the input 'point' and manages its storage in a list called 'self.points'. The class also provides methods to return the length of the list and retrieve elements by their index. The code applies transformations to the images and targets before storing them. It mentions using a 'sam_transform' function but does not specify what it does. It also notes that if 'image' is a list, it will apply transformations to each individual element in the list.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/single_image_interactive.py\":118-147",
            "content": "        assert isinstance(point, (tuple, list))\n        assert isinstance(point[0], (int, float))\n        assert len(point) == 2\n        point = tuple(point)\n        if point not in self.points:\n            self.points.append(tuple(point))\n            return len(self.points) - 1\n        else:\n            return self.points.index(point)\n    def __len__(self):\n        return 1\n    def __getitem__(self, index, debug_mode=False, return_conv=False):\n        # getitem\n        item = self.get_raw_item(index)\n        image = item.get('image', None)\n        target = item.get('target', None)\n        raw_conv = item['conversations']\n        # sam transform\n        sam_image, sam_masks, sam_hw = self.sam_transform(image, target.get(\"masks\", None))\n        # transform\n        assert isinstance(image, list) == isinstance(target, list)\n        multimage_mode = isinstance(image, list)\n        if isinstance(image, list):\n            # TODO: validate raw item\n            transformed_image, transformed_target = [], []\n            for img, tgt in zip(image, target):"
        },
        {
            "comment": "The code is responsible for data preprocessing in an image interactive dataset. It checks if transforms and image are not None, applies the transforms, updates width and height of target images, and appends to lists. If only single image, validates raw item and applies transforms. Then, it processes convolution and updates image using multimage process.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/single_image_interactive.py\":148-167",
            "content": "                if self.transforms is not None and image is not None:\n                    img, tgt = self.transforms(img, tgt)\n                if tgt is not None:\n                    tgt['width'], tgt['height'] = img.width, img.height\n                transformed_image.append(img)\n                transformed_target.append(tgt)\n            image, target = transformed_image, transformed_target\n        else:\n            self.validate_raw_item(item)  # only validate for single image.\n            if self.transforms is not None and image is not None:\n                image, target = self.transforms(image, target)\n            has_image = 'image' in item and bool(item['image'])\n            has_target = 'target' in item and bool(item['target']) and any(bool(elem) for elem in item['target'].values())\n            if has_target and has_image:\n                target['width'], target['height'] = image.width, image.height\n        # preprocess\n        raw_conv = self.process_conv(raw_conv)\n        raw_conv, image = self.process_conv_multimage(raw_conv, image)"
        },
        {
            "comment": "This function processes raw conversation data, builds a conversation and updates it with text and image data. It then adds target boxes for both input and target locations, as well as sampled images and masks. Finally, it prints the sample and returns the final dictionary of results. Debug mode optionally includes additional information like raw_conv, conv, and image in the return value.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/single_image_interactive.py\":168-187",
            "content": "        raw_conv, tar_boxes = self.process_target(raw_conv, target, multimage_mode=multimage_mode)\n        conv = self.build_conv(raw_conv)\n        if return_conv:\n            # noinspection PyTypeChecker\n            return conv\n        text_dict = self.process_text(conv)\n        image_dict = self.process_image(image)\n        # return\n        ret_dict = {}\n        ret_dict.update(text_dict)\n        ret_dict.update(image_dict)\n        ret_dict[\"loc_inputs\"] = tar_boxes[\"all_boxes\"]\n        ret_dict[\"loc_targets\"] = tar_boxes[\"gpt_boxes\"]\n        ret_dict[\"images_sam\"] = sam_image\n        ret_dict[\"masks_sam\"] = sam_masks\n        self._print_sample(ret_dict, raw_conv, conv)\n        if debug_mode:\n            return {'ret': ret_dict, 'raw_conv': raw_conv, 'conv': conv, 'image': image}\n        return ret_dict"
        }
    ]
}