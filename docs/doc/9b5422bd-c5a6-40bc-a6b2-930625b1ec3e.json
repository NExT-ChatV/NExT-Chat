{
    "summary": "The `prepare_data` function creates a dataset, applies optional transforms, constructs a process function and returns a SingleImageInteractive dataset object for NExT-Chat.",
    "details": [
        {
            "comment": "This function `prepare_data` takes in data, model arguments and training arguments as inputs. It builds datasets for 'train', 'validation', and 'test' using the provided partial functions from DATASETS module. Depending on the training arguments, if do_train is True, it will build a dataset for training; if do_eval is True, it will build a dataset for validation; if do_predict is True, it will build a dataset for testing. The function returns a DatasetDict and an optional ComputeMetrics.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/builder.py\":0-26",
            "content": "from functools import partial\nfrom typing import Callable, Dict, Tuple, Any, Optional\nfrom torch.utils.data import Dataset\nfrom transformers import EvalPrediction, TrainingArguments\nfrom .root import DATASETS, METRICS, TRANSFORMS, FUNCTIONS\nfrom .single_image_convsation import WRAPPER_DATASET\nfrom .single_image_interactive import SingleImageInteractive\nfrom ..conversation import get_conv_template\nfrom .utils import init_ceph_client_if_needed\nDatasetDict = Dict[str, Dataset]\nComputeMetrics = Callable[[EvalPrediction], Dict]\ndef prepare_data(\n        data_args,\n        model_args,\n        training_args: TrainingArguments,\n        preprocessor: Dict[str, Any],\n) -> Tuple[DatasetDict, Optional[ComputeMetrics]]:\n    # raw dataset\n    datasets = {\n        'train': partial(DATASETS.build, data_args.train) if training_args.do_train else None,\n        'validation': partial(DATASETS.build, data_args.validation) if training_args.do_eval else None,\n        'test': partial(DATASETS.build, data_args.test) if training_args.do_predict else None,"
        },
        {
            "comment": "This code appears to be building a dataset for a model, handling compute metrics, and wrapping the dataset with conv functionality. It involves getting configuration arguments, building compute metrics and transforms, and creating a process function. The final step is building a wrapper dataset class using the obtained configurations.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/builder.py\":27-52",
            "content": "    }\n    # compute metric\n    compute_metric_cfg = data_args.get('compute_metric', None)\n    compute_metrics = build_compute_metric(compute_metric_cfg, preprocessor)\n    # conv dataset wrap\n    conv_args = model_args.conv_args\n    tokenize_kwargs = conv_args.get('tokenize_kwargs', {})\n    conv_template = conv_args.get('conv_template', 'vicuna_v1.1')\n    conv_template = partial(get_conv_template, name=conv_template)\n    transforms = conv_args.get('transforms', None)\n    if transforms is not None:\n        transforms = TRANSFORMS.build(transforms)\n    # process func\n    process_func = {}\n    for k, v in model_args.process_func_args.items():\n        process_func[k] = FUNCTIONS.build(cfg=v)\n    conv_dataset_cls = partial(\n        WRAPPER_DATASET.get(data_args.get(\"dataset_wrapper\", \"conv\"), \"conv\"),\n        preprocessor=preprocessor,\n        process_func=process_func,\n        tokenize_kwargs=tokenize_kwargs,\n        conv_template=conv_template,\n        training_args=training_args,\n        transforms=transforms,\n    )"
        },
        {
            "comment": "The code creates datasets for training, validation, and test sets based on provided dataset generators. If multi-test sets are specified in data_args and do_multi_predict is True in training_args, it processes the multitest set by creating a dictionary of datasets and compute metrics for each test set.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/builder.py\":53-69",
            "content": "    ds = {\n        'train': conv_dataset_cls(dataset_generator=datasets['train'], mode='train') if datasets['train'] is not None else None,\n        'validation': conv_dataset_cls(dataset_generator=datasets['validation'], mode='validation') if datasets['validation'] is not None else None,\n        'test': conv_dataset_cls(dataset_generator=datasets['test'], mode='test') if datasets['test'] is not None else None,\n    }\n    # multi test set\n    if hasattr(data_args, 'multitest') and bool(data_args.multitest) \\\n            and hasattr(training_args, 'do_multi_predict') and training_args.do_multi_predict:\n        print(f\"processing multitest set\")\n        k2v = {}\n        for k, item in data_args.multitest.items():\n            _dataset_cls = partial(DATASETS.build, item['cfg'])\n            _compute_metric = build_compute_metric(item['compute_metric'], preprocessor)\n            k2v[k] = {\n                \"dataset\": conv_dataset_cls(dataset_generator=_dataset_cls, mode='test'),\n                \"compute_metric\": _compute_metric"
        },
        {
            "comment": "This code is part of the NExT-Chat dataset builder. It initializes a ceph client if necessary, builds compute metrics based on configuration, and prepares an interactive conversation model with a given set of arguments and a partial function for getting a conversation template. The function takes preprocessor and model_args as parameters. The code also includes a function to build compute metrics using the METRICS module.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/builder.py\":70-99",
            "content": "            }\n        ds['multitest'] = k2v\n        print(f\"processing multitest set. done.\")\n    # in default, ceph client do init at the beginning of program.\n    #  importantly, before dataloader worker fork.\n    lazy_init = data_args.get('lazy_init', True)\n    if not lazy_init:\n        init_ceph_client_if_needed()\n    return ds, compute_metrics\ndef build_compute_metric(compute_metric_cfg, preprocessor):\n    if compute_metric_cfg is not None:\n        compute_metric_cfg = dict(compute_metric_cfg)  # copy cfg because we modify it\n        compute_metric_cfg.update(dict(preprocessor=preprocessor))\n        compute_metrics = METRICS.build(cfg=compute_metric_cfg)\n    else:\n        compute_metrics = None\n    return compute_metrics\ndef prepare_interactive(\n        model_args,\n        preprocessor: Dict[str, Any],\n):\n    conv_args = model_args.conv_args\n    tokenize_kwargs = conv_args.get('tokenize_kwargs', {})\n    conv_template = conv_args.get('conv_template', 'vicuna_v1.1')\n    conv_template = partial(get_conv_template, name=conv_template)"
        },
        {
            "comment": "This code builds a dataset for the NExT-Chat system. It checks if 'transforms' is present in the config and applies them, then constructs a process function from the input arguments. It finally returns a SingleImageInteractive dataset object with the specified parameters.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/mllm/dataset/builder.py\":100-117",
            "content": "    transforms = conv_args.get('transforms', None)\n    if transforms is not None:\n        transforms = TRANSFORMS.build(transforms)\n    # process func\n    process_func = {}\n    for k, v in model_args.process_func_args.items():\n        process_func[k] = FUNCTIONS.build(cfg=v)\n    ds = SingleImageInteractive(\n        preprocessor=preprocessor,\n        process_func=process_func,\n        tokenize_kwargs=tokenize_kwargs,\n        conv_template=conv_template,\n        training_args=None,\n        transforms=transforms,\n        mode='test',\n    )\n    return ds"
        }
    ]
}