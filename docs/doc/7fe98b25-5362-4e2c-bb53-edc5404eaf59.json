{
    "summary": "NextChat is a chat LMM with image-based text generation models. Training involves three stages: VL+Detection Pre-training, VL+Detection Instruction Following, and VL+Detection+Segmentation. The `nextchat-7b-336-v1` model has been updated for better performance and easier training templates.",
    "details": [
        {
            "comment": "NExT-Chat is an LMM (Language Modeling and Machin...",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/README.md\":0-25",
            "content": "# NExT-Chat\nNExT-Chat: An LMM for Chat, Detection and Segmentation\n[Ao Zhang](https://waxnkw.github.io/), [Yuan Yao](https://yaoyuanthu.github.io/), [Wei Ji](https://jiwei0523.github.io/), [Zhiyuan Liu](https://nlp.csai.tsinghua.edu.cn/~lzy/), and [Tat-Seng Chua](https://www.chuatatseng.com/)\n**National University of Singapore, Tsinghua University**\nProject page with demo: [NExT-Chat](https://next-chatv.github.io/)\n-----------------\n<a href='https://next-chatv.github.io/'><img src='https://img.shields.io/badge/Project-Page-Green'></a>\n<a href='https://arxiv.org/abs/2311.04498'><img src='https://img.shields.io/badge/Paper-Arxiv-red'></a>\n<a href='https://ee569fe29733644a33.gradio.live'><img src='https://img.shields.io/badge/Demo-Page-blue'></a> \n[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=q0EdZgv6uQg)\n## What's New: \ud83c\udf89 \n- [x] 2023.12.12 Initial code released\n## Table of Contents\n  - [Introduction](#introduction)\n  - [Installation](#installation)\n  - [Model Zoo](#model-zoo)"
        },
        {
            "comment": "The code provides an introduction to the NExT-Chat, a Long Message Model (LMM) for chat with detection and segmentation results. The framework is demonstrated in a provided image. It explains how to install the model, cloning the repo and installing the requirements. There are currently three models available, with different sizes, resolutions, and GPU memory usage, with one recommended version and two not recommended versions.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/README.md\":26-57",
            "content": "  - [Data Preparation](#data-preparation)\n  - [Demo](#demo)\n  - [Evaluation](#evaluation)\n  - [Training](#training)\n  - [Examples](#examples)\n  - [Acknowledgement](#acknowledgement)\n## Introduction\nAn LMM for chat with detection and segmentation results.\nThe framework is shown:\n[![demo](https://next-chatv.github.io/images/method1.png)](https://next-chatv.github.io)\n## Installation\nPlease clone the repo:\n```shell\ngit clone https://github.com/NExT-ChatV/NExT-Chat.git\ncd NExT-Chat\n```\nThen install requirements:\n```shell\npip install -r requirements.txt\n```\n## Model Zoo\nCurrently, we totally have 3 models:\n|Version| ckpt | LM Size | ViT Res. | GPU Mem. |Comment|\n|----------|----------|----------|---------|----------|----------|\n|v1| [nextchat-7b-336](https://huggingface.co/AoZhang/nextchat-7b-336) | 7B | 336x336 | ~32G     |recommended|\n|v0| [nextchat-7b-224](https://www.modelscope.cn/models/ZhangAo6/nextchat/files) | 7B | 224x224 | ~24G     |not recommended|\n|v0| [nextchat-13b-224](https://www.modelscope.cn/models/ZhangAo6/nextchat/files) | 7B | 224x224 | ~35G     |not recommended|"
        },
        {
            "comment": "This code provides an updated version of the `nextchat-7b-336-v1` model, with improved performance and easier-to-use training templates. The templates are available in [templates](config/_base_/dataset/template/) for localization, grounded captioning, and VQA+Localization tasks. Users need to refer to the DATA.md file for data preparation, and download the model weights from either HuggingFace or a provided link.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/README.md\":59-88",
            "content": "We recommend to use the `nextchat-7b-336-v1`, which can achieve better performance.\nMoreover, we also update the training templates for `nextchat-7b-336-v1` to make it easier to use.\nYou can refer to [templates](config/_base_/dataset/template/) for details in eliciting concrete abilities.\nSome examples:\n1. Localize a object:\n|Version| Template |\n|----------|----------|\n|v0|Where is XXX in the <image>?|\n|v1|Where is XXX in the image?|\n2. Grounded Caption:\n|Version| Template |\n|----------|---------|\n|v0|Can you provide a description of the image <image> and include the locations for each mentioned object?|\n|v1|Can you describe the image and include object locations?|\n3. VQA+Localization\n|Version| Template |\n|----------|----------|\n|v0|<Question> Please include object locations and explain.|\n|v1|<Question> Please mention related object locations.|\n## Data Preparation\nPlease refer to [DATA.md](DATA.md).\n## Demo\nPlease first download the model weights from [huggingface](https://huggingface.co/AoZhang/nextchat-7b-336/tree/main) or our [link](https://thunlp.oss-cn-qingdao.aliyuncs.com/nextchat-7b-336.tar.gz)."
        },
        {
            "comment": "This code snippet instructs the user to use a specific pre-trained visual encoder model and segment anything (SAM) model, download them if necessary, modify the config file with their paths, and then run either web or bash demo scripts providing the respective paths. The code also provides examples for running the demos using either locally downloaded or remotely accessible models.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/README.md\":89-106",
            "content": "We also use OpenAI CLIP [ViT model](https://huggingface.co/openai/clip-vit-large-patch14-336/tree/main) as the visual encoder. Please make sure that you can connect to huggingface or you can download it to your local directory.\nThen, download the [SAM](https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth) and modify `sam_path` in [config/_base_/model/nextchat.py](https://github.com/NExT-ChatV/NExT-Chat/blob/6e92d9b13b08e978190a00793b5e7b06d70ac236/config/_base_/model/nextchat.py#L9) to your sam path.\n**Web Demo**\nPlease run:\n```shell\nCUDA_VISIBLE_DEVICES=\"0\" python mllm/demo/web_demo.py --model_path path/to/model_weights --vit_path path/to/openai-clip-vit-large-patch14-336\n```\nIf you can connect to huggingface, just run:\n```shell\nCUDA_VISIBLE_DEVICES=\"0\" python mllm/demo/web_demo.py --model_path AoZhang/nextchat-7b-336 --vit_path openai/clip-vit-large-patch14-336\n```\n**Bash Demo**\n```shell\nCUDA_VISIBLE_DEVICES=\"0\" python mllm/demo/bash_demo.py path/to/model_weights  path/to/openai-clip-vit-large-patch14-336"
        },
        {
            "comment": "This code provides instructions to run a NextChat model for generating text based on an input image. The user can choose between two methods: running the pre-existing models with the \"Easy Run\" option or initializing the model themselves. For the latter, the user needs to import the necessary classes and use them with the given model path, image path, and number of tokens. The \"Easy Run\" method utilizes the ModelScope library to run a specific task using pre-trained models.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/README.md\":107-133",
            "content": "```\nIf you can connect to huggingface, just run:\n```shell\nCUDA_VISIBLE_DEVICES=\"0\" python mllm/demo/bash_demo.py AoZhang/nextchat-7b-336  openai/clip-vit-large-patch14-336\n```\nYou can also initialize the model by yourself:\n```python\nfrom mllm.demo.demo_util import NextChatInference\nmodel = NextChatInference(model_weight_path, vit_path, 576)\n```\nYou will get into the IPython mode. Then use the model like:\n```python\ninput = {\"text\": \"What is the possible relationship between the two people? Please include object locations.\", \"image\": \"./COCO_val2014_000000222628.jpg\"}\nresponse, boxes, masks, ret_img = model(input)\n```\n## Easy Run\nWe have our old models (v0 versions) in the modelscope.\nPlease first install `pip install modelscope`.\nThen run:\n```python\nfrom modelscope import pipeline\npipe = pipeline('my-nextchat-task', 'ZhangAo6/nextchat', model_size=\"7b\") # 7b model takes around 21G GPU mem, 13b takes around 35G GPU mem\nresponse, ret_image = pipe({\"text\": \"xxxx?\", \"image\": \"xxx.jpg\"})\n# response: the text answer"
        },
        {
            "comment": "This code snippet is from the NExT-Chat project's README file. It explains that the result of the model evaluation has not been updated to arXiv and provides a link to view the results. The code also instructs users on how to configure the vision_tower, download SAM (Segment Anything Model), and modify sam_path in the nextchat.py file. Lastly, it mentions that the REC task is hard to beat top-tier pixel2seq models like Shikra in the pre-training setting.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/README.md\":134-156",
            "content": "# ret_image: image annotated with boxes and masks\n```\n## Evaluation\nThe final result have not been updated to the arxiv.\nWe show the results here:\n### Referring Expression Segmentation (RES)\n<p align=\"center\">\n  <img src=\"https://next-chatv.github.io/images/res.png\" alt=\"p1\">\n</p>\nPlease config the `vision_tower` in the [config/_base_/model/nextchat.py]([config/_base_/model/nextchat.py]) to the path of OpenAI CLIP, if you can not connect to huggingface.\nMake sure to download the [SAM](https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth) and modify `sam_path` in [config/_base_/model/nextchat.py](https://github.com/NExT-ChatV/NExT-Chat/blob/6e92d9b13b08e978190a00793b5e7b06d70ac236/config/_base_/model/nextchat.py#L9) to your sam path.\n```shell\nbash eval_res.sh /path/to/checkpoint\n```\n### Referring Expression Comprehension (REC)\nAlthough it seems to be better by modeling the localization as a regression task (also validated by toy experiments),\nwe find that pixel2emb now is **hard to beat top-tier pixel2seq models** on REC (like Shikra) in the pre-training setting."
        },
        {
            "comment": "This code snippet discusses balancing localization loss and LM loss for a REC performance, suggests configuring the vision_tower in nextchat.py file, downloading SAM, modifying sam_path, and running eval_rec.sh to evaluate the model using a checkpoint. It also introduces Pope (Image-level Hallucination) and encourages insights or discussions from users.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/README.md\":157-175",
            "content": "We guess the key factors might be to find a balance between the localization loss and LM loss, which will significantly affect the REC performance.\nWe are still working on this interesting finding and tune the model.\nIf you have some insights, welcome to discuss.\n<p align=\"center\">\n  <img src=\"https://next-chatv.github.io/images/rec.png\" alt=\"p1\">\n</p>\nPlease config the `vision_tower` in the [config/_base_/model/nextchat.py]([config/_base_/model/nextchat.py]) to the path of OpenAI CLIP, if you can not connect to huggingface.\nMake sure to download the [SAM](https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth) and modify `sam_path` in [config/_base_/model/nextchat.py](https://github.com/NExT-ChatV/NExT-Chat/blob/6e92d9b13b08e978190a00793b5e7b06d70ac236/config/_base_/model/nextchat.py#L9) to your sam path.\n```shell\nbash eval_rec.sh /path/to/checkpoint\n```\n### Pope (Image-level Hallucination)\n<p align=\"center\">\n  <img src=\"https://next-chatv.github.io/images/pope.png\" alt=\"p1\">\n</p>\nPlea"
        },
        {
            "comment": "This code is instructing the user to configure a `vision_tower` in `[config/_base_/model/nextchat.py]` using OpenAI CLIP and ensure that they download the SAM model from the provided link. The user should also modify the `sam_path` in the same configuration file. This is to be done before running a script named 'eval_pope.sh' with the desired checkpoint path as an argument. Additionally, there is a mention of RefCOCOg-google (Region Caption) and an image link.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/README.md\":175-188",
            "content": "se config the `vision_tower` in the [config/_base_/model/nextchat.py]([config/_base_/model/nextchat.py]) to the path of OpenAI CLIP, if you can not connect to huggingface.\nMake sure to download the [SAM](https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth) and modify `sam_path` in [config/_base_/model/nextchat.py](https://github.com/NExT-ChatV/NExT-Chat/blob/6e92d9b13b08e978190a00793b5e7b06d70ac236/config/_base_/model/nextchat.py#L9) to your sam path.\n```shell\nbash eval_pope.sh /path/to/checkpoint\n```\n### RefCOCOg-google (Region Caption)\n<p align=\"center\">\n  <img src=\"https://next-chatv.github.io/images/reg_cap.png\" alt=\"p1\">\n</p>\nPlease config the `vision_tower` in the [config/_base_/model/nextchat.py]([config/_base_/model/nextchat.py]) to the path of OpenAI CLIP, if you can not connect to huggingface.\nMake sure to download the [SAM](https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth) and modify `sam_path` in [config/_base_/model/nextchat.py](https://github.co"
        },
        {
            "comment": "Training consists of 3 stages: VL+Detection Pre-training, VL+Detection Instruction Following, and VL+Detection+Segmentation. Stage1 script is run_stage1.sh, Stage2 uses checkpoint from stage1 and runs with run_stage2.sh, and Stage3 uses the model from stage2 with run_stage3.sh.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/README.md\":188-210",
            "content": "m/NExT-ChatV/NExT-Chat/blob/6e92d9b13b08e978190a00793b5e7b06d70ac236/config/_base_/model/nextchat.py#L9) to your sam path.\n```shell\nbash eval_reg_cap.sh /path/to/checkpoint\n```\n## Training\nPlease config the `vision_tower` in the [config/_base_/model/nextchat.py]([config/_base_/model/nextchat.py]) to the path of OpenAI CLIP, if you can not connect to huggingface.\nMake sure to download the [SAM](https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth) and modify `sam_path` in [config/_base_/model/nextchat.py](https://github.com/NExT-ChatV/NExT-Chat/blob/6e92d9b13b08e978190a00793b5e7b06d70ac236/config/_base_/model/nextchat.py#L9) to your sam path.\nOur training consists of 3 stages:\n1. VL+Detection Pre-training\n```shell\nbash run_stage1.sh\n```\n2. VL+Detection Instruction Following\n```shell\nbash run_stage2.sh output/stage1/checkpoint-65000 # or other path to your stage1 model, we use 65000 for stage2\n```\n3. VL+Detection+Segmentation\n```shell\nbash run_stage3.sh output/stage2/checkpoint-4950 # or other path to your stage2 model"
        },
        {
            "comment": "Examples: Images generated by the nextchat-13b-v0 models, showcasing their capabilities.\nAcknowledgement: Grateful to similar projects for inspiration and code contribution.\nBibtex: Provides citation details for the NExT-Chat paper in the arXiv database.",
            "location": "\"/media/root/Prima/works/NExT-Chat/docs/src/README.md\":211-242",
            "content": "```\n## Examples\nExamples generated by our nextchat-13b-v0 models:\n<p align=\"center\">\n  <img src=\"https://next-chatv.github.io/demos/p1.png\" alt=\"p1\">\n</p>\n<p align=\"center\">\n  <img src=\"https://next-chatv.github.io/demos/p2.png\" alt=\"p2\">\n</p>\n<p align=\"center\">\n  <img src=\"https://next-chatv.github.io/demos/p3.png\" alt=\"p3\">\n</p>\n<p align=\"center\">\n  <img src=\"https://next-chatv.github.io/demos/p4.png\" alt=\"p4\">\n</p>\n## Acknowledgement\nThanks to [Shikra](https://github.com/shikras/shikra), [LLaVA](https://github.com/haotian-liu/LLaVA), [CogVLM](https://github.com/THUDM/CogVLM) for their excellent codes.\nOur bibtex:\n```bibtex\n@misc{zhang2023nextchat,\n      title={NExT-Chat: An LMM for Chat, Detection and Segmentation}, \n      author={Ao Zhang and Yuan Yao and Wei Ji and Zhiyuan Liu and Tat-Seng Chua},\n      year={2023},\n      eprint={2311.04498},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```"
        }
    ]
}