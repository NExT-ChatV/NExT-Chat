{
    "600": {
        "file_id": 50,
        "content": "        norm_layer: Type[nn.Module] = nn.LayerNorm,\n        act_layer: Type[nn.Module] = nn.GELU,\n        use_abs_pos: bool = True,\n        use_rel_pos: bool = False,\n        rel_pos_zero_init: bool = True,\n        window_size: int = 0,\n        global_attn_indexes: Tuple[int, ...] = (),\n    ) -> None:\n        \"\"\"\n        Args:\n            img_size (int): Input image size.\n            patch_size (int): Patch size.\n            in_chans (int): Number of input image channels.\n            embed_dim (int): Patch embedding dimension.\n            depth (int): Depth of ViT.\n            num_heads (int): Number of attention heads in each ViT block.\n            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n            norm_layer (nn.Module): Normalization layer.\n            act_layer (nn.Module): Activation layer.\n            use_abs_pos (bool): If True, use absolute positional embeddings.\n            use_rel_pos (bool): If True, add relative positional embeddings to the attention map.",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:307-328"
    },
    "601": {
        "file_id": 50,
        "content": "This function initializes the parameters for a vision transformer model, including input image size, patch size, number of input channels, embedding dimension, depth of ViT, number of attention heads, and more. It also defines the normalization layer as LayerNorm and activation layer as GELU.",
        "type": "comment"
    },
    "602": {
        "file_id": 50,
        "content": "            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n            window_size (int): Window size for window attention blocks.\n            global_attn_indexes (list): Indexes for blocks using global attention.\n        \"\"\"\n        super().__init__()\n        self.img_size = img_size\n        self.patch_embed = PatchEmbed(\n            kernel_size=(patch_size, patch_size),\n            stride=(patch_size, patch_size),\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n        )\n        self.pos_embed: Optional[nn.Parameter] = None\n        if use_abs_pos:\n            # Initialize absolute positional embedding with pretrain image size.\n            self.pos_embed = nn.Parameter(\n                torch.zeros(1, img_size // patch_size, img_size // patch_size, embed_dim)\n            )\n        self.blocks = nn.ModuleList()\n        for i in range(depth):\n            block = Block(\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:329-355"
    },
    "603": {
        "file_id": 50,
        "content": "The code initializes a model with specified parameters, such as image size, patch size, embedding dimensions, and block depth. It creates an optional absolute positional embedding and stores each block in a module list for later use.",
        "type": "comment"
    },
    "604": {
        "file_id": 50,
        "content": "                qkv_bias=qkv_bias,\n                norm_layer=norm_layer,\n                act_layer=act_layer,\n                use_rel_pos=use_rel_pos,\n                rel_pos_zero_init=rel_pos_zero_init,\n                window_size=window_size if i not in global_attn_indexes else 0,\n                input_size=(img_size // patch_size, img_size // patch_size),\n            )\n            self.blocks.append(block)\n        self.neck = nn.Sequential(\n            nn.Conv2d(\n                embed_dim,\n                out_chans,\n                kernel_size=1,\n                bias=False,\n            ),\n            LayerNorm2d(out_chans),\n            nn.Conv2d(\n                out_chans,\n                out_chans,\n                kernel_size=3,\n                padding=1,\n                bias=False,\n            ),\n            LayerNorm2d(out_chans),\n        )\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.patch_embed(x)\n        if self.pos_embed is not None:\n            x = x + self.pos_embed\n        for blk in self.blocks:",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:356-389"
    },
    "605": {
        "file_id": 50,
        "content": "This code initializes a Vision Transformer model with patch embedding, multiple attention blocks, and a neck. The blocks are created with specified parameters including patch size, embedding dimension, number of heads, and window size. The neck consists of convolutional layers for feature extraction and normalization.",
        "type": "comment"
    },
    "606": {
        "file_id": 50,
        "content": "            x = blk(x)\n        x = self.neck(x.permute(0, 3, 1, 2))\n        return x\nclass Block(nn.Module):\n    \"\"\"Transformer blocks with support of window attention and residual propagation blocks\"\"\"\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = True,\n        norm_layer: Type[nn.Module] = nn.LayerNorm,\n        act_layer: Type[nn.Module] = nn.GELU,\n        use_rel_pos: bool = False,\n        rel_pos_zero_init: bool = True,\n        window_size: int = 0,\n        input_size: Optional[Tuple[int, int]] = None,\n    ) -> None:\n        \"\"\"\n        Args:\n            dim (int): Number of input channels.\n            num_heads (int): Number of attention heads in each ViT block.\n            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n            norm_layer (nn.Module): Normalization layer.\n            act_layer (nn.Module): Activation layer.\n            use_rel_pos (bool): If True, add relative positional embeddings to the attention map.",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:390-421"
    },
    "607": {
        "file_id": 50,
        "content": "This code defines a transformer block class that supports window attention and residual propagation blocks. It takes in parameters such as the number of input channels, attention heads, mlp ratio, etc., and returns an instance of the transformer block with optional features like relative position embeddings and window-based attention.",
        "type": "comment"
    },
    "608": {
        "file_id": 50,
        "content": "            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n            window_size (int): Window size for window attention blocks. If it equals 0, then\n                use global attention.\n            input_size (tuple(int, int) or None): Input resolution for calculating the relative\n                positional parameter size.\n        \"\"\"\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            use_rel_pos=use_rel_pos,\n            rel_pos_zero_init=rel_pos_zero_init,\n            input_size=input_size if window_size == 0 else (window_size, window_size),\n        )\n        self.norm2 = norm_layer(dim)\n        self.mlp = MLPBlock(embedding_dim=dim, mlp_dim=int(dim * mlp_ratio), act=act_layer)\n        self.window_size = window_size\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        shortcut = x\n        x = self.norm1(x)\n        # Window partition",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:422-447"
    },
    "609": {
        "file_id": 50,
        "content": "This code initializes a new class that extends the `nn.Module` and contains an attention block, normalization layers, and MLP block. The attention block uses relative positional parameters if specified, and partitions input into windows of size determined by `window_size`. The shortcut variable holds the input for use in residual connections during forward pass.",
        "type": "comment"
    },
    "610": {
        "file_id": 50,
        "content": "        if self.window_size > 0:\n            H, W = x.shape[1], x.shape[2]\n            x, pad_hw = window_partition(x, self.window_size)\n        x = self.attn(x)\n        # Reverse window partition\n        if self.window_size > 0:\n            x = window_unpartition(x, self.window_size, pad_hw, (H, W))\n        x = shortcut + x\n        x = x + self.mlp(self.norm2(x))\n        return x\nclass Attention(nn.Module):\n    \"\"\"Multi-head Attention block with relative position embeddings.\"\"\"\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 8,\n        qkv_bias: bool = True,\n        use_rel_pos: bool = False,\n        rel_pos_zero_init: bool = True,\n        input_size: Optional[Tuple[int, int]] = None,\n    ) -> None:\n        \"\"\"\n        Args:\n            dim (int): Number of input channels.\n            num_heads (int): Number of attention heads.\n            qkv_bias (bool):  If True, add a learnable bias to query, key, value.\n            rel_pos (bool): If True, add relative positional embeddings to the attention map.",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:448-480"
    },
    "611": {
        "file_id": 50,
        "content": "This code defines a multi-head attention block with relative position embeddings. It takes an input tensor and applies window partitioning for the attention mechanism if the window size is greater than 0. It then performs multi-layer perceptron (MLP) processing, adds shortcut connection, and returns the resulting output. The Attention class initializes the attention block with specified dimensions, number of heads, and other parameters.",
        "type": "comment"
    },
    "612": {
        "file_id": 50,
        "content": "            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n            input_size (tuple(int, int) or None): Input resolution for calculating the relative\n                positional parameter size.\n        \"\"\"\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim**-0.5\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim)\n        self.use_rel_pos = use_rel_pos\n        if self.use_rel_pos:\n            assert (\n                input_size is not None\n            ), \"Input size must be provided if using relative positional encoding.\"\n            # initialize relative positional embeddings\n            self.rel_pos_h = nn.Parameter(torch.zeros(2 * input_size[0] - 1, head_dim))\n            self.rel_pos_w = nn.Parameter(torch.zeros(2 * input_size[1] - 1, head_dim))\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        B, H, W, _ = x.shape\n        # qkv with shape (3, B, nHead, H * W, C)",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:481-504"
    },
    "613": {
        "file_id": 50,
        "content": "This function initializes the Multi-Head Attention layer for a Transformer model. It takes in parameters such as num_heads, dim, qkv_bias, and input_size. The function also includes the use_rel_pos parameter to determine if relative positional encoding should be used. If so, it initializes the relative positional embeddings (rel_pos_h and rel_pos_w) with zero values. The forward function performs the multi-head attention operation on the input tensor x.",
        "type": "comment"
    },
    "614": {
        "file_id": 50,
        "content": "        qkv = self.qkv(x).reshape(B, H * W, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n        # q, k, v with shape (B * nHead, H * W, C)\n        q, k, v = qkv.reshape(3, B * self.num_heads, H * W, -1).unbind(0)\n        attn = (q * self.scale) @ k.transpose(-2, -1)\n        if self.use_rel_pos:\n            attn = add_decomposed_rel_pos(attn, q, self.rel_pos_h, self.rel_pos_w, (H, W), (H, W))\n        attn = attn.softmax(dim=-1)\n        x = (attn @ v).view(B, self.num_heads, H, W, -1).permute(0, 2, 3, 1, 4).reshape(B, H, W, -1)\n        x = self.proj(x)\n        return x\ndef window_partition(x: torch.Tensor, window_size: int) -> Tuple[torch.Tensor, Tuple[int, int]]:\n    \"\"\"\n    Partition into non-overlapping windows with padding if needed.\n    Args:\n        x (tensor): input tokens with [B, H, W, C].\n        window_size (int): window size.\n    Returns:\n        windows: windows after partition with [B * num_windows, window_size, window_size, C].\n        (Hp, Wp): padded height and width before partition\n    \"\"\"",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:505-531"
    },
    "615": {
        "file_id": 50,
        "content": "This code defines a function for partitioning input tokens into non-overlapping windows with padding if needed. It first computes the query, key, and value matrices from input features. Then, it performs attention calculation using scaled dot product attention and optionally adds relative position information. Finally, it applies a projection to the output before returning it.",
        "type": "comment"
    },
    "616": {
        "file_id": 50,
        "content": "    B, H, W, C = x.shape\n    pad_h = (window_size - H % window_size) % window_size\n    pad_w = (window_size - W % window_size) % window_size\n    if pad_h > 0 or pad_w > 0:\n        x = F.pad(x, (0, 0, 0, pad_w, 0, pad_h))\n    Hp, Wp = H + pad_h, W + pad_w\n    x = x.view(B, Hp // window_size, window_size, Wp // window_size, window_size, C)\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n    return windows, (Hp, Wp)\ndef window_unpartition(\n    windows: torch.Tensor, window_size: int, pad_hw: Tuple[int, int], hw: Tuple[int, int]\n) -> torch.Tensor:\n    \"\"\"\n    Window unpartition into original sequences and removing padding.\n    Args:\n        windows (tensor): input tokens with [B * num_windows, window_size, window_size, C].\n        window_size (int): window size.\n        pad_hw (Tuple): padded height and width (Hp, Wp).\n        hw (Tuple): original height and width (H, W) before padding.\n    Returns:\n        x: unpartitioned sequences with [B, H, W, C].\n    \"\"\"\n    Hp, Wp = pad_hw",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:532-559"
    },
    "617": {
        "file_id": 50,
        "content": "This function takes in a tensor `x` and reshapes it into windows of size `window_size` for each sequence. It also adds padding to the height and width of the input tensor, ensuring that the window sizes are consistent across sequences. The function returns the reshaped windows along with the original unpadded height and width of the input tensor.",
        "type": "comment"
    },
    "618": {
        "file_id": 50,
        "content": "    H, W = hw\n    B = windows.shape[0] // (Hp * Wp // window_size // window_size)\n    x = windows.view(B, Hp // window_size, Wp // window_size, window_size, window_size, -1)\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, Hp, Wp, -1)\n    if Hp > H or Wp > W:\n        x = x[:, :H, :W, :].contiguous()\n    return x\ndef get_rel_pos(q_size: int, k_size: int, rel_pos: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Get relative positional embeddings according to the relative positions of\n        query and key sizes.\n    Args:\n        q_size (int): size of query q.\n        k_size (int): size of key k.\n        rel_pos (Tensor): relative position embeddings (L, C).\n    Returns:\n        Extracted positional embeddings according to relative positions.\n    \"\"\"\n    max_rel_dist = int(2 * max(q_size, k_size) - 1)\n    # Interpolate rel pos if needed.\n    if rel_pos.shape[0] != max_rel_dist:\n        # Interpolate rel pos.\n        rel_pos_resized = F.interpolate(\n            rel_pos.reshape(1, rel_pos.shape[0], -1).permute(0, 2, 1),",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:560-587"
    },
    "619": {
        "file_id": 50,
        "content": "The code defines functions for handling windows and positional embeddings in a model. The first function, `get_windows`, resizes the input, calculates the window size, and rearranges the dimensions of the tensor. The second function, `get_rel_pos`, extracts relative positional embeddings based on query and key sizes. It also interpolates the relative position embeddings if necessary.",
        "type": "comment"
    },
    "620": {
        "file_id": 50,
        "content": "            size=max_rel_dist,\n            mode=\"linear\",\n        )\n        rel_pos_resized = rel_pos_resized.reshape(-1, max_rel_dist).permute(1, 0)\n    else:\n        rel_pos_resized = rel_pos\n    # Scale the coords with short length if shapes for q and k are different.\n    q_coords = torch.arange(q_size)[:, None] * max(k_size / q_size, 1.0)\n    k_coords = torch.arange(k_size)[None, :] * max(q_size / k_size, 1.0)\n    relative_coords = (q_coords - k_coords) + (k_size - 1) * max(q_size / k_size, 1.0)\n    return rel_pos_resized[relative_coords.long()]\ndef add_decomposed_rel_pos(\n    attn: torch.Tensor,\n    q: torch.Tensor,\n    rel_pos_h: torch.Tensor,\n    rel_pos_w: torch.Tensor,\n    q_size: Tuple[int, int],\n    k_size: Tuple[int, int],\n) -> torch.Tensor:\n    \"\"\"\n    Calculate decomposed Relative Positional Embeddings from :paper:`mvitv2`.\n    https://github.com/facebookresearch/mvit/blob/19786631e330df9f3622e5402b4a419a263a2c80/mvit/models/attention.py   # noqa B950\n    Args:\n        attn (Tensor): attention map.",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:588-615"
    },
    "621": {
        "file_id": 50,
        "content": "This function calculates decomposed Relative Positional Embeddings based on the attention map, query tensor, and relative positioning information for height and width. It resizes and scales the relative positions according to the shapes of the query (q) and key (k) tensors, then returns the resized embeddings.",
        "type": "comment"
    },
    "622": {
        "file_id": 50,
        "content": "        q (Tensor): query q in the attention layer with shape (B, q_h * q_w, C).\n        rel_pos_h (Tensor): relative position embeddings (Lh, C) for height axis.\n        rel_pos_w (Tensor): relative position embeddings (Lw, C) for width axis.\n        q_size (Tuple): spatial sequence size of query q with (q_h, q_w).\n        k_size (Tuple): spatial sequence size of key k with (k_h, k_w).\n    Returns:\n        attn (Tensor): attention map with added relative positional embeddings.\n    \"\"\"\n    q_h, q_w = q_size\n    k_h, k_w = k_size\n    Rh = get_rel_pos(q_h, k_h, rel_pos_h)\n    Rw = get_rel_pos(q_w, k_w, rel_pos_w)\n    B, _, dim = q.shape\n    r_q = q.reshape(B, q_h, q_w, dim)\n    rel_h = torch.einsum(\"bhwc,hkc->bhwk\", r_q, Rh)\n    rel_w = torch.einsum(\"bhwc,wkc->bhwk\", r_q, Rw)\n    attn = (\n        attn.view(B, q_h, q_w, k_h, k_w) + rel_h[:, :, :, :, None] + rel_w[:, :, :, None, :]\n    ).view(B, q_h * q_w, k_h * k_w)\n    return attn\nclass PatchEmbed(nn.Module):\n    \"\"\"\n    Image to Patch Embedding.\n    \"\"\"\n    def __init__(",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:616-647"
    },
    "623": {
        "file_id": 50,
        "content": "This code is a part of the Self-Attention mechanism in the SAM (Spatio-Temporal Attentive Memory) model. The function calculates and adds relative positional embeddings to the attention map, considering both height and width axes. The PatchEmbed class performs image embedding by flattening image patches into a 2D feature grid for further processing.",
        "type": "comment"
    },
    "624": {
        "file_id": 50,
        "content": "        self,\n        kernel_size: Tuple[int, int] = (16, 16),\n        stride: Tuple[int, int] = (16, 16),\n        padding: Tuple[int, int] = (0, 0),\n        in_chans: int = 3,\n        embed_dim: int = 768,\n    ) -> None:\n        \"\"\"\n        Args:\n            kernel_size (Tuple): kernel size of the projection layer.\n            stride (Tuple): stride of the projection layer.\n            padding (Tuple): padding size of the projection layer.\n            in_chans (int): Number of input image channels.\n            embed_dim (int): Patch embedding dimension.\n        \"\"\"\n        super().__init__()\n        self.proj = nn.Conv2d(\n            in_chans, embed_dim, kernel_size=kernel_size, stride=stride, padding=padding\n        )\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.proj(x)\n        # B C H W -> B H W C\n        x = x.permute(0, 2, 3, 1)\n        return x\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n# This source code is licensed under the license found in the",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:648-680"
    },
    "625": {
        "file_id": 50,
        "content": "This class initializes a Conv2d layer for patch embedding with specified kernel size, stride, padding, input channels, and embedding dimension. The forward pass simply applies the initialized convolution and reorders the dimensions of the output tensor.",
        "type": "comment"
    },
    "626": {
        "file_id": 50,
        "content": "# LICENSE file in the root directory of this source tree.\nclass MaskDecoder(nn.Module):\n    def __init__(\n        self,\n        *,\n        transformer_dim: int,\n        transformer: nn.Module,\n        num_multimask_outputs: int = 3,\n        activation: Type[nn.Module] = nn.GELU,\n        iou_head_depth: int = 3,\n        iou_head_hidden_dim: int = 256,\n    ) -> None:\n        \"\"\"\n        Predicts masks given an image and prompt embeddings, using a\n        transformer architecture.\n        Arguments:\n          transformer_dim (int): the channel dimension of the transformer\n          transformer (nn.Module): the transformer used to predict masks\n          num_multimask_outputs (int): the number of masks to predict\n            when disambiguating masks\n          activation (nn.Module): the type of activation to use when\n            upscaling masks\n          iou_head_depth (int): the depth of the MLP used to predict\n            mask quality\n          iou_head_hidden_dim (int): the hidden dimension of the MLP\n            used to predict mask quality",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:681-709"
    },
    "627": {
        "file_id": 50,
        "content": "This code defines a MaskDecoder class for predicting masks given an image and prompt embeddings using a transformer architecture. It takes in parameters like transformer_dim, transformer, num_multimask_outputs, activation, iou_head_depth, and iou_head_hidden_dim.",
        "type": "comment"
    },
    "628": {
        "file_id": 50,
        "content": "        \"\"\"\n        super().__init__()\n        self.transformer_dim = transformer_dim\n        self.transformer = transformer\n        self.num_multimask_outputs = num_multimask_outputs\n        self.iou_token = nn.Embedding(1, transformer_dim)\n        self.num_mask_tokens = num_multimask_outputs + 1\n        self.mask_tokens = nn.Embedding(self.num_mask_tokens, transformer_dim)\n        self.output_upscaling = nn.Sequential(\n            nn.ConvTranspose2d(transformer_dim, transformer_dim // 4, kernel_size=2, stride=2),\n            LayerNorm2d(transformer_dim // 4),\n            activation(),\n            nn.ConvTranspose2d(transformer_dim // 4, transformer_dim // 8, kernel_size=2, stride=2),\n            activation(),\n        )\n        self.output_hypernetworks_mlps = nn.ModuleList(\n            [\n                MLP(transformer_dim, transformer_dim, transformer_dim // 8, 3)\n                for i in range(self.num_mask_tokens)\n            ]\n        )\n        self.iou_prediction_head = MLP(\n            transformer_dim, iou_head_hidden_dim, self.num_mask_tokens, iou_head_depth",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:710-736"
    },
    "629": {
        "file_id": 50,
        "content": "This code defines a model for image segmentation, consisting of a transformer network and output layers. The model takes in an input image and outputs a masked version with the ability to generate multiple masks. It uses an IOU token embedding, mask tokens embedding, output upscaling convolutions, hypernetworks MLPs, and an IOU prediction head.",
        "type": "comment"
    },
    "630": {
        "file_id": 50,
        "content": "        )\n    def forward(\n        self,\n        image_embeddings: torch.Tensor,\n        image_pe: torch.Tensor,\n        sparse_prompt_embeddings: torch.Tensor,\n        dense_prompt_embeddings: torch.Tensor,\n        multimask_output: bool,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Predict masks given image and prompt embeddings.\n        Arguments:\n          image_embeddings (torch.Tensor): the embeddings from the image encoder\n          image_pe (torch.Tensor): positional encoding with the shape of image_embeddings\n          sparse_prompt_embeddings (torch.Tensor): the embeddings of the points and boxes\n          dense_prompt_embeddings (torch.Tensor): the embeddings of the mask inputs\n          multimask_output (bool): Whether to return multiple masks or a single\n            mask.\n        Returns:\n          torch.Tensor: batched predicted masks\n          torch.Tensor: batched predictions of mask quality\n        \"\"\"\n        masks, iou_pred = self.predict_masks(\n            image_embeddings=image_embeddings,",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:737-763"
    },
    "631": {
        "file_id": 50,
        "content": "This function takes image embeddings, image positional encoding, sparse prompt embeddings, dense prompt embeddings, and a boolean indicating whether to return multiple or single masks as input. It returns predicted masks and mask quality scores for each input case.",
        "type": "comment"
    },
    "632": {
        "file_id": 50,
        "content": "            image_pe=image_pe,\n            sparse_prompt_embeddings=sparse_prompt_embeddings,\n            dense_prompt_embeddings=dense_prompt_embeddings,\n        )\n        # Select the correct mask or masks for output\n        if multimask_output:\n            mask_slice = slice(1, None)\n        else:\n            mask_slice = slice(0, 1)\n        masks = masks[:, mask_slice, :, :]\n        iou_pred = iou_pred[:, mask_slice]\n        # Prepare output\n        return masks, iou_pred\n    def predict_masks(\n        self,\n        image_embeddings: torch.Tensor,\n        image_pe: torch.Tensor,\n        sparse_prompt_embeddings: torch.Tensor,\n        dense_prompt_embeddings: torch.Tensor,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Predicts masks. See 'forward' for more details.\"\"\"\n        # Concatenate output tokens\n        output_tokens = torch.cat([self.iou_token.weight, self.mask_tokens.weight], dim=0)\n        output_tokens = output_tokens.unsqueeze(0).expand(sparse_prompt_embeddings.size(0), -1, -1)\n        tokens = torch.cat((output_tokens, sparse_prompt_embeddings), dim=1)",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:764-791"
    },
    "633": {
        "file_id": 50,
        "content": "This code snippet defines a model for predicting masks based on image embeddings, image position encoding (image_pe), sparse prompt embeddings, and dense prompt embeddings. It uses IOU token and mask tokens to concatenate output tokens and then expands them accordingly. The function returns the predicted masks and intersection over union (IOU) predictions.",
        "type": "comment"
    },
    "634": {
        "file_id": 50,
        "content": "        # Expand per-image data in batch direction to be per-mask\n        if image_embeddings.shape[0] != tokens.shape[0]:\n            src = torch.repeat_interleave(image_embeddings, tokens.shape[0], dim=0)\n        else:\n            src = image_embeddings\n        src = src + dense_prompt_embeddings\n        pos_src = torch.repeat_interleave(image_pe, tokens.shape[0], dim=0)\n        b, c, h, w = src.shape\n        # Run the transformer\n        hs, src = self.transformer(src, pos_src, tokens)\n        iou_token_out = hs[:, 0, :]\n        mask_tokens_out = hs[:, 1 : (1 + self.num_mask_tokens), :]\n        # Upscale mask embeddings and predict masks using the mask tokens\n        src = src.transpose(1, 2).view(b, c, h, w)\n        upscaled_embedding = self.output_upscaling(src)\n        hyper_in_list: List[torch.Tensor] = []\n        for i in range(self.num_mask_tokens):\n            hyper_in_list.append(self.output_hypernetworks_mlps[i](mask_tokens_out[:, i, :]))\n        hyper_in = torch.stack(hyper_in_list, dim=1)\n        b, c, h, w = upscaled_embedding.shape",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:793-814"
    },
    "635": {
        "file_id": 50,
        "content": "This code is performing mask token generation and mask upscaling using a transformer model. It first expands image embeddings to match the number of tokens, then concatenates them with dense prompt embeddings. The transformed input is passed through the transformer to generate mask tokens and iou values. Finally, the image embeddings are upscaled and used in combination with mask tokens to produce masks using a hypernetwork.",
        "type": "comment"
    },
    "636": {
        "file_id": 50,
        "content": "        masks = (hyper_in @ upscaled_embedding.view(b, c, h * w)).view(b, -1, h, w)\n        # Generate mask quality predictions\n        iou_pred = self.iou_prediction_head(iou_token_out)\n        return masks, iou_pred\n# Lightly adapted from\n# https://github.com/facebookresearch/MaskFormer/blob/main/mask_former/modeling/transformer/transformer_predictor.py # noqa\nclass MLP(nn.Module):\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int,\n        output_dim: int,\n        num_layers: int,\n        sigmoid_output: bool = False,\n    ) -> None:\n        super().__init__()\n        self.num_layers = num_layers\n        h = [hidden_dim] * (num_layers - 1)\n        self.layers = nn.ModuleList(\n            nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim])\n        )\n        self.sigmoid_output = sigmoid_output\n    def forward(self, x):\n        for i, layer in enumerate(self.layers):\n            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n        if self.sigmoid_output:",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:815-845"
    },
    "637": {
        "file_id": 50,
        "content": "This code defines a Multi-Layer Perceptron (MLP) with optional sigmoid output. It takes an input dimension, hidden dimension, output dimension, and number of layers as parameters. The MLP consists of a series of linear layers with ReLU activation functions between them, followed by a final layer without activation. If sigmoid_output is set to True, the output is passed through a sigmoid function. This code is adapted from the MaskFormer model's TransformerPredictor class.",
        "type": "comment"
    },
    "638": {
        "file_id": 50,
        "content": "            x = F.sigmoid(x)\n        return x\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nclass PromptEncoder(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int,\n        image_embedding_size: Tuple[int, int],\n        input_image_size: Tuple[int, int],\n        mask_in_chans: int,\n        activation: Type[nn.Module] = nn.GELU,\n    ) -> None:\n        \"\"\"\n        Encodes prompts for input to SAM's mask decoder.\n        Arguments:\n          embed_dim (int): The prompts' embedding dimension\n          image_embedding_size (tuple(int, int)): The spatial size of the\n            image embedding, as (H, W).\n          input_image_size (int): The padded size of the image as input\n            to the image encoder, as (H, W).\n          mask_in_chans (int): The number of hidden channels used for\n            encoding input masks.\n          activation (nn.Module): The activation to use when encoding",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:846-879"
    },
    "639": {
        "file_id": 50,
        "content": "This code defines the PromptEncoder class, which encodes prompts for input to SAM's mask decoder. It takes arguments such as embed_dim (embedding dimension), image_embedding_size, input_image_size, and mask_in_chans. The activation function can also be specified.",
        "type": "comment"
    },
    "640": {
        "file_id": 50,
        "content": "            input masks.\n        \"\"\"\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.input_image_size = input_image_size\n        self.image_embedding_size = image_embedding_size\n        self.pe_layer = PositionEmbeddingRandom(embed_dim // 2)\n        self.num_point_embeddings: int = 4  # pos/neg point + 2 box corners\n        point_embeddings = [nn.Embedding(1, embed_dim) for i in range(self.num_point_embeddings)]\n        self.point_embeddings = nn.ModuleList(point_embeddings)\n        self.not_a_point_embed = nn.Embedding(1, embed_dim)\n        self.mask_input_size = (4 * image_embedding_size[0], 4 * image_embedding_size[1])\n        self.mask_downscaling = nn.Sequential(\n            nn.Conv2d(1, mask_in_chans // 4, kernel_size=2, stride=2),\n            LayerNorm2d(mask_in_chans // 4),\n            activation(),\n            nn.Conv2d(mask_in_chans // 4, mask_in_chans, kernel_size=2, stride=2),\n            LayerNorm2d(mask_in_chans),\n            activation(),\n            nn.Conv2d(mask_in_chans, embed_dim, kernel_size=1),",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:880-901"
    },
    "641": {
        "file_id": 50,
        "content": "The code initializes an object with specified dimensions for embedding, input image size, and image embedding. It creates a position embedding layer and a list of point embeddings, followed by a not-a-point embedding. The mask input size is also defined. Finally, it sets up a series of convolutional layers to scale the mask inputs to the appropriate size.",
        "type": "comment"
    },
    "642": {
        "file_id": 50,
        "content": "        )\n        self.no_mask_embed = nn.Embedding(1, embed_dim)\n    def get_dense_pe(self) -> torch.Tensor:\n        \"\"\"\n        Returns the positional encoding used to encode point prompts,\n        applied to a dense set of points the shape of the image encoding.\n        Returns:\n          torch.Tensor: Positional encoding with shape\n            1x(embed_dim)x(embedding_h)x(embedding_w)\n        \"\"\"\n        return self.pe_layer(self.image_embedding_size).unsqueeze(0)\n    def _embed_points(\n        self,\n        points: torch.Tensor,\n        labels: torch.Tensor,\n        pad: bool,\n    ) -> torch.Tensor:\n        \"\"\"Embeds point prompts.\"\"\"\n        points = points + 0.5  # Shift to center of pixel\n        if pad:\n            padding_point = torch.zeros((points.shape[0], 1, 2), device=points.device)\n            padding_label = -torch.ones((labels.shape[0], 1), device=labels.device)\n            points = torch.cat([points, padding_point], dim=1)\n            labels = torch.cat([labels, padding_label], dim=1)\n        point_embedding = self.pe_layer.forward_with_coords(points, self.input_image_size)",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:902-929"
    },
    "643": {
        "file_id": 50,
        "content": "This code defines a model for embedding point prompts in an image encoding. It includes functions to generate positional encodings, embed points, and handle padding if necessary. The model uses an embedding layer, positional encoding layer (pe_layer), and input image size for embedding the points.",
        "type": "comment"
    },
    "644": {
        "file_id": 50,
        "content": "        point_embedding[labels == -1] = 0.0\n        point_embedding[labels == -1] += self.not_a_point_embed.weight\n        point_embedding[labels == 0] += self.point_embeddings[0].weight\n        point_embedding[labels == 1] += self.point_embeddings[1].weight\n        return point_embedding\n    def _embed_boxes(self, boxes: torch.Tensor) -> torch.Tensor:\n        \"\"\"Embeds box prompts.\"\"\"\n        boxes = boxes + 0.5  # Shift to center of pixel\n        coords = boxes.reshape(-1, 2, 2)\n        corner_embedding = self.pe_layer.forward_with_coords(coords, self.input_image_size)\n        corner_embedding[:, 0, :] += self.point_embeddings[2].weight\n        corner_embedding[:, 1, :] += self.point_embeddings[3].weight\n        return corner_embedding\n    def _embed_masks(self, masks: torch.Tensor) -> torch.Tensor:\n        \"\"\"Embeds mask inputs.\"\"\"\n        mask_embedding = self.mask_downscaling(masks)\n        return mask_embedding\n    def _get_batch_size(\n        self,\n        points: Optional[Tuple[torch.Tensor, torch.Tensor]],",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:930-952"
    },
    "645": {
        "file_id": 50,
        "content": "This code is for a model that embeds points, boxes, and masks as input features. The `_embed_points` function assigns default embeddings to labels -1, 0, and 1, then adds additional embeddings if necessary. The `_embed_boxes` function shifts box prompts to the center of pixels, applies a position encoding layer, and adds specific embeddings for each corner. The `_embed_masks` function downscales mask inputs before returning their embedding. The `_get_batch_size` function retrieves the batch size from a tuple of optional points.",
        "type": "comment"
    },
    "646": {
        "file_id": 50,
        "content": "        boxes: Optional[torch.Tensor],\n        masks: Optional[torch.Tensor],\n    ) -> int:\n        \"\"\"\n        Gets the batch size of the output given the batch size of the input prompts.\n        \"\"\"\n        if points is not None:\n            return points[0].shape[0]\n        elif boxes is not None:\n            return boxes.shape[0]\n        elif masks is not None:\n            return masks.shape[0]\n        else:\n            return 1\n    def _get_device(self) -> torch.device:\n        return self.point_embeddings[0].weight.device\n    def forward(\n        self,\n        points: Optional[Tuple[torch.Tensor, torch.Tensor]],\n        boxes: Optional[torch.Tensor],\n        masks: Optional[torch.Tensor],\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Embeds different types of prompts, returning both sparse and dense\n        embeddings.\n        Arguments:\n          points (tuple(torch.Tensor, torch.Tensor) or none): point coordinates\n            and labels to embed.\n          boxes (torch.Tensor or none): boxes to embed",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:953-984"
    },
    "647": {
        "file_id": 50,
        "content": "The function returns the batch size based on the input type, sets the device for point embeddings, and performs embedding for points, boxes, or masks in a single forward pass.",
        "type": "comment"
    },
    "648": {
        "file_id": 50,
        "content": "          masks (torch.Tensor or none): masks to embed\n        Returns:\n          torch.Tensor: sparse embeddings for the points and boxes, with shape\n            BxNx(embed_dim), where N is determined by the number of input points\n            and boxes.\n          torch.Tensor: dense embeddings for the masks, in the shape\n            Bx(embed_dim)x(embed_H)x(embed_W)\n        \"\"\"\n        bs = self._get_batch_size(points, boxes, masks)\n        sparse_embeddings = torch.empty((bs, 0, self.embed_dim), device=self._get_device(),\n                                        dtype=self.no_mask_embed.weight.dtype)\n        if points is not None:\n            coords, labels = points\n            point_embeddings = self._embed_points(coords, labels, pad=(boxes is None))\n            sparse_embeddings = torch.cat([sparse_embeddings, point_embeddings], dim=1)\n        if boxes is not None:\n            box_embeddings = self._embed_boxes(boxes)\n            sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=1)",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:985-1003"
    },
    "649": {
        "file_id": 50,
        "content": "The function takes points, boxes, and masks as inputs and returns sparse embeddings for the points and boxes, and dense embeddings for the masks. It checks the input batch size and creates an empty tensor for storing the sparse embeddings. If points are provided, it embeds them and appends the result to the sparse_embeddings tensor. Similarly, if boxes are provided, they are embedded and appended to the same tensor. The function then returns the combined sparse_embeddings and dense masks.",
        "type": "comment"
    },
    "650": {
        "file_id": 50,
        "content": "        if masks is not None:\n            dense_embeddings = self._embed_masks(masks)\n        else:\n            dense_embeddings = self.no_mask_embed.weight.reshape(1, -1, 1, 1).expand(\n                bs, -1, self.image_embedding_size[0], self.image_embedding_size[1]\n            )\n        return sparse_embeddings, dense_embeddings\nclass PositionEmbeddingRandom(nn.Module):\n    \"\"\"\n    Positional encoding using random spatial frequencies.\n    \"\"\"\n    def __init__(self, num_pos_feats: int = 64, scale: Optional[float] = None) -> None:\n        super().__init__()\n        if scale is None or scale <= 0.0:\n            scale = 1.0\n        self.register_buffer(\n            \"positional_encoding_gaussian_matrix\",\n            scale * torch.randn((2, num_pos_feats)),\n        )\n    def _pe_encoding(self, coords: torch.Tensor) -> torch.Tensor:\n        \"\"\"Positionally encode points that are normalized to [0,1].\"\"\"\n        # assuming coords are in [0, 1]^2 square and have d_1 x ... x d_n x 2 shape\n        coords = coords.type(self.positional_encoding_gaussian_matrix.dtype)",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:1005-1032"
    },
    "651": {
        "file_id": 50,
        "content": "This code appears to be from a model called 'Sam'. It checks if there are any masks present, and if so, it embeds them. If no masks are present, it uses a default no-mask embedding. The code also defines a class 'PositionEmbeddingRandom' which utilizes random spatial frequencies for positional encoding.",
        "type": "comment"
    },
    "652": {
        "file_id": 50,
        "content": "        coords = 2 * coords - 1\n        coords = coords @ self.positional_encoding_gaussian_matrix\n        coords = 2 * np.pi * coords\n        # outputs d_1 x ... x d_n x C shape\n        return torch.cat([torch.sin(coords), torch.cos(coords)], dim=-1)\n    def forward(self, size: Tuple[int, int]) -> torch.Tensor:\n        \"\"\"Generate positional encoding for a grid of the specified size.\"\"\"\n        h, w = size\n        device: Any = self.positional_encoding_gaussian_matrix.device\n        grid = torch.ones((h, w), device=device, dtype=self.positional_encoding_gaussian_matrix.dtype)\n        y_embed = grid.cumsum(dim=0) - 0.5\n        x_embed = grid.cumsum(dim=1) - 0.5\n        y_embed = y_embed / h\n        x_embed = x_embed / w\n        pe = self._pe_encoding(torch.stack([x_embed, y_embed], dim=-1))\n        return pe.permute(2, 0, 1)  # C x H x W\n    def forward_with_coords(\n        self, coords_input: torch.Tensor, image_size: Tuple[int, int]\n    ) -> torch.Tensor:\n        \"\"\"Positionally encode points that are not normalized to [0,1].\"\"\"",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:1033-1055"
    },
    "653": {
        "file_id": 50,
        "content": "This code defines a class for positional encoding generation. The `forward` method generates a grid-based positional encoding of size (H, W) and returns it. The `forward_with_coords` method takes coordinate input and generates positional encoding based on those coordinates. The `_pe_encoding` method is not shown here but is responsible for generating the actual encoding matrix.",
        "type": "comment"
    },
    "654": {
        "file_id": 50,
        "content": "        coords = coords_input.clone()\n        coords[:, :, 0] = coords[:, :, 0] / image_size[1]\n        coords[:, :, 1] = coords[:, :, 1] / image_size[0]\n        return self._pe_encoding(coords.to(torch.float))  # B x N x C\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nclass Sam(nn.Module):\n    mask_threshold: float = 0.0\n    image_format: str = \"RGB\"\n    def __init__(\n        self,\n        image_encoder: ImageEncoderViT,\n        prompt_encoder: PromptEncoder,\n        mask_decoder: MaskDecoder,\n        pixel_mean: List[float] = [123.675, 116.28, 103.53],\n        pixel_std: List[float] = [58.395, 57.12, 57.375],\n    ) -> None:\n        \"\"\"\n        SAM predicts object masks from an image and input prompts.\n        Arguments:\n          image_encoder (ImageEncoderViT): The backbone used to encode the\n            image into image embeddings that allow for efficient mask prediction.",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:1056-1088"
    },
    "655": {
        "file_id": 50,
        "content": "This code snippet is part of the SAM (Spatio-Attentional Memory) model in the NExT-Chat/mllm repository. It defines a class called Sam and initializes its attributes like image_encoder, prompt_encoder, mask_decoder, pixel_mean, and pixel_std. The method _pe_encoding is used to encode coordinates in the image space which are then returned after being converted to float type. The SAM model predicts object masks from an image and input prompts.",
        "type": "comment"
    },
    "656": {
        "file_id": 50,
        "content": "          prompt_encoder (PromptEncoder): Encodes various types of input prompts.\n          mask_decoder (MaskDecoder): Predicts masks from the image embeddings\n            and encoded prompts.\n          pixel_mean (list(float)): Mean values for normalizing pixels in the input image.\n          pixel_std (list(float)): Std values for normalizing pixels in the input image.\n        \"\"\"\n        super().__init__()\n        self.image_encoder = image_encoder\n        self.prompt_encoder = prompt_encoder\n        self.mask_decoder = mask_decoder\n        self.register_buffer(\"pixel_mean\", torch.Tensor(pixel_mean).view(-1, 1, 1), False)\n        self.register_buffer(\"pixel_std\", torch.Tensor(pixel_std).view(-1, 1, 1), False)\n    @property\n    def device(self) -> Any:\n        return self.pixel_mean.device\n    @torch.no_grad()\n    def forward(\n        self,\n        batched_input: List[Dict[str, Any]],\n        multimask_output: bool,\n    ) -> List[Dict[str, torch.Tensor]]:\n        \"\"\"\n        Predicts masks end-to-end from provided images and prompts.",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:1089-1113"
    },
    "657": {
        "file_id": 50,
        "content": "This code initializes a class that takes in an image encoder, prompt encoder, and mask decoder as inputs. It also registers buffer for pixel mean and pixel std values. The device property returns the device of the pixel_mean tensor. The forward method predicts masks from images and prompts end-to-end.",
        "type": "comment"
    },
    "658": {
        "file_id": 50,
        "content": "        If prompts are not known in advance, using SamPredictor is\n        recommended over calling the model directly.\n        Arguments:\n          batched_input (list(dict)): A list over input images, each a\n            dictionary with the following keys. A prompt key can be\n            excluded if it is not present.\n              'image': The image as a torch tensor in 3xHxW format,\n                already transformed for input to the model.\n              'original_size': (tuple(int, int)) The original size of\n                the image before transformation, as (H, W).\n              'point_coords': (torch.Tensor) Batched point prompts for\n                this image, with shape BxNx2. Already transformed to the\n                input frame of the model.\n              'point_labels': (torch.Tensor) Batched labels for point prompts,\n                with shape BxN.\n              'boxes': (torch.Tensor) Batched box inputs, with shape Bx4.\n                Already transformed to the input frame of the model.",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:1114-1131"
    },
    "659": {
        "file_id": 50,
        "content": "This code describes the function signature for the SamPredictor model in the NExT-Chat codebase. It accepts a list of input dictionaries, each containing keys like 'image', 'original_size', 'point_coords', 'point_labels', and 'boxes'. These inputs are already transformed to match the input frame of the model, making it easier for users who don't know prompts in advance.",
        "type": "comment"
    },
    "660": {
        "file_id": 50,
        "content": "              'mask_inputs': (torch.Tensor) Batched mask inputs to the model,\n                in the form Bx1xHxW.\n          multimask_output (bool): Whether the model should predict multiple\n            disambiguating masks, or return a single mask.\n        Returns:\n          (list(dict)): A list over input images, where each element is\n            as dictionary with the following keys.\n              'masks': (torch.Tensor) Batched binary mask predictions,\n                with shape BxCxHxW, where B is the number of input prompts,\n                C is determined by multimask_output, and (H, W) is the\n                original size of the image.\n              'iou_predictions': (torch.Tensor) The model's predictions\n                of mask quality, in shape BxC.\n              'low_res_logits': (torch.Tensor) Low resolution logits with\n                shape BxCxHxW, where H=W=256. Can be passed as mask input\n                to subsequent iterations of prediction.\n        \"\"\"\n        input_images = torch.stack([self.preprocess(x[\"image\"]) for x in batched_input], dim=0)",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:1132-1150"
    },
    "661": {
        "file_id": 50,
        "content": "This function takes batched input images and preprocesses them using the model's 'preprocess' method. The result is a stack of processed input images along dimension 0, with shape Bx3xHxW where B is the batch size, H and W are the original image dimensions, and C=3 for RGB images. This function serves as part of a larger model that performs some type of mask prediction or disambiguation task on the processed input images.",
        "type": "comment"
    },
    "662": {
        "file_id": 50,
        "content": "        image_embeddings = self.image_encoder(input_images)\n        outputs = []\n        for image_record, curr_embedding in zip(batched_input, image_embeddings):\n            if \"point_coords\" in image_record:\n                points = (image_record[\"point_coords\"], image_record[\"point_labels\"])\n            else:\n                points = None\n            sparse_embeddings, dense_embeddings = self.prompt_encoder(\n                points=points,\n                boxes=image_record.get(\"boxes\", None),\n                masks=image_record.get(\"mask_inputs\", None),\n            )\n            low_res_masks, iou_predictions = self.mask_decoder(\n                image_embeddings=curr_embedding.unsqueeze(0),\n                image_pe=self.prompt_encoder.get_dense_pe(),\n                sparse_prompt_embeddings=sparse_embeddings,\n                dense_prompt_embeddings=dense_embeddings,\n                multimask_output=multimask_output,\n            )\n            masks = self.postprocess_masks(\n                low_res_masks,\n                input_size=image_record[\"image\"].shape[-2:],",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:1151-1173"
    },
    "663": {
        "file_id": 50,
        "content": "This code is for image-based text generation using a model with separate encoders and decoders. It takes input images, extracts their embeddings, then uses these embeddings along with other inputs like point coordinates, bounding boxes, and masks to generate low-resolution masks and IOU predictions. The masks are then post-processed for the final output.",
        "type": "comment"
    },
    "664": {
        "file_id": 50,
        "content": "                original_size=image_record[\"original_size\"],\n            )\n            masks = masks > self.mask_threshold\n            outputs.append(\n                {\n                    \"masks\": masks,\n                    \"iou_predictions\": iou_predictions,\n                    \"low_res_logits\": low_res_masks,\n                }\n            )\n        return outputs\n    def postprocess_masks(\n        self,\n        masks: torch.Tensor,\n        input_size: Tuple[int, ...],\n        original_size: Tuple[int, ...],\n    ) -> torch.Tensor:\n        \"\"\"\n        Remove padding and upscale masks to the original image size.\n        Arguments:\n          masks (torch.Tensor): Batched masks from the mask_decoder,\n            in BxCxHxW format.\n          input_size (tuple(int, int)): The size of the image input to the\n            model, in (H, W) format. Used to remove padding.\n          original_size (tuple(int, int)): The original size of the image\n            before resizing for input to the model, in (H, W) format.\n        Returns:",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:1174-1203"
    },
    "665": {
        "file_id": 50,
        "content": "This code contains a model that takes in masks, input and original sizes as inputs and returns outputs. The 'masks' are processed to remove padding and upscale to the original image size using the mask_decoder. The result is a batched mask, where each batch element represents an image, with dimensions BxCxHxW format.",
        "type": "comment"
    },
    "666": {
        "file_id": 50,
        "content": "          (torch.Tensor): Batched masks in BxCxHxW format, where (H, W)\n            is given by original_size.\n        \"\"\"\n        masks = F.interpolate(\n            masks,\n            (self.image_encoder.img_size, self.image_encoder.img_size),\n            mode=\"bilinear\",\n            align_corners=False,\n        )\n        h, w = input_size[0], input_size[1]\n        max_dim = max(w, h)\n        pad_w = (max_dim - w) // 2\n        pad_h = (max_dim - h) // 2\n        masks = masks[..., pad_h: input_size[0]+pad_h, pad_w: pad_w+input_size[1]]\n        masks = F.interpolate(masks, original_size, mode=\"bilinear\", align_corners=False)\n        return masks\n    def preprocess(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Normalize pixel values and pad to a square input.\"\"\"\n        # Normalize colors\n        x = (x - self.pixel_mean) / self.pixel_std\n        # Pad\n        h, w = x.shape[-2:]\n        padh = self.image_encoder.img_size - h\n        padw = self.image_encoder.img_size - w\n        x = F.pad(x, (0, padw, 0, padh))",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:1204-1231"
    },
    "667": {
        "file_id": 50,
        "content": "This code is from a machine learning model, specifically the Sam model. It includes functions for generating masks and preprocessing inputs. The `generate_masks` function interpolates and pads masks to match the input size. The `preprocess` function normalizes pixel values and pads inputs to a square shape. This model is likely used in image processing tasks.",
        "type": "comment"
    },
    "668": {
        "file_id": 50,
        "content": "        return x\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nfrom functools import partial\ndef build_sam_vit_h(checkpoint=None):\n    return _build_sam(\n        encoder_embed_dim=1280,\n        encoder_depth=32,\n        encoder_num_heads=16,\n        encoder_global_attn_indexes=[7, 15, 23, 31],\n        checkpoint=checkpoint,\n    )\nbuild_sam = build_sam_vit_h\ndef build_sam_vit_l(checkpoint=None):\n    return _build_sam(\n        encoder_embed_dim=1024,\n        encoder_depth=24,\n        encoder_num_heads=16,\n        encoder_global_attn_indexes=[5, 11, 17, 23],\n        checkpoint=checkpoint,\n    )\ndef build_sam_vit_b(checkpoint=None):\n    return _build_sam(\n        encoder_embed_dim=768,\n        encoder_depth=12,\n        encoder_num_heads=12,\n        encoder_global_attn_indexes=[2, 5, 8, 11],\n        checkpoint=checkpoint,\n    )\nsam_model_registry = {\n    \"default\": build_sam_vit_h,",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:1232-1279"
    },
    "669": {
        "file_id": 50,
        "content": "The code defines three functions build_sam_vit_h, build_sam_vit_l, and build_sam_vit_b that return different implementations of the SAM model using different configurations. The SAM model is a transformer-based architecture with global attention mechanisms. The functions accept an optional checkpoint parameter. The sam_model_registry dictionary maps \"default\" to build_sam_vit_h, which appears to be the default implementation.",
        "type": "comment"
    },
    "670": {
        "file_id": 50,
        "content": "    \"vit_h\": build_sam_vit_h,\n    \"vit_l\": build_sam_vit_l,\n    \"vit_b\": build_sam_vit_b,\n}\ndef _build_sam(\n    encoder_embed_dim,\n    encoder_depth,\n    encoder_num_heads,\n    encoder_global_attn_indexes,\n    checkpoint=None,\n):\n    prompt_embed_dim = 256\n    image_size = 1024\n    vit_patch_size = 16\n    image_embedding_size = image_size // vit_patch_size\n    sam = Sam(\n        image_encoder=ImageEncoderViT(\n            depth=encoder_depth,\n            embed_dim=encoder_embed_dim,\n            img_size=image_size,\n            mlp_ratio=4,\n            norm_layer=partial(torch.nn.LayerNorm, eps=1e-6),\n            num_heads=encoder_num_heads,\n            patch_size=vit_patch_size,\n            qkv_bias=True,\n            use_rel_pos=True,\n            global_attn_indexes=encoder_global_attn_indexes,\n            window_size=14,\n            out_chans=prompt_embed_dim,\n        ),\n        prompt_encoder=PromptEncoder(\n            embed_dim=prompt_embed_dim,\n            image_embedding_size=(image_embedding_size, image_embedding_size),",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:1280-1314"
    },
    "671": {
        "file_id": 50,
        "content": "This code is defining a function `_build_sam` that builds a SAM (Sampling Model) using different Vision Transformers (ViT) of varying sizes (\"vit_h\", \"vit_l\", and \"vit_b\"). The SAM consists of an image encoder, which is a ViT, and a prompt encoder. It takes in parameters such as the embedding dimension, depth, number of heads, global attention indexes for the image encoder, and optional checkpointing. The prompt encoder has an embed_dim (prompt embedding size) and an image_embedding_size matching the image encoder's output channel count.",
        "type": "comment"
    },
    "672": {
        "file_id": 50,
        "content": "            input_image_size=(image_size, image_size),\n            mask_in_chans=16,\n        ),\n        mask_decoder=MaskDecoder(\n            num_multimask_outputs=3,\n            transformer=TwoWayTransformer(\n                depth=2,\n                embedding_dim=prompt_embed_dim,\n                mlp_dim=2048,\n                num_heads=8,\n            ),\n            transformer_dim=prompt_embed_dim,\n            iou_head_depth=3,\n            iou_head_hidden_dim=256,\n        ),\n        pixel_mean=[123.675, 116.28, 103.53],\n        pixel_std=[58.395, 57.12, 57.375],\n    )\n    sam.eval()\n    if checkpoint is not None:\n        with open(checkpoint, \"rb\") as f:\n            state_dict = torch.load(f)\n        sam.load_state_dict(state_dict)\n    return sam\nfrom mllm.models.sam.transforms import ResizeAndPad, ResizeLongestSide\nclass SamForLMSeg(nn.Module):\n    def __init__(self, model_type, ckpt):\n        super().__init__()\n        self.model = sam_model_registry[model_type](checkpoint=ckpt)\n    def forward(self, images, sparse_embeddings, boxes):",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:1315-1348"
    },
    "673": {
        "file_id": 50,
        "content": "This code defines a class `SamForLMSeg` which takes in `model_type` and `ckpt`, initializes an instance of the model specified by `model_type` from the `sam_model_registry`, and provides a `forward` method for running inference on images, sparse embeddings, and boxes. The model is initialized with optional checkpoint `ckpt`.",
        "type": "comment"
    },
    "674": {
        "file_id": 50,
        "content": "        _, _, H, W = images.shape\n        image_embeddings = self.model.image_encoder(images)\n        pred_masks = []\n        ious = []\n        device = image_embeddings.device\n        box_embeddings, dense_embeddings = self.model.prompt_encoder(\n            points=None,\n            boxes=boxes,\n            masks=None,\n        )\n        sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], 1)\n        low_res_masks, iou_predictions = self.model.mask_decoder(\n            image_embeddings=image_embeddings,\n            image_pe=self.model.prompt_encoder.get_dense_pe(),\n            sparse_prompt_embeddings=sparse_embeddings,\n            dense_prompt_embeddings=dense_embeddings,\n            multimask_output=False,\n        )\n        pred_masks = F.interpolate(\n            low_res_masks.float(),\n            (H, W),\n            mode=\"bilinear\",\n            align_corners=False,\n        )\n        return pred_masks, iou_predictions\n    @property\n    def device(self):\n        return self.model.image_encoder.patch_embed.proj.weight.device",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:1349-1380"
    },
    "675": {
        "file_id": 50,
        "content": "This code defines a model for generating mask predictions using image and prompt embeddings. It involves image encoding, prompt encoding, and mask decoding. The predicted masks are interpolated to match the input image's shape. The device property is used to specify the hardware device for computations.",
        "type": "comment"
    },
    "676": {
        "file_id": 50,
        "content": "    def predict(self,\n                image,\n                sparse_embedding=None,\n                point_coords: Optional[np.ndarray] = None,\n                point_labels: Optional[np.ndarray] = None,\n                box: Optional[np.ndarray] = None,\n                mask_input: Optional[np.ndarray] = None,\n                multimask_output: bool = False,\n                return_logits: bool = False,\n                ):\n        dtype = self.model.image_encoder.patch_embed.proj.weight.dtype\n        self.original_size = [image.height, image.width]\n        input_image, _, hw = ResizeAndPad(1024)(image, None)\n        self.input_size = [int(hw[0]), int(hw[1])]\n        self.features = self.model.image_encoder(input_image.to(self.device).unsqueeze(0).type(dtype))\n        self.transform = ResizeAndPad(1024) # TODO modify this\n        # Transform input prompts\n        coords_torch, labels_torch, box_torch, mask_input_torch = None, None, None, None\n        if point_coords is not None:\n            assert (\n                    point_labels is not None",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:1382-1404"
    },
    "677": {
        "file_id": 50,
        "content": "This function defines a predict method for a model. It resizes and pads input images to a specific size, extracts features from the image, and transforms point coords, labels, box and mask_input inputs if not None. The dtype is set according to the weight of the projection matrix in the image encoder, and the original and input sizes are stored for reference.",
        "type": "comment"
    },
    "678": {
        "file_id": 50,
        "content": "            ), \"point_labels must be supplied if point_coords is supplied.\"\n            point_coords = self.transform.apply_coords(point_coords, self.original_size)\n            coords_torch = torch.as_tensor(point_coords, dtype=torch.float, device=self.device)\n            labels_torch = torch.as_tensor(point_labels, dtype=torch.int, device=self.device)\n            coords_torch, labels_torch = coords_torch[None, :, :], labels_torch[None, :]\n        if box is not None:\n            # box = self.transform.apply_boxes(box, self.original_size)\n            box_torch = torch.as_tensor(box, dtype=dtype, device=self.device)\n            box_torch = box_torch[None, :]\n        if mask_input is not None:\n            mask_input_torch = torch.as_tensor(mask_input, dtype=torch.float, device=self.device)\n            mask_input_torch = mask_input_torch[None, :, :, :]\n        masks, iou_predictions, low_res_masks = self.predict_torch(\n            sparse_embedding,\n            coords_torch,\n            labels_torch,\n            box_torch,",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:1405-1422"
    },
    "679": {
        "file_id": 50,
        "content": "This code prepares and transforms inputs for the model's predict method. It applies coordinate transformation, converts input to tensors, adjusts dimensions, and handles optional box and mask_input parameters. The model then makes predictions using these prepared inputs.",
        "type": "comment"
    },
    "680": {
        "file_id": 50,
        "content": "            mask_input_torch,\n            multimask_output,\n            return_logits=return_logits,\n        )\n        masks_np = masks[0].detach().float().cpu().numpy()\n        iou_predictions_np = iou_predictions[0].detach().float().cpu().numpy()\n        low_res_masks_np = low_res_masks[0].detach().float().cpu().numpy()\n        return masks_np, iou_predictions_np, low_res_masks_np\n    @torch.no_grad()\n    def predict_torch(\n        self,\n        input_sparse_embedding=None,\n        point_coords: Optional[torch.Tensor]=None,\n        point_labels: Optional[torch.Tensor]=None,\n        boxes: Optional[torch.Tensor] = None,\n        mask_input: Optional[torch.Tensor] = None,\n        multimask_output: bool = True,\n        return_logits: bool = False,\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        if point_coords is not None:\n            points = (point_coords, point_labels)\n        else:\n            points = None\n        # Embed prompts\n        sparse_embeddings, dense_embeddings = self.model.prompt_encoder(",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:1423-1449"
    },
    "681": {
        "file_id": 50,
        "content": "This function takes input sparse embedding, point coordinates and labels, boxes, mask input, multimask output, and return logits as arguments. It returns three tensors of masks_np, iou_predictions_np, and low_res_masks_np after converting them into numpy format and moving them to CPU. The function also defines another function called predict_torch that takes similar arguments and embeds prompts using the prompt_encoder method from self.model.",
        "type": "comment"
    },
    "682": {
        "file_id": 50,
        "content": "            points=points,\n            boxes=boxes,\n            masks=mask_input,\n        )\n        if input_sparse_embedding is not None:\n            sparse_embeddings = input_sparse_embedding\n        # Predict masks\n        low_res_masks, iou_predictions = self.model.mask_decoder(\n            image_embeddings=self.features,\n            image_pe=self.model.prompt_encoder.get_dense_pe(),\n            sparse_prompt_embeddings=sparse_embeddings,\n            dense_prompt_embeddings=dense_embeddings,\n            multimask_output=multimask_output,\n        )\n        # Upscale the masks to the original image resolution\n        masks = self.model.postprocess_masks(low_res_masks, self.input_size, self.original_size)\n        if not return_logits:\n            masks = masks > self.model.mask_threshold\n        return masks, iou_predictions, low_res_masks",
        "type": "code",
        "location": "/mllm/models/sam/modeling_sam.py:1450-1473"
    },
    "683": {
        "file_id": 50,
        "content": "This code predicts masks and calculates IOU predictions from image embeddings, prompt embeddings, and multimask output. The masks are then upscaled to the original image resolution and returned based on a threshold.",
        "type": "comment"
    },
    "684": {
        "file_id": 51,
        "content": "/mllm/models/sam/sam_loss.py",
        "type": "filepath"
    },
    "685": {
        "file_id": 51,
        "content": "This code defines custom loss functions FocalLoss, DiceLoss, and IoU loss for segmentation models. It combines these losses to measure prediction accuracy with weights: 20 times focal loss and sum of Dice and IOU losses.",
        "type": "summary"
    },
    "686": {
        "file_id": 51,
        "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nALPHA = 0.8\nGAMMA = 2\nclass FocalLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super().__init__()\n    def forward(self, inputs, targets, alpha=ALPHA, gamma=GAMMA, smooth=1):\n        # inputs = F.sigmoid(inputs)\n        # inputs = torch.clamp(inputs, min=0, max=1)\n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        BCE = F.binary_cross_entropy_with_logits(inputs, targets, reduction='mean')\n        BCE_EXP = torch.exp(-BCE)\n        focal_loss = alpha * (1 - BCE_EXP)**gamma * BCE\n        return focal_loss\nclass DiceLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super().__init__()\n    def forward(self, inputs, targets, smooth=1):\n        inputs = F.sigmoid(inputs)\n        inputs = torch.clamp(inputs, min=0, max=1)\n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)",
        "type": "code",
        "location": "/mllm/models/sam/sam_loss.py:1-37"
    },
    "687": {
        "file_id": 51,
        "content": "Code from NExT-Chat/mllm/models/sam/sam_loss.py:0-36 defines two custom loss functions, FocalLoss and DiceLoss, extending torch.nn.Module. Both classes overwrite the forward() method to compute their respective loss values based on given inputs and targets. FocalLoss applies a focal loss function that combines binary cross entropy with a modulating factor for class imbalance. DiceLoss computes the Dice coefficient between inputs (probability map) and targets, which measures the similarity and is helpful in imbalanced datasets.",
        "type": "comment"
    },
    "688": {
        "file_id": 51,
        "content": "        intersection = (inputs * targets).sum()\n        dice = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n        return 1 - dice\ndef calc_iou(pred_mask: torch.Tensor, gt_mask: torch.Tensor):\n    pred_mask = (pred_mask >= 0.5)\n    intersection = torch.sum(torch.mul(pred_mask, gt_mask), dim=(1, 2))\n    union = torch.sum(pred_mask, dim=(1, 2)) + torch.sum(gt_mask, dim=(1, 2)) - intersection\n    epsilon = 1e-7\n    batch_iou = intersection / (union + epsilon)\n    # batch_iou = batch_iou.unsqueeze(1)\n    return batch_iou\nclass SamLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.focal_loss = FocalLoss()\n        self.dice_loss = DiceLoss()\n    def forward(self, pred_masks, gt_masks, iou_predictions, device):\n        loss_focal = 0.\n        loss_dice = 0.\n        loss_iou = 0.\n        num_masks = sum(len(pred_mask) for pred_mask in pred_masks)\n        for pred_mask, gt_mask, iou_prediction in zip(pred_masks, gt_masks, iou_predictions):\n            gt_mask = gt_mask.to(device)",
        "type": "code",
        "location": "/mllm/models/sam/sam_loss.py:39-68"
    },
    "689": {
        "file_id": 51,
        "content": "The code defines a SamLoss class that computes the focal loss, dice loss, and intersection over union (IoU) loss for predicted masks. The calc_iou function calculates the IoU between pred_mask and gt_mask. The SamLoss forward method takes in pred_masks, gt_masks, iou_predictions, and device as input to compute focal loss, dice loss, and iou loss for each mask.",
        "type": "comment"
    },
    "690": {
        "file_id": 51,
        "content": "            batch_iou = calc_iou(pred_mask, gt_mask)\n            loss_focal += self.focal_loss(pred_mask, gt_mask, num_masks)\n            loss_dice += self.dice_loss(pred_mask, gt_mask, num_masks)\n            loss_iou += F.mse_loss(iou_prediction, batch_iou, reduction='sum') / num_masks\n        loss_total = 20. * loss_focal + loss_dice + loss_iou\n        return loss_total",
        "type": "code",
        "location": "/mllm/models/sam/sam_loss.py:69-75"
    },
    "691": {
        "file_id": 51,
        "content": "This code calculates the loss function for a segmentation model. It combines focal loss, Dice loss, and IOU MSE loss to measure prediction accuracy. The weights are set with 20 times the focal loss and the sum of Dice and IOU losses.",
        "type": "comment"
    },
    "692": {
        "file_id": 52,
        "content": "/mllm/models/sam/transforms.py",
        "type": "filepath"
    },
    "693": {
        "file_id": 52,
        "content": "The ResizeLongestSide class handles image resizing, while the ResizeAndPad class resizes and pads images to target size, normalizes, and adjusts bounding boxes if masks are provided. Both classes define transform functions in `transforms.py`.",
        "type": "summary"
    },
    "694": {
        "file_id": 52,
        "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\nimport PIL.Image\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nimport numpy as np\nimport torch\nfrom torch.nn import functional as F\nfrom torchvision.transforms.functional import resize, to_pil_image  # type: ignore\nfrom copy import deepcopy\nfrom typing import Tuple\nimport torchvision.transforms as transforms\nclass ResizeLongestSide:\n    \"\"\"\n    Resizes images to the longest side 'target_length', as well as provides\n    methods for resizing coordinates and boxes. Provides methods for\n    transforming both numpy array and batched torch tensors.\n    \"\"\"\n    def __init__(self, target_length: int) -> None:\n        self.target_length = target_length\n    def apply_image(self, image) -> np.ndarray:\n        \"\"\"\n        Expects a numpy array with shape HxWxC in uint8 format.\n        \"\"\"\n        if type(image) is np.ndarray:\n            h, w = image.shape[0], image.shape[1]\n            target_size = self.get_preprocess_shape(h, w, self.target_length)",
        "type": "code",
        "location": "/mllm/models/sam/transforms.py:1-33"
    },
    "695": {
        "file_id": 52,
        "content": "This code defines a ResizeLongestSide class that resizes images to the longest side specified and provides methods for resizing coordinates and boxes. It accepts numpy arrays or batched torch tensors, with the expectation of a numpy array being in uint8 format.",
        "type": "comment"
    },
    "696": {
        "file_id": 52,
        "content": "            return np.array(resize(to_pil_image(image), target_size))\n        else:\n            h, w = image.height, image.width\n            target_size = self.get_preprocess_shape(h, w, self.target_length)\n            return np.array(resize(image, target_size))\n    def apply_coords(self, coords: np.ndarray, original_size: Tuple[int, ...]) -> np.ndarray:\n        \"\"\"\n        Expects a numpy array of length 2 in the final dimension. Requires the\n        original image size in (H, W) format.\n        \"\"\"\n        old_h, old_w = original_size\n        new_h, new_w = self.get_preprocess_shape(\n            original_size[0], original_size[1], self.target_length\n        )\n        coords = deepcopy(coords).astype(float)\n        coords[..., 0] = coords[..., 0] * (new_w / old_w)\n        coords[..., 1] = coords[..., 1] * (new_h / old_h)\n        return coords\n    def apply_boxes(self, boxes: np.ndarray, original_size: Tuple[int, ...]) -> np.ndarray:\n        \"\"\"\n        Expects a numpy array shape Bx4. Requires the original image size",
        "type": "code",
        "location": "/mllm/models/sam/transforms.py:34-56"
    },
    "697": {
        "file_id": 52,
        "content": "The code defines a class with methods to resize and apply transformations to images. It has a method for resizing images with target sizes, another for applying coordinate transformations based on image size changes, and yet another for applying box transformations to numpy arrays of shape Bx4. The class is likely used in an image processing pipeline for machine learning tasks.",
        "type": "comment"
    },
    "698": {
        "file_id": 52,
        "content": "        in (H, W) format.\n        \"\"\"\n        boxes = self.apply_coords(boxes.reshape(-1, 2, 2), original_size)\n        return boxes.reshape(-1, 4)\n    def apply_image_torch(self, image: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Expects batched images with shape BxCxHxW and float format. This\n        transformation may not exactly match apply_image. apply_image is\n        the transformation expected by the model.\n        \"\"\"\n        # Expects an image in BCHW format. May not exactly match apply_image.\n        target_size = self.get_preprocess_shape(image.shape[2], image.shape[3], self.target_length)\n        return F.interpolate(\n            image, target_size, mode=\"bilinear\", align_corners=False, antialias=True\n        )\n    def apply_coords_torch(\n        self, coords: torch.Tensor, original_size: Tuple[int, ...]\n    ) -> torch.Tensor:\n        \"\"\"\n        Expects a torch tensor with length 2 in the last dimension. Requires the\n        original image size in (H, W) format.\n        \"\"\"\n        old_h, old_w = original_size",
        "type": "code",
        "location": "/mllm/models/sam/transforms.py:57-81"
    },
    "699": {
        "file_id": 52,
        "content": "The code contains functions for applying transformations to images and coordinates in a deep learning model. The apply_image_torch function resizes the input image to match the target size, while the apply_coords_torch function applies coordinate transformations based on the original image size. These functions are designed to be used with torch tensors and may not exactly match the apply_image function.",
        "type": "comment"
    }
}